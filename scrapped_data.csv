,Names,Data
0,Artificial_intelligence,"





Artificial intelligence

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Intelligence demonstrated by machines
""AI"" redirects here. For other uses, see AI (disambiguation) and Artificial intelligence (disambiguation).


Part of a series onArtificial intelligence
Major goals
Artificial general intelligence
Planning
Computer vision
General game playing
Knowledge reasoning
Machine learning
Natural language processing
Robotics

Approaches
Symbolic
Deep learning
Bayesian networks
Evolutionary algorithms

Philosophy
Chinese room
Friendly AI
Control problem/Takeover
Ethics
Existential risk
Turing test

History
Timeline
Progress
AI winter

Technology
Applications
Projects
Programming languages

Glossary
Glossary
vte

Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.[a]
The term ""artificial intelligence"" had previously been used to describe machines that mimic and display ""human"" cognitive skills that are associated with the human mind, such as ""learning"" and ""problem-solving"". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.[b]
AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[2][citation needed]
As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.[3]  For instance, optical character recognition is frequently excluded from things considered to be AI,[4] having become a routine technology.[5]
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[6][7] followed by disappointment and the loss of funding (known as an ""AI winter""),[8][9] followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[11][10]
The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.[c] General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.[12] To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[d]
This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[14] Science fiction writers and futurologists have since suggested that AI may become an existential risk to humanity if its rational capacities are not overseen.[15][16]

Contents

1 History
2 Goals

2.1 Reasoning, problem-solving
2.2 Knowledge representation
2.3 Planning
2.4 Learning
2.5 Natural language processing
2.6 Perception
2.7 Motion and manipulation
2.8 Social intelligence
2.9 General intelligence


3 Tools

3.1 Search and optimization
3.2 Logic
3.3 Probabilistic methods for uncertain reasoning
3.4 Classifiers and statistical learning methods
3.5 Artificial neural networks

3.5.1 Deep learning


3.6 Specialized languages and hardware


4 Applications
5 Philosophy

5.1 Defining artificial intelligence

5.1.1 Thinking vs. acting: the Turing test
5.1.2 Acting humanly vs. acting intelligently: intelligent agents


5.2 Evaluating approaches to AI

5.2.1 Symbolic AI and its limits
5.2.2 Neat vs. scruffy
5.2.3 Soft vs. hard computing
5.2.4 Narrow vs. general AI


5.3 Machine consciousness, sentience and mind

5.3.1 Consciousness
5.3.2 Computationalism and functionalism
5.3.3 Robot rights




6 Future

6.1 Superintelligence
6.2 Risks

6.2.1 Technological unemployment
6.2.2 Bad actors and weaponized AI
6.2.3 Algorithmic bias
6.2.4 Existential risk


6.3 Ethical machines
6.4 Regulation


7 In fiction
8 See also
9 Explanatory notes
10 Citations
11 References

11.1 AI textbooks
11.2 History of AI
11.3 Other sources


12 Further reading
13 External links
14 Sources



History
Main articles: History of artificial intelligence and Timeline of artificial intelligence
 Silver didrachma from Crete depicting Talos, an ancient mythical automaton with artificial intelligence
Artificial beings with intelligence appeared as storytelling devices in antiquity,[17]
and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.[18] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[19]
The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.[20]
The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.[21]
The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".[22]
By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the ""heuristic search"" approach, which likened intelligence to a problem of exploring a space of possibilities for answers. The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[23] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to it's connection to intellectual traditions of Descarte, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[24]
The field of AI research was born at a workshop at Dartmouth College in 1956.[e][27]
The attendees became the founders and leaders of AI research.[f]
They and their students produced programs that the press described as ""astonishing"":[g]
computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[h][29]
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[30]
and laboratories had been established around the world.[31]
Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.[32]
Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".[33]
Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".[34]
They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[35]
and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.
[8]
In the early 1980s, AI research was revived by the commercial success of expert systems,[36]
a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[7]
However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[9]
Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems.[37] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[i]
Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[42]
Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[43]
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".[11]
Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[44]
According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects.[j] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[10] In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"".[45] The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.[46]
Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.[12]

Goals
The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[c]

Reasoning, problem-solving
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[47]
By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[48]
Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger.[49]
Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[50]

Knowledge representation
Main articles: Knowledge representation, Commonsense knowledge, Description logic, and Ontology
 An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts.
Knowledge representation and knowledge engineering[51]
allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[52]
The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[53]
AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[53]
situations, events, states and time;[54]
causes and effects;[55]
knowledge about knowledge (what we know about what other people know);.[56]
default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);
[57]
as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[58]
and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).[50]
Formal knowledge representations are used in content-based indexing and retrieval,[59]
scene interpretation,[60]
clinical decision support,[61]
knowledge discovery (mining ""interesting"" and actionable inferences from large databases),[62]
and other areas.[63]

Planning
Main article: Automated planning and scheduling
An intelligent agent that can plan makes a representation of the state of the world, makes predictions about how their actions will change it and make choices that maximize the utility (or ""value"") of the available choices.[64]
In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[65]
However, if the agent is not the only actor, then it requires that the agent reason under uncertainty, and continuously re-assess its environment and adapt.[66]
Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[67]

Learning
Main article: Machine learning
Machine learning (ML), a fundamental concept of AI research since the field's inception,[k]
is the study of computer algorithms that improve automatically through experience.[l]
Unsupervised learning finds patterns in a stream of input. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in—the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".[71]
In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[72]
Transfer learning is when the knowledge gained from one problem is applied to a new problem.[73]
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[74]


Natural language processing
Main article: Natural language processing
 A parse tree represents the syntactic structure of a sentence according to some formal grammar.
Natural language processing (NLP)[75]
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[76]

Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[49] and the breadth of commonsense knowledge.[58] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[77] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[78]
Perception
Main articles: Machine perception, Computer vision, and Speech recognition
 Feature detection (pictured: edge detection) helps AI compose informative abstract structures out of raw data.
Machine perception[79]
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[80]
facial recognition, and object recognition.[81]

Computer vision is the ability to analyze visual input.[82]
Motion and manipulation
Main article: Robotics
AI is heavily used in robotics.[83]
Localization is how a robot knows its location and maps its environment. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge.[84]

Motion planning is the process of breaking down a movement task into ""primitives"" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Robots can learn from experience how to move efficiently despite the presence of friction and gear slippage.[85]
Social intelligence
Main article: Affective computing
 Kismet, a robot with rudimentary social skills[86]
Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.[87] 
For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[88]

Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[89]
General intelligence
Main article: Artificial general intelligence
A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[90]
Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.[91]
Others believe that anthropomorphic features like an artificial brain[92]
or simulated child development[m]
will someday reach a critical point where general intelligence emerges.

Tools
Search and optimization
Main articles: Search algorithm, Mathematical optimization, and Evolutionary computation
Many problems in AI can be solved theoretically by intelligently searching through many possible solutions:[93]
Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[94]
Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[95]
Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[96]
Simple exhaustive searches[97]
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.[98]
Heuristics limit the search for solutions into a smaller sample size.[99]

 A particle swarm seeking the global minimum
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[100]
Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[101]
Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[102]

Logic
Main articles: Logic programming and Automated reasoning
Logic[103]
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[104]
and inductive logic programming is a method for learning.[105]
Several different forms of logic are used in AI research. Propositional logic[106] involves truth functions such as ""or"" and ""not"". First-order logic[107]
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[108]
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[57]
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[53]
situation calculus, event calculus and fluent calculus (for representing events and time);[54]
causal calculus;[55]
belief calculus (belief revision); and modal logics.[56]
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[citation needed]

Probabilistic methods for uncertain reasoning
Main articles: Bayesian network, Hidden Markov model, Kalman filter, Particle filter, Decision theory, and Utility theory
 Expectation-maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption.
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[109]
Bayesian networks[110]
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[n][112]
learning (using the expectation-maximization algorithm),[o][114]
planning (using decision networks)[115] and perception (using dynamic Bayesian networks).[116]
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[116]
A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[117]
and information value theory.[118] These tools include models such as Markov decision processes,[119] dynamic decision networks,[116] game theory and mechanism design.[120]

Classifiers and statistical learning methods
Main articles: Classifier (mathematics), Statistical classification, and Machine learning
The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[121]
A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.[122]
K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[123]
Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[124]
The naive Bayes classifier is reportedly the ""most widely used learner""[125] at Google, due in part to its scalability.[126]
Neural networks are also used for classification.[127]
Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.[128]

Artificial neural networks
Main articles: Artificial neural network and Connectionism
 A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain.
Neural networks[127]
were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization — they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.[129]
Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.[130]
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[131]


Deep learning
 Representing images on multiple layers of abstraction in deep learning[132]
Deep learning[133]
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[134] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[135] and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[136] and creates a hierarchy similar to the organization of the animal visual cortex.[137]
In a recurrent neural network the signal will propagate through a layer more than once;[138] 
thus, an RNN is an example of deep learning.[139]
RNNs can be trained by gradient descent,[140]
however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.[141]
The long short term memory (LSTM) technique can prevent this in most cases.[142]

Specialized languages and hardware
Main articles: Programming languages for artificial intelligence and Hardware for artificial intelligence
Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.

Applications
Main article: Applications of artificial intelligenceSee also: Embodied cognition and Legal informatics
 For this project the AI had to learn the typical patterns in the colors and brushstrokes of Renaissance painter Raphael. The portrait shows the face of the actress Ornella Muti, ""painted"" by AI in the style of Raphael.
AI is relevant to any intellectual task.[143]
Modern artificial intelligence techniques are pervasive and are too numerous to list here.[144]
Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[145]
In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements,[146][non-primary source needed]
recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,[147][148]
targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),[149]
autonomous vehicles (including drones and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace),
image labeling (used by Facebook, Apple's iPhoto and TikTok)
and spam filtering.
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are 
energy storage,[150]
deepfakes,[151]
medical diagnosis, 
military logistics, or 
supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[152] 
In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[153] 
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[154]
Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[p]
and Cepheus.[156]
DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.[157]
By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[158]
DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[159]
Other applications predict the result of judicial decisions,[160] create art (such as poetry or painting) and prove mathematical theorems.

 AI Patent families for functional application categories and sub categories. Computer vision represents 49 percent of patent families related to a functional application in 2016.
In 2019, WIPO reported that AI was the most prolific emerging technology in terms of number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G).[161] Since AI emerged in the 1950s, 340000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four.[162] The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134777 machine learning patents filed for a total of 167038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.[163]

Philosophy
Main article: Philosophy of artificial intelligence
Defining artificial intelligence
Thinking vs. acting: the Turing test
Main articles: Turing test, Dartmouth Workshop, and Synthetic intelligence
Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""[164]
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".[165] 
The only thing visible is the behavior of the machine, so it does not matter if the machine is conscious, or has a mind, or whether the intelligence is merely a ""simulation"" and not ""the real thing"".  He noted that we also don't know these things about other people, but that we extend a ""polite convention"" that they are actually ""thinking"". This idea forms the basis of the Turing test.[166][q]

Acting humanly vs. acting intelligently: intelligent agents
Main article: Intelligent agents
AI founder John McCarthy said: ""Artificial intelligence is not, by definition, simulation of human intelligence"".[168] Russell and Norvig agree and criticize the Turing test. They wrote: ""Aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'""[167] Other researchers and analysts disagree and have argued that AI should simulate natural intelligence by studying psychology or neurobiology.[r]
The intelligent agent paradigm[170]
defines intelligent behavior in general, without reference to human beings. An intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. Any system that has goal-directed behavior can be analyzed as an intelligent agent: something as simple as a thermostat, as complex as a human being, as well as large systems such as firms, biomes or nations. The intelligent agent paradigm became widely accepted during the 1990s, and currently serves as the definition of the field.[a]
The paradigm has other advantages for AI. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given ""goal function"".  It also gives them a common language to communicate with other fields — such as mathematical optimization (which is defined in terms of ""goals"") or economics (which uses the same definition of a ""rational agent"").[171]

Evaluating approaches to AI
No established unifying theory or paradigm has guided AI research for most of its history.[s] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.

Symbolic AI and its limits
Main articles: Symbolic AI, Physical symbol systems hypothesis, Moravec's paradox, and Dreyfus' critique of artificial intelligence
Symbolic AI (or ""GOFAI"")[173] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""[174]
However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.[175]
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.[176]
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[t][50]
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as  Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[178][179] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.

Neat vs. scruffy
Main article: Neats and scruffies
""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. This issue was actively discussed in the 70s and 80s,[180]
but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed ""the victory of the neats"".[181]

Soft vs. hard computing
Main article: Soft computing
Finding a provably correct or optimal solution is intractable for many important problems.[49] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.

Narrow vs. general AI
Main article: Artificial general intelligence
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals[182][183]
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.

Machine consciousness, sentience and mind
Main articles: Philosophy of artificial intelligence and Artificial Consciousness
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] — as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.""[184] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.

Consciousness
Main articles: Hard problem of consciousness and Theory of mind
David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.[185] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[186]

Computationalism and functionalism
Main articles: Computationalism, Functionalism (philosophy of mind), and Chinese room
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[187]
Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""[u]
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[190]

Robot rights
Main article: Robot rights
If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.[191]
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[192]
This issue has been considered in fiction for centuries,[193]
and is now being considered by, for example, California's Institute for the Future, however, critics argue that the discussion is premature.[194]

Future
Superintelligence
Main articles: Superintelligence, Technological singularity, and Transhumanism
A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[183]
If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[195]
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".[196]
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[197]
Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[198]
Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[199]

Risks
 Lecture by Fillipo Santoni de Sio (Delft University of Technology) on the risks of  artificial intelligence and how we can keep artificial intelligence under control
Technological unemployment
Main articles: Workplace impact of artificial intelligence and Technological unemployment
In the past technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.[200]
A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[201]
Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"".[v][203]
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".[204]
Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[205]

Bad actors and weaponized AI
Main articles: Lethal autonomous weapon and Artificial intelligence arms race
AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.[206]
Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.[207]
Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.[208]

Algorithmic bias
Main article: Algorithmic bias
AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[209]
Bias can be inadvertently introduced by the way training data is selected.[210]
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[211]
An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[212] Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.

Existential risk
Main articles: Existential risk from artificial general intelligence and Superintelligence
Superintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, ""spell the end of the human race"".[213] Philosopher Nick Bostrom argues that sufficiently intelligent AI if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or ""friendly"" its stated goals might be.[214]
Political scientist Charles T. Rubin argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.[215]
The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[216]
Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.[217]
Prominent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.[218]
Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.[219]
Other experts argue is that the risks are far enough in the future to not be worth researching,
or that humans will be valuable from the perspective of a superintelligent machine.[220]
Rodney Brooks, in particular, has said that ""malevolent"" AI is still centuries away.[w]

Ethical machines
Main articles: Machine ethics, Friendly AI, Artificial moral agents, and Human Compatible
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[222]
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[223]
Machine ethics is also called machine morality, computational ethics or computational morality,[223]
and was founded at an AAAI symposium in 2005.[224]
Other approaches include Wendell Wallach's ""artificial moral agents""[225]
and Stuart J. Russell's three principles for developing provably beneficial machines.[226]

Regulation
Main articles: Regulation of artificial intelligence, Regulation of algorithms, and AI control problem
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[227]
The regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally.[228]
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[46]
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[46]
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[46] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[229]

In fiction
Main article: Artificial intelligence in fiction
 The word ""robot"" itself was coined by Karel Čapek in his 1921 play R.U.R., the title standing for ""Rossum's Universal Robots""
Thought-capable artificial beings have appeared as storytelling devices since antiquity,[17]
and have been a persistent theme in science fiction.[19]
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[230]
Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[231]
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[232]
Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[233]

See also


Computer programming portal
A.I. Rising
AI control problem
Artificial intelligence arms race
Artificial general intelligence
Behavior selection algorithm
Business process automation
Case-based reasoning
Citizen science
Emergent algorithm
Female gendering of AI technologies
Glossary of artificial intelligence
Robotic process automation
Synthetic intelligence
Universal basic income
Weak AI
Explanatory notes


^ a b Definition of AI as the study of intelligent agents, drawn from leading AI textbooks.
Poole, Mackworth & Goebel (1998, p. 1), which provides the version that is used in this article. These authors use the term ""computational intelligence"" as a synonym for artificial intelligence.
Russell & Norvig (2003, p. 55) (who prefer the term ""rational agent"") and write ""The whole-agent view is now widely accepted in the field"".
Nilsson (1998)
Legg & Hutter (2007)

^ Stuart Russell and Peter Norvig characterize this definition as ""thinking humanly"" and reject it in favor of ""acting rationally"".[1]

^ a b This list of intelligent traits is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2003), Luger & Stubblefield (2004), Poole, Mackworth & Goebel (1998) and Nilsson (1998)

^ This statement comes from the proposal for the Dartmouth workshop of 1956, which reads: ""Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.""[13]

^ 
Daniel Crevier wrote ""the conference is generally recognized as the official birthdate of the new science.""[25] Russell and Norvifg call the conference ""the birth of artificial intelligence.""[26]

^ 
Russell and Norvig wrote ""for the next 20 years the field would be dominated by these people and their students.""[26]

^ 
Russell and Norvig wrote ""it was astonishing whenever a computer did anything kind of smartish"".[28]

^ 
The programs described are Arthur Samuel's checkers program for the IBM 701, Daniel Bobrow's STUDENT, Newell and Simon's Logic Theorist and Terry Winograd's SHRDLU.

^ 
Embodied approaches to AI[38] were championed by Hans Moravec[39] and Rodney Brooks[40] and went by many names: Nouvelle AI,[40] Developmental robotics,[41]
situated AI, behavior-based AI as well as others. A similar movement in cognitive science was the embodied mind thesis.

^ 
Clark wrote: ""After a half-decade of quiet breakthroughs in artificial intelligence, 2015 has been a landmark year. Computers are smarter and learning faster than ever.""[10]

^ Alan Turing discussed the centrality of learning as early as 1950, in his classic paper ""Computing Machinery and Intelligence"".[68] In 1956, at the original Dartmouth AI summer conference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning: ""An Inductive Inference Machine"".[69]

^ This is a form of Tom Mitchell's widely quoted definition of machine learning: ""A computer program is set to learn from an experience E with respect to some task T and some performance measure P if its performance on T as measured by P improves with experience E.""[70]

^ 
Alan Turing suggested in ""Computing Machinery and Intelligence"" that a ""thinking machine"" would need to be educated like a child.[68] Developmental robotics is a modern version of the idea.[41]

^ 
Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve.[111]

^ Expectation-maximization, one of the most popular algorithms in machine learning, allows clustering in the presence of unknown latent variables.[113]

^ 
The Smithsonian reports: ""Pluribus has bested poker pros in a series of six-player no-limit Texas Hold'em games, reaching a milestone in artificial intelligence research. It is the first bot to beat humans in a complex multiplayer competition.""[155]

^ 
The distinction between ""acting"" and ""thinking"" is due to Russell and Norvig.[167]

^ 
The distinction between ""acting humanly"" and ""acting rationally"" is due to Russell and Norvig.[167] Pamela McCorduck wrote in 2004 that there are ""two major branches of artificial intelligence: one aimed at producing intelligent behavior regardless of how it was accomplished, and the other aimed at modeling intelligent processes found in nature, particularly human ones.""[169]

^ Nils Nilsson wrote in 1983: ""Simply put, there is wide disagreement in the field about what AI is all about.""[172]

^ 
Daniel Crevier wrote that ""time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.""[177]

^ 
Searle presented this definition of ""Strong AI"" in 1999.[188] Searle's original formulation was ""The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.""[189] Strong AI is defined similarly by Russell and Norvig: ""The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis.""[184]

^ See table 4; 9% is both the OECD average and the US average.[202]

^ Rodney Brooks writes, ""I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI and the enormity and complexity of building sentient volitional intelligence.""[221]


Citations


^ Russell & Norvig (2009), p. 2.

^ Google (2016).

^ McCorduck (2004), p. 204.

^ Ashok83 (2019).

^ Schank (1991), p. 38.

^ Crevier (1993), p. 109.

^ a b c 
Funding initiatives in the early 80s: Fifth Generation Project (Japan), Alvey (UK), Microelectronics and Computer Technology Corporation (US), Strategic Computing Initiative (US):
McCorduck (2004, pp. 426–441)
Crevier (1993, pp. 161–162, 197–203, 211, 240)
Russell & Norvig (2003, p. 24)
NRC (1999, pp. 210–211)
Newquist (1994, pp. 235–248)

^ a b 
First AI Winter, Lighthill report, Mansfield Amendment
Crevier (1993, pp. 115–117)
Russell & Norvig (2003, p. 22)
NRC (1999, pp. 212–213)
Howe (1994)
Newquist (1994, pp. 189–201)

^ a b 
Second AI Winter:
McCorduck (2004, pp. 430–435)
Crevier (1993, pp. 209–210)
NRC (1999, pp. 214–216)
Newquist (1994, pp. 301–318)

^ a b c d Clark (2015b).

^ a b 
AI widely used in late 1990s:
Russell & Norvig (2003, p. 28)
Kurzweil (2005, p. 265)
NRC (1999, pp. 216–222)
Newquist (1994, pp. 189–201)

^ a b 
Pennachin & Goertzel (2007); Roberts (2016)

^ McCarthy et al. (1955).

^ Newquist (1994), pp. 45–53.

^ Spadafora (2016).

^ Lombardo, Boehm & Nairz (2020).

^ a b 
AI in myth:
McCorduck (2004, pp. 4–5)
Russell & Norvig (2003, p. 939)

^ McCorduck (2004), pp. 17–25.

^ a b McCorduck (2004), pp. 340–400.

^ Berlinski (2000).

^ 
AI's immediate precursors:
McCorduck (2004, pp. 51–107)
Crevier (1993, pp. 27–32)
Russell & Norvig (2003, pp. 15, 940)
Moravec (1988, p. 3)

^ Russell & Norvig (2009), p. 16.

^ Manyika 2022, p. 9.

^ Manyika 2022, p. 10.

^ Crevier (1993), pp. 47–49.

^ a b Russell & Norvig (2003), p. 17.

^ 
Dartmouth workshop:
Russell & Norvig (2003, p. 17)
McCorduck (2004, pp. 111–136)
NRC (1999, pp. 200–201)
The proposal:
McCarthy et al. (1955)

^ Russell & Norvig (2003), p. 18.

^ 
Successful Symbolic AI programs:
McCorduck (2004, pp. 243–252)
Crevier (1993, pp. 52–107)
Moravec (1988, p. 9)
Russell & Norvig (2003, pp. 18–21)

^ 
AI heavily funded in 1960s:
McCorduck (2004, p. 131)
Crevier (1993, pp. 51, 64–65)
NRC (1999, pp. 204–205)

^ Howe (1994).

^ Newquist (1994), pp. 86–86.

^ 
Simon (1965, p. 96) quoted in Crevier (1993, p. 109)

^ 
Minsky (1967, p. 2) quoted in Crevier (1993, p. 109)

^ Lighthill (1973).

^ 
Expert systems:
Russell & Norvig (2003, pp. 22–24)
Luger & Stubblefield (2004, pp. 227–331)
Nilsson (1998, chpt. 17.4)
McCorduck (2004, pp. 327–335, 434–435)
Crevier (1993, pp. 145–62, 197–203)
Newquist (1994, pp. 155–183)

^ Nilsson (1998), p. 7.

^ McCorduck (2004), pp. 454–462.

^ Moravec (1988).

^ a b Brooks (1990).

^ a b 
Developmental robotics:
Weng et al. (2001)
Lungarella et al. (2003)
Asada et al. (2009)
Oudeyer (2010)

^ 
Revival of connectionism:
Crevier (1993, pp. 214–215)
Russell & Norvig (2003, p. 25)

^ 
Formal and narrow methods adopted in the 1990s:
Russell & Norvig (2003, pp. 25–26)
McCorduck (2004, pp. 486–487)

^ McKinsey (2018).

^ MIT Sloan Management Review (2018); Lorica (2017)

^ a b c d UNESCO (2021).

^ 
Problem solving, puzzle solving, game playing and deduction:
Russell & Norvig (2003, chpt. 3–9)
Poole, Mackworth & Goebel (1998, chpt. 2,3,7,9)
Luger & Stubblefield (2004, chpt. 3,4,6,8)
Nilsson (1998, chpt. 7–12)

^ 
Uncertain reasoning:
Russell & Norvig (2003, pp. 452–644)
Poole, Mackworth & Goebel (1998, pp. 345–395)
Luger & Stubblefield (2004, pp. 333–381)
Nilsson (1998, chpt. 19)

^ a b c 
Intractability and efficiency and the combinatorial explosion:
Russell & Norvig (2003, pp. 9, 21–22)

^ a b c 
Psychological evidence of the prevalence sub-symbolic reasoning and knowledge:
Kahneman (2011)
Wason & Shapiro (1966)
Kahneman, Slovic & Tversky (1982)
Dreyfus & Dreyfus (1986)

^ 
Knowledge representation and knowledge engineering:
Russell & Norvig (2003, pp. 260–266, 320–363)
Poole, Mackworth & Goebel (1998, pp. 23–46, 69–81, 169–233, 235–277, 281–298, 319–345)
Luger & Stubblefield (2004, pp. 227–243),
Nilsson (1998, chpt. 17.1–17.4, 18)

^ Russell & Norvig (2003), pp. 320–328.

^ a b c 
Representing categories and relations: Semantic networks, description logics, inheritance (including frames and scripts):
Russell & Norvig (2003, pp. 349–354),
Poole, Mackworth & Goebel (1998, pp. 174–177),
Luger & Stubblefield (2004, pp. 248–258),
Nilsson (1998, chpt. 18.3)

^ a b Representing events and time:Situation calculus, event calculus, fluent calculus (including solving the frame problem):
Russell & Norvig (2003, pp. 328–341),
Poole, Mackworth & Goebel (1998, pp. 281–298),
Nilsson (1998, chpt. 18.2)

^ a b 
Causal calculus:
Poole, Mackworth & Goebel (1998, pp. 335–337)

^ a b 
Representing knowledge about knowledge: Belief calculus, modal logics:
Russell & Norvig (2003, pp. 341–344),
Poole, Mackworth & Goebel (1998, pp. 275–277)

^ a b 
Default reasoning, Frame problem, default logic, non-monotonic logics, circumscription, closed world assumption, abduction:
Russell & Norvig (2003, pp. 354–360)
Poole, Mackworth & Goebel (1998, pp. 248–256, 323–335)
Luger & Stubblefield (2004, pp. 335–363)
Nilsson (1998, ~18.3.3)
(Poole et al. places abduction under ""default reasoning"". Luger et al. places this under ""uncertain reasoning"").

^ a b 
Breadth of commonsense knowledge:
Russell & Norvig (2003, p. 21),
Crevier (1993, pp. 113–114),
Moravec (1988, p. 13),
Lenat & Guha (1989, Introduction)

^ Smoliar & Zhang (1994).

^ Neumann & Möller (2008).

^ Kuperman, Reichley & Bailey (2006).

^ McGarry (2005).

^ Bertini, Del Bimbo & Torniai (2006).

^ 
Planning:
Russell & Norvig (2003, pp. 375–459)
Poole, Mackworth & Goebel (1998, pp. 281–316)
Luger & Stubblefield (2004, pp. 314–329)
Nilsson (1998, chpt. 10.1–2, 22)
Information value theory:
Russell & Norvig (2003, pp. 600–604)

^ 
Classical planning:
Russell & Norvig (2003, pp. 375–430)
Poole, Mackworth & Goebel (1998, pp. 281–315)
Luger & Stubblefield (2004, pp. 314–329)
Nilsson (1998, chpt. 10.1–2, 22)

^ 
Planning and acting in non-deterministic domains: conditional planning, execution monitoring, replanning and continuous planning:
Russell & Norvig (2003, pp. 430–449)

^ 
Multi-agent planning and emergent behavior:
Russell & Norvig (2003, pp. 449–455)

^ a b Turing (1950). sfnp error: no target: CITEREFTuring1950 (help)

^ Solomonoff (1956).

^ Russell & Norvig (2003), pp. 649–788.

^ 
Learning:
Russell & Norvig (2003, pp. 649–788)
Poole, Mackworth & Goebel (1998, pp. 397–438)
Luger & Stubblefield (2004, pp. 385–542)
Nilsson (1998, chpt. 3.3, 10.3, 17.5, 20)

^ 
Reinforcement learning:
Russell & Norvig (2003, pp. 763–788)
Luger & Stubblefield (2004, pp. 442–449)

^ The Economist (2016).

^ Jordan & Mitchell (2015).

^ 
Natural language processing (NLP):
Russell & Norvig (2003, pp. 790–831)
Poole, Mackworth & Goebel (1998, pp. 91–104)
Luger & Stubblefield (2004, pp. 591–632)

^ 
Applications of NLP:
Russell & Norvig (2003, pp. 840–857)
Luger & Stubblefield (2004, pp. 623–630)

^ Modern statistical approaches to NLP:
Cambria & White (2014)

^ Vincent (2019).

^ 
Machine perception:
Russell & Norvig (2003, pp. 537–581, 863–898)
Nilsson (1998, ~chpt. 6)

^ 
Speech recognition:
Russell & Norvig (2003, pp. 568–578)

^ 
Object recognition:
Russell & Norvig (2003, pp. 885–892)

^ 
Computer vision:
Russell & Norvig (2003, pp. 863–898)
Nilsson (1998, chpt. 6)

^ 
Robotics:
Russell & Norvig (2003, pp. 901–942)
Poole, Mackworth & Goebel (1998, pp. 443–460)

^ 
Robotic mapping and Localization:
Russell & Norvig (2003, pp. 908–915)
Cadena et al. (2016)

^ Motion planning and configuration space:
Russell & Norvig (2003, pp. 916–932)
Tecuci (2012)

^ MIT AIL (2014).

^ 
Affective computing:
Thro (1993)
Edelson (1991)
Tao & Tan (2005)
Scassellati (2002)

^ Waddell (2018).

^ Poria et al. (2017).

^ 
The Society of Mind:
Minsky (1986)
Moravec's ""golden spike"":
Moravec (1988, p. 20)
Multi-agent systems, hybrid intelligent systems, agent architectures, cognitive architecture:
Russell & Norvig (2003, pp. 27, 932, 970–972)
Nilsson (1998, chpt. 25)

^ Domingos (2015), Chpt. 9.

^ 
Artificial brain as an approach to AGI: 
Russell & Norvig (2003, p. 957)
Crevier (1993, pp. 271 & 279)
Goertzel et al. (2010)
A few of the people who make some form of the argument: 
Moravec (1988, p. 20)
Kurzweil (2005, p. 262)
Hawkins & Blakeslee (2005)

^ 
Search algorithms:
Russell & Norvig (2003, pp. 59–189)
Poole, Mackworth & Goebel (1998, pp. 113–163)
Luger & Stubblefield (2004, pp. 79–164, 193–219)
Nilsson (1998, chpt. 7–12)

^ 
Forward chaining, backward chaining, Horn clauses, and logical deduction as search:
Russell & Norvig (2003, pp. 217–225, 280–294)
Poole, Mackworth & Goebel (1998, pp. ~46–52)
Luger & Stubblefield (2004, pp. 62–73)
Nilsson (1998, chpt. 4.2, 7.2)

^ 
State space search and planning:
Russell & Norvig (2003, pp. 382–387)
Poole, Mackworth & Goebel (1998, pp. 298–305)
Nilsson (1998, chpt. 10.1–2)

^ Moving and configuration space:
Russell & Norvig (2003, pp. 916–932)

^ Uninformed searches (breadth first search, depth first search and general state space search):
Russell & Norvig (2003, pp. 59–93)
Poole, Mackworth & Goebel (1998, pp. 113–132)
Luger & Stubblefield (2004, pp. 79–121)
Nilsson (1998, chpt. 8)

^ 
Heuristic or informed searches (e.g., greedy best first and A*):
Russell & Norvig (2003, pp. 94–109)
Poole, Mackworth & Goebel (1998, pp. pp. 132–147)
Poole & Mackworth (2017, Section 3.6)
Luger & Stubblefield (2004, pp. 133–150)

^ Tecuci (2012).

^ Optimization searches:
Russell & Norvig (2003, pp. 110–116, 120–129)
Poole, Mackworth & Goebel (1998, pp. 56–163)
Luger & Stubblefield (2004, pp. 127–133)

^ 
Genetic programming and genetic algorithms:
Luger & Stubblefield (2004, pp. 509–530)
Nilsson (1998, chpt. 4.2)

^ 
Artificial life and society based learning:
Luger & Stubblefield (2004, pp. 530–541)
Merkle & Middendorf (2013)

^ 
Logic:
Russell & Norvig (2003, pp. 194–310),
Luger & Stubblefield (2004, pp. 35–77),
Nilsson (1998, chpt. 13–16)

^ 
Satplan:
Russell & Norvig (2003, pp. 402–407),
Poole, Mackworth & Goebel (1998, pp. 300–301),
Nilsson (1998, chpt. 21)

^ 
Explanation based learning, relevance based learning, inductive logic programming, case based reasoning:
Russell & Norvig (2003, pp. 678–710),
Poole, Mackworth & Goebel (1998, pp. 414–416),
Luger & Stubblefield (2004, pp. ~422–442),
Nilsson (1998, chpt. 10.3, 17.5)

^ 
Propositional logic:
Russell & Norvig (2003, pp. 204–233),
Luger & Stubblefield (2004, pp. 45–50)
Nilsson (1998, chpt. 13)

^ First-order logic and features such as equality:
Russell & Norvig (2003, pp. 240–310),
Poole, Mackworth & Goebel (1998, pp. 268–275),
Luger & Stubblefield (2004, pp. 50–62),
Nilsson (1998, chpt. 15)

^ 
Fuzzy logic:
Russell & Norvig (2003, pp. 526–527)
Scientific American (1999)

^ 
Stochastic methods for uncertain reasoning:
Russell & Norvig (2003, pp. 462–644),
Poole, Mackworth & Goebel (1998, pp. 345–395),
Luger & Stubblefield (2004, pp. 165–191, 333–381),
Nilsson (1998, chpt. 19)

^ 
Bayesian networks:
Russell & Norvig (2003, pp. 492–523),
Poole, Mackworth & Goebel (1998, pp. 361–381),
Luger & Stubblefield (2004, pp. ~182–190, ≈363–379),
Nilsson (1998, chpt. 19.3–4)

^ Domingos (2015), chapter 6.

^ 
Bayesian inference algorithm:
Russell & Norvig (2003, pp. 504–519),
Poole, Mackworth & Goebel (1998, pp. 361–381),
Luger & Stubblefield (2004, pp. ~363–379),
Nilsson (1998, chpt. 19.4 & 7)

^ Domingos (2015), p. 210.

^ 
Bayesian learning and the expectation-maximization algorithm:
Russell & Norvig (2003, pp. 712–724),
Poole, Mackworth & Goebel (1998, pp. 424–433),
Nilsson (1998, chpt. 20)
Domingos (2015, p. 210)

^ Bayesian decision theory and Bayesian decision networks:
Russell & Norvig (2003, pp. 597–600)

^ a b c Stochastic temporal models:
Russell & Norvig (2003, pp. 537–581)
Dynamic Bayesian networks:
Russell & Norvig (2003, pp. 551–557)
Hidden Markov model:
(Russell & Norvig 2003, pp. 549–551)
Kalman filters:
Russell & Norvig (2003, pp. 551–557)

^ 
decision theory and decision analysis:
Russell & Norvig (2003, pp. 584–597),
Poole, Mackworth & Goebel (1998, pp. 381–394)

^ 
Information value theory:
Russell & Norvig (2003, pp. 600–604)

^ Markov decision processes and dynamic decision networks:
Russell & Norvig (2003, pp. 613–631)

^ Game theory and mechanism design:
Russell & Norvig (2003, pp. 631–643)

^ 
Statistical learning methods and classifiers:
Russell & Norvig (2003, pp. 712–754),
Luger & Stubblefield (2004, pp. 453–541)

^ 
Decision tree:
Domingos (2015, p. 88)
Russell & Norvig (2003, pp. 653–664),
Poole, Mackworth & Goebel (1998, pp. 403–408),
Luger & Stubblefield (2004, pp. 408–417)

^ 
K-nearest neighbor algorithm:
Domingos (2015, p. 187)
Russell & Norvig (2003, pp. 733–736)

^ 
kernel methods such as the support vector machine:
Domingos (2015, p. 88)
Russell & Norvig (2003, pp. 749–752)
Gaussian mixture model:
Russell & Norvig (2003, pp. 725–727)

^ Domingos (2015), p. 152.

^ 
Naive Bayes classifier:
Domingos (2015, p. 152)
Russell & Norvig (2003, p. 718)

^ a b 
Neural networks:
Russell & Norvig (2003, pp. 736–748),
Poole, Mackworth & Goebel (1998, pp. 408–414),
Luger & Stubblefield (2004, pp. 453–505),
Nilsson (1998, chpt. 3)
Domingos (2015, Chapter 4)

^ 
Classifier performance:
van der Walt & Bernard (2006)
Russell & Norvig (2009, 18.12: Learning from Examples: Summary)

^ 
Backpropagation:
Russell & Norvig (2003, pp. 744–748),
Luger & Stubblefield (2004, pp. 467–474),
Nilsson (1998, chpt. 3.3)
Paul Werbos' introduction of backpropagation to AI:
Werbos (1974); Werbos (1982)
Automatic differentiation, an essential precursor:
Linnainmaa (1970); Griewank (2012)

^ 
Competitive learning, Hebbian coincidence learning, Hopfield networks and attractor networks:
Luger & Stubblefield (2004, pp. 474–505)

^ 
Feedforward neural networks, perceptrons and radial basis networks:
Russell & Norvig (2003, pp. 739–748, 758)
Luger & Stubblefield (2004, pp. 458–467)

^ Schulz & Behnke (2012).

^ 
Deep learning:
Goodfellow, Bengio & Courville (2016)
Hinton et al. (2016)
Schmidhuber (2015)

^ Deng & Yu (2014), pp. 199–200.

^ Ciresan, Meier & Schmidhuber (2012).

^ Habibi (2017).

^ Fukushima (2007).

^ 
Recurrent neural networks, Hopfield nets:
Russell & Norvig (2003, p. 758)
Luger & Stubblefield (2004, pp. 474–505)
Schmidhuber (2015)

^ Schmidhuber (2015).

^ 
Werbos (1988); 
Robinson & Fallside (1987);
Williams & Zipser (1994)

^ 
Goodfellow, Bengio & Courville (2016);
Hochreiter (1991)

^ Hochreiter & Schmidhuber (1997); Gers, Schraudolph & Schraudolph (2002)

^ Russell & Norvig (2009), p. 1.

^ European Commission (2020), p. 1.

^ CNN (2006).

^ 
Targeted advertising:
Russell & Norvig (2009, p. 1)
Economist (2016)
Lohr (2016)

^ Lohr (2016).

^ Smith (2016).

^ Rowinski (2013).

^ Frangoul (2019).

^ Brown (2019).

^ McCorduck (2004), pp. 480–483.

^ Markoff (2011).

^ 
Google (2016); BBC (2016)

^ Solly (2019).

^ Bowling et al. (2015).

^ Sample (2017).

^ Anadiotis (2020).

^ Heath (2020).

^ Aletras et al. (2016).

^ ""Intellectual Property and Frontier Technologies"". WIPO.

^ ""WIPO Technology Trends 2019 - Artificial Intelligence"" (PDF). WIPO. 2019.

^ ""WIPO Technology Trends 2019 - Artificial Intelligence"" (PDF). WIPO. 2019.

^ Turing (1950), p. 1. sfnp error: no target: CITEREFTuring1950 (help)

^ Turing (1948).

^ 
Turing's original publication of the Turing test in ""Computing machinery and intelligence"":
Turing (1950) harvtxt error: no target: CITEREFTuring1950 (help)
Historical influence and philosophical implications:
Haugeland (1985, pp. 6–9)
Crevier (1993, p. 24)
McCorduck (2004, pp. 70–71)
Russell & Norvig (2003, pp. 2–3 and 948)

^ a b c Russell & Norvig (2003), p. 3.

^ Maker (2006).

^ McCorduck (2004), pp. 100–101.

^ 
The intelligent agent paradigm:
Russell & Norvig (2003, pp. 27, 32–58, 968–972)
Poole, Mackworth & Goebel (1998, pp. 7–21)
Luger & Stubblefield (2004, pp. 235–240)
Hutter (2005, pp. 125–126)
The definition used in this article, in terms of goals, actions, perception and environment, is due to Russell & Norvig (2003). Other definitions also include knowledge, learning and autonomy as additional criteria.

^ Russell & Norvig (2003), p. 27.

^ Nilsson (1983), p. 10.

^ Haugeland (1985), pp. 112–117.

^ 
Physical symbol system hypothesis:
Newell & Simon (1976, p. 116)
Historical significance:
McCorduck (2004, p. 153)
Russell & Norvig (2003, p. 18)

^ 
Moravec's paradox:
Moravec (1988, pp. 15–16)
Minsky (1986, p. 29)
Pinker (2007, pp. 190–91)

^ 
Dreyfus' critique of AI:
Dreyfus (1972)
Dreyfus & Dreyfus (1986)
Historical significance and philosophical implications:
Crevier (1993, pp. 120–132)
McCorduck (2004, pp. 211–239)
Russell & Norvig (2003, pp. 950–952)
Fearn (2007, Chpt. 3)

^ Crevier (1993), p. 125.

^ Langley (2011).

^ Katz (2012).

^ 
Neats vs. scruffies, the historic debate:
McCorduck (2004, pp. 421–424, 486–489)
Crevier (1993, p. 168)
Nilsson (1983, pp. 10–11)
A classic example of the ""scruffy"" approach to intelligence:
Minsky (1986)
A modern example of neat AI and its aspirations:
Domingos (2015)

^ Russell & Norvig (2003), p. 25-26.

^ Pennachin & Goertzel (2007).

^ a b Roberts (2016).

^ a b Russell & Norvig (2003), p. 947.

^ Chalmers (1995).

^ Dennett (1991).

^ Horst (2005).

^ Searle (1999).

^ Searle (1980), p. 1.

^ 
Searle's Chinese room argument:
Searle (1980). Searle's original presentation of the thought experiment.
Searle (1999).
Discussion:
Russell & Norvig (2003, pp. 958–960)
McCorduck (2004, pp. 443–445)
Crevier (1993, pp. 269–271)

^ 
Robot rights:
Russell & Norvig (2003, p. 964)
BBC (2006)
Maschafilm (2010) (the film Plug & Pray)

^ Evans (2015).

^ McCorduck (2004), pp. 19–25.

^ Henderson (2007).

^ Omohundro (2008).

^ Vinge (1993).

^ Russell & Norvig (2003), p. 963.

^ 
Transhumanism:
Moravec (1988)
Kurzweil (2005)
Russell & Norvig (2003, p. 963)

^ 
AI as evolution:
Edward Fredkin is quoted in McCorduck (2004, p. 401)
Butler (1863)
Dyson (1998)

^ 
Ford & Colvin (2015);
McGaughey (2018)

^ IGM Chicago (2017).

^ Arntz, Gregory & Zierahn (2016), p. 33.

^ 
Lohr (2017);
Frey & Osborne (2017);
Arntz, Gregory & Zierahn (2016, p. 33)

^ Morgenstern (2015).

^ Mahdawi (2017); Thompson (2014)

^ Harari (2018).

^ 
Weaponized AI:
Robitzski (2018)
Sainato (2015)

^ Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (7 March 2022). ""Dual use of artificial-intelligence-powered drug discovery"". Nature Machine Intelligence. 4 (3): 189–191. doi:10.1038/s42256-022-00465-9. S2CID 247302391. Retrieved 15 March 2022.

^ CNA (2019).

^ Goffrey (2008), p. 17.

^ Lipartito (2011, p. 36); Goodman & Flaxman (2017, p. 6)

^ Larson & Angwin (2016).

^ Cellan-Jones (2014).

^ Bostrom (2014); Müller & Bostrom (2014); Bostrom (2015)

^ Rubin (2003).

^ Müller & Bostrom (2014).

^ 
Leaders' concerns about the existential risks of AI:
Rawlinson (2015)
Holley (2015)
Gibbs (2014)
Churm (2019)
Sainato (2015)

^ 
Funding to mitigate risks of AI:
Post (2015)
Del Prado (2015)
Clark (2015a)
FastCompany (2015)

^ 
Leaders who argue the benefits of AI outweigh the risks:
Thibodeau (2019)
Bhardwaj (2018)

^ 
Arguments that AI is not an imminent risk:
Brooks (2014)
Geist (2015)
Madrigal (2015)
Lee (2014)

^ Brooks (2014).

^ Yudkowsky (2008).

^ a b Anderson & Anderson (2011).

^ AAAI (2014).

^ Wallach (2010).

^ Russell (2019), p. 173.

^ 
Regulation of AI to mitigate risks:
Berryhill et al. (2019)
Barfield & Pagallo (2018)
Iphofen & Kritikos (2019)
Wirtz, Weyerer & Geyer (2018)
Buiten (2019)

^ Law Library of Congress (U.S.). Global Legal Research Directorate (2019).

^ Kissinger, Henry (1 November 2021). ""The Challenge of Being Human in the Age of AI"". The Wall Street Journal.

^ Buttazzo (2001).

^ Anderson (2008).

^ McCauley (2007).

^ Galvan (1997).


References
AI textbooks
These were the four the most widely used AI textbooks in 2008.


Luger, George; Stubblefield, William (2004). Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th ed.). Benjamin/Cummings. ISBN 978-0-8053-4780-7. Archived from the original on 26 July 2020. Retrieved 17 December 2019.
Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Later editions.

Russell, Stuart J.; Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, New Jersey: Prentice Hall. ISBN 978-0-13-604259-4..
Poole, David; Mackworth, Alan (2017). Artificial Intelligence: Foundations of Computational Agents (2nd ed.). Cambridge University Press. ISBN 978-1-107-19539-4.
The two most widely used textbooks in 2021.[1]

Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 9780134610993. LCCN 20190474.
Knight, Kevin; Rich, Elaine (1 January 2010). Artificial Intelligence (3rd ed.). Mc Graw Hill India. ISBN 9780070087705.

History of AI

Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3..
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 1-56881-205-1.
Newquist, HP (1994). The Brain Makers: Genius, Ego, And Greed In The Quest For Machines That Think. New York: Macmillan/SAMS. ISBN 978-0-672-30412-5.
Nilsson, Nils (2009). The Quest for Artificial Intelligence: A History of Ideas and Achievements. New York: Cambridge University Press. ISBN 978-0-521-12293-1.

Other sources
Werbos, P. J. (1988), ""Generalization of backpropagation with application to a recurrent gas market model"", Neural Networks, 1 (4): 339–356, doi:10.1016/0893-6080(88)90007-X
Gers, Felix A.; Schraudolph, Nicol N.; Schraudolph, Jürgen (2002). ""Learning Precise Timing with LSTM Recurrent Networks"" (PDF). Journal of Machine Learning Research. 3: 115–143. Retrieved 13 June 2017.
Deng, L.; Yu, D. (2014). ""Deep Learning: Methods and Applications"" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039. Archived (PDF) from the original on 14 March 2016. Retrieved 18 October 2014.
Schulz, Hannes; Behnke, Sven (1 November 2012). ""Deep Learning"". KI - Künstliche Intelligenz. 26 (4): 357–363. doi:10.1007/s13218-012-0198-z. ISSN 1610-1987. S2CID 220523562.
Fukushima, K. (2007). ""Neocognitron"". Scholarpedia. 2 (1): 1717. Bibcode:2007SchpJ...2.1717F. doi:10.4249/scholarpedia.1717. was introduced by Kunihiko Fukushima in 1980.
Habibi, Aghdam, Hamed (30 May 2017). Guide to convolutional neural networks : a practical application to traffic-sign detection and classification. Heravi, Elnaz Jahani. Cham, Switzerland. ISBN 9783319575490. OCLC 987790957.
Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). ""Multi-column deep neural networks for image classification"". 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8. S2CID 2161592.
""From not working to neural networking"". The Economist. 2016. Archived from the original on 31 December 2016. Retrieved 26 April 2018.
Thompson, Derek (23 January 2014). ""What Jobs Will the Robots Take?"". The Atlantic. Archived from the original on 24 April 2018. Retrieved 24 April 2018.
Scassellati, Brian (2002). ""Theory of mind for a humanoid robot"". Autonomous Robots. 12 (1): 13–24. doi:10.1023/A:1013298507114. S2CID 1979315.
Sample, Ian (14 March 2017). ""Google's DeepMind makes AI program that can learn like a human"". The Guardian. Archived from the original on 26 April 2018. Retrieved 26 April 2018.
Heath, Nick (11 December 2020). ""What is AI? Everything you need to know about Artificial Intelligence"". ZDNet. Retrieved 1 March 2021.
Bowling, Michael; Burch, Neil; Johanson, Michael; Tammelin, Oskari (9 January 2015). ""Heads-up limit hold'em poker is solved"". Science. 347 (6218): 145–149. Bibcode:2015Sci...347..145B. doi:10.1126/science.1259433. ISSN 0036-8075. PMID 25574016. S2CID 3796371.
Solly, Meilan (15 July 2019). ""This Poker-Playing A.I. Knows When to Hold 'Em and When to Fold 'Em"". Smithsonian.
""Artificial intelligence: Google's AlphaGo beats Go master Lee Se-dol"". BBC News. 12 March 2016. Archived from the original on 26 August 2016. Retrieved 1 October 2016.
Rowinski, Dan (15 January 2013). ""Virtual Personal Assistants & The Future Of Your Smartphone [Infographic]"". ReadWrite. Archived from the original on 22 December 2015.
Manyika, James (2022). ""Getting AI Right: Introductory Notes on AI & Society"". Daedalus. Retrieved 5 May 2022.
Markoff, John (16 February 2011). ""Computer Wins on 'Jeopardy!': Trivial, It's Not"". The New York Times. Archived from the original on 22 October 2014. Retrieved 25 October 2014.
Anadiotis, George (1 October 2020). ""The state of AI in 2020: Democratization, industrialization, and the way to artificial general intelligence"". ZDNet. Retrieved 1 March 2021.
Goertzel, Ben; Lian, Ruiting; Arel, Itamar; de Garis, Hugo; Chen, Shuo (December 2010). ""A world survey of artificial brain projects, Part II: Biologically inspired cognitive architectures"". Neurocomputing. 74 (1–3): 30–49. doi:10.1016/j.neucom.2010.08.012.
Robinson, A. J.; Fallside, F. (1987), ""The utility driven dynamic error propagation network."", Technical Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department
Hochreiter, Sepp (1991). Untersuchungen zu dynamischen neuronalen Netzen (PDF) (diploma thesis). Munich: Institut f. Informatik, Technische Univ. Archived from the original (PDF) on 6 March 2015. Retrieved 16 April 2016.
Williams, R. J.; Zipser, D. (1994), ""Gradient-based learning algorithms for recurrent networks and their computational complexity"", Back-propagation: Theory, Architectures and Applications, Hillsdale, NJ: Erlbaum
Hochreiter, Sepp; Schmidhuber, Jürgen (1997), ""Long Short-Term Memory"", Neural Computation, 9 (8): 1735–1780, doi:10.1162/neco.1997.9.8.1735, PMID 9377276, S2CID 1915014
Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning, MIT Press., archived from the original on 16 April 2016, retrieved 12 November 2017
Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). ""Deep Neural Networks for Acoustic Modeling in Speech Recognition – The shared views of four research groups"". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/msp.2012.2205597. S2CID 206485943.
Schmidhuber, J. (2015). ""Deep Learning in Neural Networks: An Overview"". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509.
Linnainmaa, Seppo (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors (Thesis) (in Finnish). Univ. Helsinki, 6–7.|
Griewank, Andreas (2012). ""Who Invented the Reverse Mode of Differentiation? Optimization Stories"". Documenta Matematica, Extra Volume ISMP: 389–400.
Werbos, Paul (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences (Ph.D. thesis). Harvard University.
Werbos, Paul (1982). ""Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences"" (PDF). System Modeling and Optimization. Applications of advances in nonlinear sensitivity analysis. Berlin, Heidelberg: Springer. Archived from the original (PDF) on 14 April 2016. Retrieved 16 April 2016.
""What is 'fuzzy logic'? Are there computers that are inherently fuzzy and do not apply the usual binary logic?"". Scientific American. 21 October 1999. Retrieved 5 May 2018.
Merkle, Daniel; Middendorf, Martin (2013). ""Swarm Intelligence"".  In Burke, Edmund K.; Kendall, Graham (eds.). Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques. Springer Science & Business Media. ISBN 978-1-4614-6940-7.
van der Walt, Christiaan; Bernard, Etienne (2006). ""Data characteristics that determine classifier performance"" (PDF). Archived from the original (PDF) on 25 March 2009. Retrieved 5 August 2009.
Hutter, Marcus (2005). Universal Artificial Intelligence. Berlin: Springer. ISBN 978-3-540-22139-5.
Howe, J. (November 1994). ""Artificial Intelligence at Edinburgh University: a Perspective"". Archived from the original on 15 May 2007. Retrieved 30 August 2007.
Galvan, Jill (1 January 1997). ""Entering the Posthuman Collective in Philip K. Dick's ""Do Androids Dream of Electric Sheep?"""". Science Fiction Studies. 24 (3): 413–429. JSTOR 4240644.
McCauley, Lee (2007). ""AI armageddon and the three laws of robotics"". Ethics and Information Technology. 9 (2): 153–164. CiteSeerX 10.1.1.85.8904. doi:10.1007/s10676-007-9138-2. S2CID 37272949.
Buttazzo, G. (July 2001). ""Artificial consciousness: Utopia or real possibility?"". Computer. 34 (7): 24–30. doi:10.1109/2.933500.
Anderson, Susan Leigh (2008). ""Asimov's ""three laws of robotics"" and machine metaethics"". AI & Society. 22 (4): 477–493. doi:10.1007/s00146-007-0094-5. S2CID 1809459.
Yudkowsky, E (2008), ""Artificial Intelligence as a Positive and Negative Factor in Global Risk"" (PDF), Global Catastrophic Risks, Oxford University Press, 2008, Bibcode:2008gcr..book..303Y
McGaughey, E (2018), Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy, p. SSRN part 2(3), SSRN 3044448, archived from the original on 24 May 2018, retrieved 12 January 2018
IGM Chicago (30 June 2017). ""Robots and Artificial Intelligence"". www.igmchicago.org. Archived from the original on 1 May 2019. Retrieved 3 July 2019.
Lohr, Steve (2017). ""Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says"". The New York Times. Archived from the original on 14 January 2018. Retrieved 13 January 2018.
Frey, Carl Benedikt; Osborne, Michael A (1 January 2017). ""The future of employment: How susceptible are jobs to computerisation?"". Technological Forecasting and Social Change. 114: 254–280. CiteSeerX 10.1.1.395.416. doi:10.1016/j.techfore.2016.08.019. ISSN 0040-1625.
Arntz, Melanie; Gregory, Terry; Zierahn, Ulrich (2016), ""The risk of automation for jobs in OECD countries: A comparative analysis"", OECD Social, Employment, and Migration Working Papers 189
Morgenstern, Michael (9 May 2015). ""Automation and anxiety"". The Economist. Archived from the original on 12 January 2018. Retrieved 13 January 2018.
Mahdawi, Arwa (26 June 2017). ""What jobs will still be around in 20 years? Read this to prepare your future"". The Guardian. Archived from the original on 14 January 2018. Retrieved 13 January 2018.
Rubin, Charles (Spring 2003). ""Artificial Intelligence and Human Nature"". The New Atlantis. 1: 88–100. Archived from the original on 11 June 2012.
Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.
Brooks, Rodney (10 November 2014). ""artificial intelligence is a tool, not a threat"". Archived from the original on 12 November 2014.
Sainato, Michael (19 August 2015). ""Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence"". Observer. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Harari, Yuval Noah (October 2018). ""Why Technology Favors Tyranny"". The Atlantic.
Robitzski, Dan (5 September 2018). ""Five experts share what scares them the most about AI"". Archived from the original on 8 December 2019. Retrieved 8 December 2019.
Goffrey, Andrew (2008). ""Algorithm"".  In Fuller, Matthew (ed.). Software studies: a lexicon. Cambridge, Mass.: MIT Press. pp. 15–20. ISBN 978-1-4356-4787-9.
Lipartito, Kenneth (6 January 2011), The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today (PDF) (Unpublished manuscript), doi:10.2139/ssrn.1736283, S2CID 166742927
Goodman, Bryce; Flaxman, Seth (2017). ""EU regulations on algorithmic decision-making and a ""right to explanation"""". AI Magazine. 38 (3): 50. arXiv:1606.08813. doi:10.1609/aimag.v38i3.2741. S2CID 7373959.
CNA (12 January 2019). ""Commentary: Bad news. Artificial intelligence is biased"". CNA. Archived from the original on 12 January 2019. Retrieved 19 June 2020.
Larson, Jeff; Angwin, Julia (23 May 2016). ""How We Analyzed the COMPAS Recidivism Algorithm"". ProPublica. Archived from the original on 29 April 2019. Retrieved 19 June 2020.
Müller, Vincent C.; Bostrom, Nick (2014). ""Future Progress in Artificial Intelligence: A Poll Among Experts"" (PDF). AI Matters. 1 (1): 9–11. doi:10.1145/2639475.2639478. S2CID 8510016. Archived (PDF) from the original on 15 January 2016.
Cellan-Jones, Rory (2 December 2014). ""Stephen Hawking warns artificial intelligence could end mankind"". BBC News. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Rawlinson, Kevin (29 January 2015). ""Microsoft's Bill Gates insists AI is a threat"". BBC News. Archived from the original on 29 January 2015. Retrieved 30 January 2015.
Holley, Peter (28 January 2015). ""Bill Gates on dangers of artificial intelligence: 'I don't understand why some people are not concerned'"". The Washington Post. ISSN 0190-8286. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Gibbs, Samuel (27 October 2014). ""Elon Musk: artificial intelligence is our biggest existential threat"". The Guardian. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Churm, Philip Andrew (14 May 2019). ""Yuval Noah Harari talks politics, technology and migration"". euronews. Archived from the original on 14 May 2019. Retrieved 15 November 2020.
Bostrom, Nick (2015). ""What happens when our computers get smarter than we are?"". TED (conference). Archived from the original on 25 July 2020. Retrieved 30 January 2020.
Post, Washington (2015). ""Tech titans like Elon Musk are spending $1 billion to save you from terminators"". Chicago Tribune. Archived from the original on 7 June 2016.
Del Prado, Guia Marie (9 October 2015). ""The mysterious artificial intelligence company Elon Musk invested in is developing game-changing smart computers"". Tech Insider. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
FastCompany (15 January 2015). ""Elon Musk Is Donating $10M Of His Own Money To Artificial Intelligence Research"". Fast Company. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Thibodeau, Patrick (25 March 2019). ""Oracle CEO Mark Hurd sees no reason to fear ERP AI"". SearchERP. Archived from the original on 6 May 2019. Retrieved 6 May 2019.
Bhardwaj, Prachi (24 May 2018). ""Mark Zuckerberg responds to Elon Musk's paranoia about AI: 'AI is going to... help keep our communities safe.'"". Business Insider. Archived from the original on 6 May 2019. Retrieved 6 May 2019.
Geist, Edward Moore (9 August 2015). ""Is artificial intelligence really an existential threat to humanity?"". Bulletin of the Atomic Scientists. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Madrigal, Alexis C. (27 February 2015). ""The case against killer robots, from a guy actually working on artificial intelligence"". Fusion.net. Archived from the original on 4 February 2016. Retrieved 31 January 2016.
Lee, Timothy B. (22 August 2014). ""Will artificial intelligence destroy humanity? Here are 5 reasons not to worry"". Vox. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Law Library of Congress (U.S.). Global Legal Research Directorate, issuing body. (2019). Regulation of artificial intelligence in selected jurisdictions. LCCN 2019668143. OCLC 1110727808.
UNESCO Science Report: the Race Against Time for Smarter Development. Paris: UNESCO. 11 June 2021. ISBN 978-92-3-100450-6.
Berryhill, Jamie; Heang, Kévin Kok; Clogher, Rob; McBride, Keegan (2019). Hello, World: Artificial Intelligence and its Use in the Public Sector (PDF). Paris: OECD Observatory of Public Sector Innovation. Archived (PDF) from the original on 20 December 2019. Retrieved 9 August 2020.
Barfield, Woodrow; Pagallo, Ugo (2018). Research handbook on the law of artificial intelligence. Cheltenham, UK. ISBN 978-1-78643-904-8. OCLC 1039480085.
Iphofen, Ron; Kritikos, Mihalis (3 January 2019). ""Regulating artificial intelligence and robotics: ethics by design in a digital society"". Contemporary Social Science. 16 (2): 170–184. doi:10.1080/21582041.2018.1563803. ISSN 2158-2041. S2CID 59298502.
Wirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). ""Artificial Intelligence and the Public Sector – Applications and Challenges"". International Journal of Public Administration. 42 (7): 596–615. doi:10.1080/01900692.2018.1498103. ISSN 0190-0692. S2CID 158829602. Archived from the original on 18 August 2020. Retrieved 22 August 2020.
Buiten, Miriam C (2019). ""Towards Intelligent Regulation of Artificial Intelligence"". European Journal of Risk Regulation. 10 (1): 41–59. doi:10.1017/err.2019.8. ISSN 1867-299X.
Wallach, Wendell (2010). Moral Machines. Oxford University Press.
Brown, Eileen (5 November 2019). ""Half of Americans do not believe deepfake news could target them online"". ZDNet. Archived from the original on 6 November 2019. Retrieved 3 December 2019.
Frangoul, Anmar (14 June 2019). ""A Californian business is using A.I. to change the way we think about energy storage"". CNBC. Archived from the original on 25 July 2020. Retrieved 5 November 2019.
""The Economist Explains: Why firms are piling into artificial intelligence"". The Economist. 31 March 2016. Archived from the original on 8 May 2016. Retrieved 19 May 2016.
Lohr, Steve (28 February 2016). ""The Promise of Artificial Intelligence Unfolds in Small Steps"". The New York Times. Archived from the original on 29 February 2016. Retrieved 29 February 2016.
Smith, Mark (22 July 2016). ""So you think you chose to read this article?"". BBC News. Archived from the original on 25 July 2016.
Aletras, N.; Tsarapatsanis, D.; Preotiuc-Pietro, D.; Lampos, V. (2016). ""Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective"". PeerJ Computer Science. 2: e93. doi:10.7717/peerj-cs.93.
Cadena, Cesar; Carlone, Luca; Carrillo, Henry; Latif, Yasir; Scaramuzza, Davide; Neira, Jose; Reid, Ian; Leonard, John J. (December 2016). ""Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age"". IEEE Transactions on Robotics. 32 (6): 1309–1332. arXiv:1606.05830. Bibcode:2016arXiv160605830C. doi:10.1109/TRO.2016.2624754. S2CID 2596787.
Cambria, Erik; White, Bebo (May 2014). ""Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]"". IEEE Computational Intelligence Magazine. 9 (2): 48–57. doi:10.1109/MCI.2014.2307227. S2CID 206451986.
Vincent, James (7 November 2019). ""OpenAI has published the text-generating AI it said was too dangerous to share"". The Verge. Archived from the original on 11 June 2020. Retrieved 11 June 2020.
Jordan, M. I.; Mitchell, T. M. (16 July 2015). ""Machine learning: Trends, perspectives, and prospects"". Science. 349 (6245): 255–260. Bibcode:2015Sci...349..255J. doi:10.1126/science.aaa8415. PMID 26185243. S2CID 677218.
Maschafilm (2010). ""Content: Plug & Pray Film – Artificial Intelligence – Robots -"". plugandpray-film.de. Archived from the original on 12 February 2016.
Evans, Woody (2015). ""Posthuman Rights: Dimensions of Transhuman Worlds"". Teknokultura. 12 (2). doi:10.5209/rev_TK.2015.v12.n2.49072.
Waddell, Kaveh (2018). ""Chatbots Have Entered the Uncanny Valley"". The Atlantic. Archived from the original on 24 April 2018. Retrieved 24 April 2018.
Poria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). ""A review of affective computing: From unimodal analysis to multimodal fusion"". Information Fusion. 37: 98–125. doi:10.1016/j.inffus.2017.02.003. hdl:1893/25490.
""Robots could demand legal rights"". BBC News. 21 December 2006. Archived from the original on 15 October 2019. Retrieved 3 February 2011.
Horst, Steven (2005). ""The Computational Theory of Mind"". The Stanford Encyclopedia of Philosophy.
Omohundro, Steve (2008). The Nature of Self-Improving Artificial Intelligence. presented and distributed at the 2007 Singularity Summit, San Francisco, CA.
Ford, Martin; Colvin, Geoff (6 September 2015). ""Will robots create more jobs than they destroy?"". The Guardian. Archived from the original on 16 June 2018. Retrieved 13 January 2018.
White Paper: On Artificial Intelligence – A European approach to excellence and trust (PDF). Brussels: European Commission. 2020. Archived (PDF) from the original on 20 February 2020. Retrieved 20 February 2020.
Anderson, Michael; Anderson, Susan Leigh (2011). Machine Ethics. Cambridge University Press.
""Machine Ethics"". aaai.org. Archived from the original on 29 November 2014.
Russell, Stuart (8 October 2019). Human Compatible: Artificial Intelligence and the Problem of Control. United States: Viking. ISBN 978-0-525-55861-3. OCLC 1083694322.
""AI set to exceed human brain power"". CNN. 9 August 2006. Archived from the original on 19 February 2008.
""Robots could demand legal rights"". BBC News. 21 December 2006. Archived from the original on 15 October 2019. Retrieved 3 February 2011.
""Kismet"". MIT Artificial Intelligence Laboratory, Humanoid Robotics Group. Archived from the original on 17 October 2014. Retrieved 25 October 2014.
Smoliar, Stephen W.; Zhang, HongJiang (1994). ""Content based video indexing and retrieval"". IEEE Multimedia. 1 (2): 62–72. doi:10.1109/93.311653. S2CID 32710913.
Neumann, Bernd; Möller, Ralf (January 2008). ""On scene interpretation with description logics"". Image and Vision Computing. 26 (1): 82–101. doi:10.1016/j.imavis.2007.08.013.
Kuperman, G. J.; Reichley, R. M.; Bailey, T. C. (1 July 2006). ""Using Commercial Knowledge Bases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations"". Journal of the American Medical Informatics Association. 13 (4): 369–371. doi:10.1197/jamia.M2055. PMC 1513681. PMID 16622160.
McGarry, Ken (1 December 2005). ""A survey of interestingness measures for knowledge discovery"". The Knowledge Engineering Review. 20 (1): 39–61. doi:10.1017/S0269888905000408. S2CID 14987656.
Bertini, M; Del Bimbo, A; Torniai, C (2006). ""Automatic annotation and semantic retrieval of video sequences using multimedia ontologies"". MM '06 Proceedings of the 14th ACM international conference on Multimedia. 14th ACM international conference on Multimedia. Santa Barbara: ACM. pp. 679–682.
Kahneman, Daniel (25 October 2011). Thinking, Fast and Slow. Macmillan. ISBN 978-1-4299-6935-2. Retrieved 8 April 2012.
Turing, Alan (1948), ""Machine Intelligence"",  in Copeland, B. Jack (ed.), The Essential Turing: The ideas that gave birth to the computer age, Oxford: Oxford University Press, p. 412, ISBN 978-0-19-825080-7
Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.
Minsky, Marvin (1986), The Society of Mind, Simon and Schuster
Pinker, Steven (4 September 2007) [1994], The Language Instinct, Perennial Modern Classics, Harper, ISBN 978-0-06-133646-1
Chalmers, David (1995). ""Facing up to the problem of consciousness"". Journal of Consciousness Studies. 2 (3): 200–219. Archived from the original on 8 March 2005. Retrieved 11 October 2018.
Roberts, Jacob (2016). ""Thinking Machines: The Search for Artificial Intelligence"". Distillations. Vol. 2, no. 2. pp. 14–23. Archived from the original on 19 August 2018. Retrieved 20 March 2018.
Pennachin, C.; Goertzel, B. (2007). ""Contemporary Approaches to Artificial General Intelligence"". Artificial General Intelligence. Cognitive Technologies. Berlin, Heidelberg: Springer. doi:10.1007/978-3-540-68677-4_1. ISBN 978-3-540-23733-4.
""Ask the AI experts: What's driving today's progress in AI?"". McKinsey & Company. Archived from the original on 13 April 2018. Retrieved 13 April 2018.
""Reshaping Business With Artificial Intelligence"". MIT Sloan Management Review. Archived from the original on 19 May 2018. Retrieved 2 May 2018.
Lorica, Ben (18 December 2017). ""The state of AI adoption"". O'Reilly Media. Archived from the original on 2 May 2018. Retrieved 2 May 2018.
""AlphaGo – Google DeepMind"". Archived from the original on 20 October 2021.
Asada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida, C. (2009). ""Cognitive developmental robotics: a survey"". IEEE Transactions on Autonomous Mental Development. 1 (1): 12–34. doi:10.1109/tamd.2009.2021702. S2CID 10168773.
Ashok83 (10 September 2019). ""How AI Is Getting Groundbreaking Changes In Talent Management And HR Tech"". Hackernoon. Archived from the original on 11 September 2019. Retrieved 14 February 2020.
Berlinski, David (2000). The Advent of the Algorithm. Harcourt Books. ISBN 978-0-15-601391-8. OCLC 46890682. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Brooks, Rodney (1990). ""Elephants Don't Play Chess"" (PDF). Robotics and Autonomous Systems. 6 (1–2): 3–15. CiteSeerX 10.1.1.588.7539. doi:10.1016/S0921-8890(05)80025-9. Archived (PDF) from the original on 9 August 2007.
Butler, Samuel (13 June 1863). ""Darwin among the Machines"". Letters to the Editor. The Press. Christchurch, New Zealand. Archived from the original on 19 September 2008. Retrieved 16 October 2014 – via Victoria University of Wellington.
Clark, Jack (2015a). ""Musk-Backed Group Probes Risks Behind Artificial Intelligence"". Bloomberg.com. Archived from the original on 30 October 2015. Retrieved 30 October 2015.
Clark, Jack (2015b). ""Why 2015 Was a Breakthrough Year in Artificial Intelligence"". Bloomberg.com. Archived from the original on 23 November 2016. Retrieved 23 November 2016.
Dennett, Daniel (1991). Consciousness Explained. The Penguin Press. ISBN 978-0-7139-9037-9.
Dreyfus, Hubert (1972). What Computers Can't Do. New York: MIT Press. ISBN 978-0-06-011082-6.
Dreyfus, Hubert; Dreyfus, Stuart (1986). Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer. Oxford, UK: Blackwell. ISBN 978-0-02-908060-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Dyson, George (1998). Darwin among the Machines. Allan Lane Science. ISBN 978-0-7382-0030-9. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Edelson, Edward (1991). The Nervous System. New York: Chelsea House. ISBN 978-0-7910-0464-7. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Fearn, Nicholas (2007). The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers. New York: Grove Press. ISBN 978-0-8021-1839-4.
Haugeland, John (1985). Artificial Intelligence: The Very Idea. Cambridge, Mass.: MIT Press. ISBN 978-0-262-08153-5.
Hawkins, Jeff; Blakeslee, Sandra (2005). On Intelligence. New York: Owl Books. ISBN 978-0-8050-7853-4.
Henderson, Mark (24 April 2007). ""Human rights for robots? We're getting carried away"". The Times Online. London. Archived from the original on 31 May 2014. Retrieved 31 May 2014.
Kahneman, Daniel; Slovic, D.; Tversky, Amos (1982). Judgment under uncertainty: Heuristics and biases. Science. Vol. 185. New York: Cambridge University Press. pp. 1124–1131. doi:10.1126/science.185.4157.1124. ISBN 978-0-521-28414-1. PMID 17835457. S2CID 143452957.
Katz, Yarden (1 November 2012). ""Noam Chomsky on Where Artificial Intelligence Went Wrong"". The Atlantic. Archived from the original on 28 February 2019. Retrieved 26 October 2014.
Kurzweil, Ray (2005). The Singularity is Near. Penguin Books. ISBN 978-0-670-03384-3.
Langley, Pat (2011). ""The changing science of machine learning"". Machine Learning. 82 (3): 275–279. doi:10.1007/s10994-011-5242-y.
Legg, Shane; Hutter, Marcus (15 June 2007). A Collection of Definitions of Intelligence (Technical report). IDSIA. arXiv:0706.3639. Bibcode:2007arXiv0706.3639L. 07-07.
Lenat, Douglas; Guha, R. V. (1989). Building Large Knowledge-Based Systems. Addison-Wesley. ISBN 978-0-201-51752-1.
Lighthill, James (1973). ""Artificial Intelligence: A General Survey"". Artificial Intelligence: a paper symposium. Science Research Council.
Lombardo, P; Boehm, I; Nairz, K (2020). ""RadioComics – Santa Claus and the future of radiology"". Eur J Radiol. 122 (1): 108771. doi:10.1016/j.ejrad.2019.108771. PMID 31835078.
Lungarella, M.; Metta, G.; Pfeifer, R.; Sandini, G. (2003). ""Developmental robotics: a survey"". Connection Science. 15 (4): 151–190. CiteSeerX 10.1.1.83.7615. doi:10.1080/09540090310001655110. S2CID 1452734.
Maker, Meg Houston (2006). ""AI@50: AI Past, Present, Future"". Dartmouth College. Archived from the original on 3 January 2007. Retrieved 16 October 2008.
McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955). ""A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence"". Archived from the original on 26 August 2007. Retrieved 30 August 2007.
Minsky, Marvin (1967). Computation: Finite and Infinite Machines. Englewood Cliffs, N.J.: Prentice-Hall. ISBN 978-0-13-165449-5. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Moravec, Hans (1988). Mind Children. Harvard University Press. ISBN 978-0-674-57616-2. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
NRC (United States National Research Council) (1999). ""Developments in Artificial Intelligence"". Funding a Revolution: Government Support for Computing Research. National Academy Press.
Newell, Allen; Simon, H. A. (1976). ""Computer Science as Empirical Inquiry: Symbols and Search"". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022..
Nilsson, Nils (1983). ""Artificial Intelligence Prepares for 2001"" (PDF). AI Magazine. 1 (1). Archived (PDF) from the original on 17 August 2020. Retrieved 22 August 2020. Presidential Address to the Association for the Advancement of Artificial Intelligence.
Oudeyer, P-Y. (2010). ""On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development"" (PDF). IEEE Transactions on Autonomous Mental Development. 2 (1): 2–16. doi:10.1109/tamd.2009.2039057. S2CID 6362217. Archived (PDF) from the original on 3 October 2018. Retrieved 4 June 2013.
Schank, Roger C. (1991). ""Where's the AI"". AI magazine. Vol. 12, no. 4.
Searle, John (1980). ""Minds, Brains and Programs"" (PDF). Behavioral and Brain Sciences. 3 (3): 417–457. doi:10.1017/S0140525X00005756. S2CID 55303721. Archived (PDF) from the original on 17 March 2019. Retrieved 22 August 2020.
Searle, John (1999). Mind, language and society. New York: Basic Books. ISBN 978-0-465-04521-1. OCLC 231867665. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Simon, H. A. (1965). The Shape of Automation for Men and Management. New York: Harper & Row. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Solomonoff, Ray (1956). An Inductive Inference Machine (PDF). Dartmouth Summer Research Conference on Artificial Intelligence. Archived (PDF) from the original on 26 April 2011. Retrieved 22 March 2011 – via std.com, pdf scanned copy of the original. Later published asSolomonoff, Ray (1957). ""An Inductive Inference Machine"". IRE Convention Record. Vol. Section on Information Theory, part 2. pp. 56–62.
Spadafora, Anthony (21 October 2016). ""Stephen Hawking believes AI could be mankind's last accomplishment"". BetaNews. Archived from the original on 28 August 2017.
Tao, Jianhua; Tan, Tieniu (2005). Affective Computing and Intelligent Interaction. Affective Computing: A Review. Vol. LNCS 3784. Springer. pp. 981–995. doi:10.1007/11573548.
Tecuci, Gheorghe (March–April 2012). ""Artificial Intelligence"". Wiley Interdisciplinary Reviews: Computational Statistics. 4 (2): 168–180. doi:10.1002/wics.200. S2CID 196141190.
Thro, Ellen (1993). Robotics: The Marriage of Computers and Machines. New York: Facts on File. ISBN 978-0-8160-2628-9. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Turing, Alan (October 1950), ""Computing Machinery and Intelligence"", Mind, LIX (236): 433–460, doi:10.1093/mind/LIX.236.433, ISSN 0026-4423.
Vinge, Vernor (1993). ""The Coming Technological Singularity: How to Survive in the Post-Human Era"". Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace: 11. Bibcode:1993vise.nasa...11V. Archived from the original on 1 January 2007. Retrieved 14 November 2011.
Wason, P. C.; Shapiro, D. (1966). ""Reasoning"".  In Foss, B. M. (ed.). New horizons in psychology. Harmondsworth: Penguin. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Weng, J.; McClelland; Pentland, A.; Sporns, O.; Stockman, I.; Sur, M.; Thelen, E. (2001). ""Autonomous mental development by robots and animals"" (PDF). Science. 291 (5504): 599–600. doi:10.1126/science.291.5504.599. PMID 11229402. S2CID 54131797. Archived (PDF) from the original on 4 September 2013. Retrieved 4 June 2013 – via msu.edu.
Further reading

DH Author, ""Why Are There Still So Many Jobs? The History and Future of Workplace Automation"" (2015) 29(3) Journal of Economic Perspectives 3.
Boden, Margaret, Mind As Machine, Oxford University Press, 2006.
Cukier, Kenneth, ""Ready for Robots?  How to Think about the Future of AI"", Foreign Affairs, vol. 98, no. 4 (July/August 2019), pp. 192–98.  George Dyson, historian of computing, writes (in what might be called ""Dyson's Law"") that ""Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand."" (p. 197.)  Computer scientist Alex Pentland writes:  ""Current AI machine-learning algorithms are, at their core, dead simple stupid.  They work, but they work by brute force."" (p. 198.)
Domingos, Pedro, ""Our Digital Doubles:  AI will serve our species, not control it"", Scientific American, vol. 319, no. 3 (September 2018), pp. 88–93.
Gopnik, Alison, ""Making AI More Human:  Artificial intelligence has staged a revival by starting to incorporate what we know about how children learn"", Scientific American, vol. 316, no. 6 (June 2017), pp. 60–65.
Halpern, Sue, ""The Human Costs of AI"" (review of Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence, Yale University Press, 2021, 327 pp.; Simon Chesterman, We, the Robots?: Regulating Artificial Intelligence and the Limits of the Law, Cambridge University Press, 2021, 289 pp.; Keven Roose, Futureproof: 9 Rules for Humans in the Age of Automation, Random House, 217 pp.; Erik J. Larson, The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do, Belknap Press / Harvard University Press, 312 pp.), The New York Review of Books, vol. LXVIII, no. 16 (21 October 2021), pp. 29–31. ""AI training models can replicate entrenched social and cultural biases. [...] Machines only know what they know from the data they have been given. [p. 30.] [A]rtificial general intelligence–machine-based intelligence that matches our own–is beyond the capacity of algorithmic machine learning... 'Your brain is one piece in a broader system which includes your body, your environment, other humans, and culture as a whole.' [E]ven machines that master the tasks they are trained to perform can't jump domains. AIVA, for example, can't drive a car even though it can write music (and wouldn't even be able to do that without Bach and Beethoven [and other composers on which AIVA is trained])."" (p. 31.)
Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI, MIT Press.
Koch, Christof, ""Proust among the Machines"", Scientific American, vol. 321, no. 6 (December 2019), pp. 46–49. Christof Koch doubts the possibility of ""intelligent"" machines attaining consciousness, because ""[e]ven the most sophisticated brain simulations are unlikely to produce conscious feelings."" (p. 48.) According to Koch, ""Whether machines can become sentient [is important] for ethical reasons. If computers experience life through their own senses, they cease to be purely a means to an end determined by their usefulness to... humans. Per GNW [the Global Neuronal Workspace theory], they turn from mere objects into subjects... with a point of view.... Once computers' cognitive abilities rival those of humanity, their impulse to push for legal and political rights will become irresistible—the right not to be deleted, not to have their memories wiped clean, not to suffer pain and degradation. The alternative, embodied by IIT [Integrated Information Theory], is that computers will remain only supersophisticated machinery, ghostlike empty shells, devoid of what we value most: the feeling of life itself."" (p. 49.)
Marcus, Gary, ""Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind"", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63.  A stumbling block to AI has been an incapacity for reliable disambiguation.  An example is the ""pronoun disambiguation problem"":  a machine has no way of determining to whom or what a pronoun in a sentence refers. (p. 61.)
E McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2018) SSRN, part 2(3) Archived 24 May 2018 at the Wayback Machine.
George Musser, ""Artificial Imagination:  How machines could learn creativity and common sense, among other human qualities"", Scientific American, vol. 320, no. 5 (May 2019), pp. 58–63.
Myers, Courtney Boyd ed. (2009). ""The AI Report"" Archived 29 July 2017 at the Wayback Machine. Forbes June 2009
Raphael, Bertram (1976). The Thinking Computer. W.H. Freeman and Co. ISBN 978-0716707233. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Scharre, Paul, ""Killer Apps:  The Real Dangers of an AI Arms Race"", Foreign Affairs, vol. 98, no. 3 (May/June 2019), pp. 135–44.  ""Today's AI technologies are powerful but unreliable.  Rules-based systems cannot deal with circumstances their programmers did not anticipate.  Learning systems are limited by the data on which they were trained.  AI failures have already led to tragedy.  Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars.  In the wrong situation, AI systems go from supersmart to superdumb in an instant.  When an enemy is trying to manipulate and hack an AI system, the risks are even greater.""  (p. 140.)
Serenko, Alexander (2010). ""The development of an AI journal ranking based on the revealed preference approach"" (PDF). Journal of Informetrics. 4 (4): 447–59. doi:10.1016/j.joi.2010.04.001. Archived (PDF) from the original on 4 October 2013. Retrieved 24 August 2013.
Serenko, Alexander; Michael Dohan (2011). ""Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence"" (PDF). Journal of Informetrics. 5 (4): 629–49. doi:10.1016/j.joi.2011.06.002. Archived (PDF) from the original on 4 October 2013. Retrieved 12 September 2013.
Tom Simonite (29 December 2014). ""2014 in Computing: Breakthroughs in Artificial Intelligence"". MIT Technology Review.
Sun, R. & Bookman, L. (eds.), Computational Architectures: Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994.
Taylor, Paul, ""Insanely Complicated, Hopelessly Inadequate"" (review of Brian Cantwell Smith, The Promise of Artificial Intelligence: Reckoning and Judgment, MIT, 2019, ISBN 978-0262043045, 157 pp.; Gary Marcus and Ernest Davis, Rebooting AI: Building Artificial Intelligence We Can Trust, Ballantine, 2019, ISBN 978-1524748258, 304 pp.; Judea Pearl and Dana Mackenzie, The Book of Why: The New Science of Cause and Effect, Penguin, 2019, ISBN 978-0141982410, 418 pp.), London Review of Books, vol. 43, no. 2 (21 January 2021), pp. 37–39. Paul Taylor writes (p. 39): ""Perhaps there is a limit to what a computer can do without knowing that it is manipulating imperfect representations of an external reality.""
Tooze, Adam, ""Democracy and Its Discontents"", The New York Review of Books, vol. LXVI, no. 10 (6 June 2019), pp. 52–53, 56–57.  ""Democracy has no clear answer for the mindless operation of bureaucratic and technological power.  We may indeed be witnessing its extension in the form of artificial intelligence and robotics.  Likewise, after decades of dire warning, the environmental problem remains fundamentally unaddressed.... Bureaucratic overreach and environmental catastrophe are precisely the kinds of slow-moving existential challenges that democracies deal with very badly.... Finally, there is the threat du jour:  corporations and the technologies they promote.""  (pp. 56–57.)

External links
Artificial intelligenceat Wikipedia's sister projectsDefinitions from WiktionaryMedia from CommonsQuotations from WikiquoteTextbooks from WikibooksResources from WikiversityData from Wikidata
""Artificial Intelligence"". Internet Encyclopedia of Philosophy.
Thomason, Richmond. ""Logic and Artificial Intelligence"".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005).
Articles related to Artificial intelligence
vteJohn McCarthy
Artificial intelligence
Circumscription
Dartmouth workshop
Frame problem
Garbage collection
Lisp
McCarthy Formalism
McCarthy 91 function
Situation calculus
Space fountain

vtePhilosophy of mindPhilosophers
Anscombe
Austin
Aquinas
Bain
Bergson
Bhattacharya
Block
Brentano
Broad
Burge
Chalmers
Churchland
Dennett
Dharmakirti
Davidson
Descartes
Goldman
Heidegger
Husserl
Feyerabend
Fodor
James
Kierkegaard
Leibniz
Lewis
McDowell
Merleau-Ponty
Minsky
Moore
Nagel
Parfit
Putnam
Popper
Rorty
Ryle
Searle
Spinoza
Turing
Vasubandhu
Wittgenstein
Zhuangzi
more...
Theories
Behaviorism
Biological naturalism
Dualism
Eliminative materialism
Emergent materialism
Epiphenomenalism
Functionalism
Idealism
Interactionism
Materialism
Monism
Naïve realism
Neurophenomenology
Neutral monism
Occasionalism
Parallelism
Phenomenalism
Phenomenology
Physicalism
identity theory
Property dualism
Representational
Solipsism
Substance dualism
Concepts
Abstract object
Artificial intelligence
Chinese room
Cognition
Cognitive closure
Concept
Concept and object
Consciousness
Hard problem of consciousness
Hypostatic abstraction
Idea
Identity
Ingenuity
Intelligence
Intentionality
Introspection
Intuition
Language of thought
Materialism
Mental event
Mental image
Mental process
Mental property
Mental representation
Mind
Mind–body problem
New mysterianism
Pain
Problem of other minds
Propositional attitude
Qualia
Tabula rasa
Understanding
Zombie
more...
Related
Metaphysics
Philosophy of artificial intelligence / information / perception / self

Category
Philosophers category
Project
Task Force

vtePhilosophy of scienceConcepts
Analysis
Analytic–synthetic distinction
A priori and a posteriori
Causality
Commensurability
Consilience
Construct
Creative synthesis
Demarcation problem
Empirical evidence
Explanatory power
Fact
Falsifiability
Feminist method
Functional contextualism
Ignoramus et ignorabimus
Inductive reasoning
Intertheoretic reduction
Inquiry
Nature
Objectivity
Observation
Paradigm
Problem of induction
Scientific law
Scientific method
Scientific pluralism
Scientific revolution
Scientific theory
Testability
Theory choice
Theory-ladenness
Underdetermination
Unity of science
Metatheoryof science
Coherentism
Confirmation holism
Constructive empiricism
Constructive realism
Constructivist epistemology
Contextualism
Conventionalism
Deductive-nomological model
Hypothetico-deductive model
Inductionism
Epistemological anarchism
Evolutionism
Fallibilism
Foundationalism
Instrumentalism
Pragmatism
Model-dependent realism
Naturalism
Physicalism
Positivism / Reductionism / Determinism
Rationalism / Empiricism
Received view / Semantic view of theories
Scientific realism / Anti-realism
Scientific essentialism
Scientific formalism
Scientific skepticism
Scientism
Structuralism
Uniformitarianism
Vitalism
Philosophy of
Physics
thermal and statistical
Motion
Chemistry
Biology
Geography
Social science
Technology
Engineering
Artificial intelligence
Computer science
Information
Mind
Psychiatry
Psychology
Perception
Space and time
Related topics
Alchemy
Criticism of science
Descriptive science
Epistemology
Faith and rationality
Hard and soft science
History and philosophy of science
History of science
History of evolutionary thought
Logic
Metaphysics
Normative science
Pseudoscience
Relationship between religion and science
Rhetoric of science
Science studies
Sociology of scientific knowledge
Sociology of scientific ignorance
Philosophers of science by eraAncient
Plato
Aristotle
Stoicism
Epicureans
Epicurus
Medieval
Averroes
Avicenna
Roger Bacon
William of Ockham
Hugh of Saint Victor
Dominicus Gundissalinus
Robert Kilwardby
Early modern
Francis Bacon
Thomas Hobbes
René Descartes
Galileo Galilei
Pierre Gassendi
Isaac Newton
David Hume
Late modern
Immanuel Kant
Friedrich Schelling
William Whewell
Auguste Comte
John Stuart Mill
Herbert Spencer
Wilhelm Wundt
Charles Sanders Peirce
Wilhelm Windelband
Henri Poincaré
Pierre Duhem
Rudolf Steiner
Karl Pearson
Contemporary
Alfred North Whitehead
Bertrand Russell
Albert Einstein
Otto Neurath
C. D. Broad
Michael Polanyi
Hans Reichenbach
Rudolf Carnap
Karl Popper
Carl Gustav Hempel
W. V. O. Quine
Thomas Kuhn
Imre Lakatos
Paul Feyerabend
Jürgen Habermas
Ian Hacking
Bas van Fraassen
Larry Laudan
Daniel Dennett

Category
 Philosophy portal
 Science portal

vteEvolutionary computationMain Topics
Convergence (evolutionary computing)
Evolutionary algorithm
Evolutionary data mining
Evolutionary multimodal optimization
Human-based evolutionary computation
Interactive evolutionary computation
Algorithms
Cellular evolutionary algorithm
Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
Differential evolution
Evolutionary programming
Genetic algorithm
Genetic programming
Gene expression programming
Evolution strategy
Natural evolution strategy
Neuroevolution
Learning classifier system
Related techniques
Swarm intelligence
Ant colony optimization
Bees algorithm
Cuckoo search
Particle swarm optimization
Bacterial Colony Optimization
Metaheuristic methods
Firefly algorithm
Harmony search
Gaussian adaptation
Memetic algorithm
Related topics
Artificial development
Artificial intelligence
Artificial life
Digital organism
Evolutionary robotics
Fitness function
Fitness landscape
Fitness approximation
Genetic operators
Interactive evolutionary computation
No free lunch in search and optimization
Machine learning
Mating pool
Program synthesis
Journals
Evolutionary Computation (journal)

vteDifferentiable computingGeneral
Differentiable programming
Neural Turing machine
Differentiable neural computer
Automatic differentiation
Neuromorphic engineering
Cable theory
Pattern recognition
Computational learning theory
Tensor calculus
Concepts
Gradient descent
SGD
Clustering
Regression
Overfitting
Adversary
Attention
Convolution
Loss functions
Backpropagation
Normalization
Activation
Softmax
Sigmoid
Rectifier
Regularization
Datasets
Augmentation
Programming languages
Python
Julia
Application
Machine learning
Artificial neural network
Deep learning
Scientific computing
Artificial Intelligence
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software library
TensorFlow
PyTorch
Keras
Theano
ImplementationAudio–visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
Speech recognition
Facial recognition
AlphaFold
DALL-E
Verbal
Word2vec
Transformer
BERT
NMT
Project Debater
Watson
GPT-2
GPT-3
Decisional
AlphaGo
AlphaZero
Q-learning
SARSA
OpenAI Five
Self-driving car
MuZero
Action selection
Robot control
People
Alex Graves
Ian Goodfellow
Yoshua Bengio
Geoffrey Hinton
Yann LeCun
Andrew Ng
Demis Hassabis
David Silver
Fei-Fei Li
Organizations
DeepMind
OpenAI
MIT CSAIL
Mila
Google Brain
FAIR

 Portals
Computer programming
Technology
 Category
Artificial neural networks
Machine learning

vteComputable knowledgeTopics andconcepts
Alphabet of human thought
Authority control
Automated reasoning
Commonsense knowledge
Commonsense reasoning
Computability
Discovery system
Formal system
Inference engine
Knowledge base
Personal knowledge base
Knowledge-based systems
Knowledge engineering
Knowledge extraction
Knowledge graph
Knowledge representation
Knowledge retrieval
Library classification
Logic programming
Ontology
Question answering
Semantic reasoner
Proposals andimplementations
Antikythera mechanism (ca. 100 BCE)
Zairja (ca. 1000 CE)
Ars Magna (1300)
An Essay Towards a Real Character, and a Philosophical Language (1688)
Calculus ratiocinator and characteristica universalis (1700)
Dewey Decimal Classification (1876)
Begriffsschrift (1879)
Mundaneum (1910)
Logical atomism (1918)
Tractatus Logico-Philosophicus (1921)
Hilbert's program (1920s)
Incompleteness theorem (1931)
World Brain (1938)
Memex (1945)
General Problem Solver (1959)
Prolog (1972)
Cyc (1984)
Semantic Web (2001)
Wikipedia (2001)
Evi (2007)
Wolfram Alpha (2009)
Watson (2011)
Siri (2011)
Google Knowledge Graph (2012)
Wikidata (2012)
Cortana (2014)
Viv (2016)
In fiction
The Engine (Gulliver's Travels, 1726)
Joe (""A Logic Named Joe"", 1946)
The Librarian (Snow Crash, 1992)
Dr. Know (A.I. (film), 2001)
Waterhouse (The Baroque Cycle, 2003)
See also: Logic machines in fiction and List of fictional computers


vteComputer scienceNote: This template roughly follows the 2012 ACM Computing Classification System.Hardware
Printed circuit board
Peripheral
Integrated circuit
Very Large Scale Integration
Systems on Chip (SoCs)
Energy consumption (Green computing)
Electronic design automation
Hardware acceleration
Computer systems organization
Computer architecture
Embedded system
Real-time computing
Dependability
Networks
Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service
Software organization
Interpreter
Middleware
Virtual machine
Operating system
Software quality
Software notations and tools
Programming paradigm
Programming language
Compiler
Domain-specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository
Software development
Control variable
Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software maintenance
Programming team
Open-source model
Theory of computation
Model of computation
Formal language
Automata theory
Computability theory
Computational complexity theory
Logic
Semantics
Algorithms
Algorithm design
Analysis of algorithms
Algorithmic efficiency
Randomized algorithm
Computational geometry
Mathematics of computing
Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis
Theoretical computer science
Information systems
Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval
Security
Cryptography
Formal methods
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security
Human–computer interaction
Interaction design
Social computing
Ubiquitous computing
Visualization
Accessibility
Concurrency
Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing
Artificial intelligence
Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence
Machine learning
Supervised learning
Unsupervised learning
Reinforcement learning
Multi-task learning
Cross-validation
Graphics
Animation
Rendering
Image manipulation
Graphics processing unit
Mixed reality
Virtual reality
Image compression
Solid modeling
Applied computing
E-commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video games
Word processing
Operations research
Educational technology
Document management

 Category
 Outline
WikiProject
 Commons

vteEmerging technologiesFieldsInformation andcommunications
Ambient intelligence
Internet of things
Artificial intelligence
Applications of artificial intelligence
Progress in artificial intelligence
Machine translation
Mobile translation
Machine vision
Semantic Web
Speech recognition
Atomtronics
Carbon nanotube field-effect transistor
Cybermethodology
Fourth-generation optical discs
3D optical data storage
Holographic data storage
GPGPU
Memory
CBRAM
FRAM
Millipede
MRAM
NRAM
PRAM
Racetrack memory
RRAM
SONOS
ECRAM
UltraRAM
Optical computing
RFID
Chipless RFID
Software-defined radio
Three-dimensional integrated circuit
Topics
Collingridge dilemma
Differential technological development
Disruptive innovation
Ephemeralization
Ethics
Bioethics
Cyberethics
Neuroethics
Robot ethics
Exploratory engineering
Fictional technology
Proactionary principle
Technological change
Technological unemployment
Technological convergence
Technological evolution
Technological paradigm
Technology forecasting
Accelerating change
Horizon scanning
Moore's law
Technological singularity
Technology scouting
Technology readiness level
Technology roadmap
Transhumanism

 Category
 List

vteRoboticsMain articles
Outline
Glossary
Indexc
History
Geography
Hall of Fame
Ethics
Laws
Competitions
AI competitions
Types
Anthropomorphic
Humanoid
Android
Cyborg
Claytronics
Companion
Animatronic
Audio-Animatronics
Industrial
Articulated
arm
Domestic
Educational
Entertainment
Juggling
Military
Medical
Service
Disability
Agricultural
Food service
Retail
BEAM robotics
Soft robotics
Classifications
Biorobotics
Unmanned vehicle
aerial
ground
Mobile robot
Microbotics
Nanorobotics
Robotic spacecraft
Space probe
Swarm
Telerobotics
Underwater
remotely-operated
Locomotion
Tracks
Walking
Hexapod
Climbing
Electric unicycle
Robot navigation
Research
Evolutionary
Kits
Simulator
Suite
Open-source
Software
Adaptable
Developmental
Paradigms
Ubiquitous
Related
Critique of work
Powered exoskeleton
Technological unemployment
Terrainability
Fictional robots

 Category
 Outline

vteExistential risk from artificial intelligenceConcepts
Accelerating change
AI box
AI takeover
Control problem
Existential risk from artificial general intelligence
Friendly artificial intelligence
Instrumental convergence
Intelligence explosion
Machine ethics
Superintelligence
Technological singularity
Organizations
Allen Institute for AI
Center for Applied Rationality
Center for Human-Compatible Artificial Intelligence
Centre for the Study of Existential Risk
DeepMind
Foundational Questions Institute
Future of Humanity Institute
Future of Life Institute
Humanity+
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Nick Bostrom
Eric Drexler
Sam Harris
Stephen Hawking
Bill Hibbard
Bill Joy
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J. Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Andrew Yang
Eliezer Yudkowsky
Other
Artificial intelligence as a global catastrophic risk
Controversies and dangers of artificial general intelligence
Ethics of artificial intelligence
Suffering risks
Human Compatible
Open Letter on Artificial Intelligence
Our Final Invention
The Precipice
Superintelligence: Paths, Dangers, Strategies
Do You Trust This Computer?
 Category
vteSubfields of and cyberneticians involved in cyberneticsSubfields
Artificial intelligence
Biological cybernetics
Biomedical cybernetics
Biorobotics
Biosemiotics
Neurocybernetics
Catastrophe theory
Computational neuroscience
Connectionism
Control theory
Cybernetics in the Soviet Union
Decision theory
Emergence
Engineering cybernetics
Homeostasis
Information theory
Management cybernetics
Medical cybernetics
Second-order cybernetics
Semiotics
Sociocybernetics
Polycontexturality
Synergetics
Cyberneticians
Alexander Lerner
Alexey Lyapunov
Alfred Radcliffe-Brown
Allenna Leonard
Anthony Wilden
Buckminster Fuller
Charles François
Genevieve Bell
Margaret Boden
Claude Bernard
Cliff Joslyn
Erich von Holst
Ernst von Glasersfeld
Francis Heylighen
Francisco Varela
Frederic Vester
Charles Geoffrey Vickers
Gordon Pask
Gordon S. Brown
Gregory Bateson
Heinz von Foerster
Humberto Maturana
I. A. Richards
Igor Aleksander
Jacque Fresco
Jakob von Uexküll
Jason Jixuan Hu
Jay Wright Forrester
Jennifer Wilby
John N. Warfield
Kevin Warwick
Ludwig von Bertalanffy
Maleyka Abbaszadeh
Manfred Clynes
Margaret Mead
Marian Mazur
N. Katherine Hayles
Natalia Bekhtereva
Niklas Luhmann
Norbert Wiener
Pyotr Grigorenko
Qian Xuesen
Ranulph Glanville
Robert Trappl
Sergei P. Kurdyumov
Anthony Stafford Beer
Stuart Kauffman
Stuart Umpleby
Talcott Parsons
Ulla Mitzdorf
Valentin Turchin
Valentin Braitenberg
William Ross Ashby
Walter Bradford Cannon
Walter Pitts
Warren McCulloch
William Grey Walter

vteGlossaries of science and engineering
Aerospace engineering
Agriculture
Archaeology
Architecture
Artificial intelligence
Astronomy
Biology
Botany
Calculus
Chemistry
Civil engineering
Clinical research
Computer hardware
Computer science
Ecology
Economics
Electrical and electronics engineering
Engineering
A–L
M–Z
Entomology
Environmental science
Evolutionary biology
Genetics
Geography
Arabic toponyms
Geology
Ichthyology
Machine vision
Mathematics
Mechanical engineering
Medicine
Meteorology
Nanotechnology
Ornithology
Physics
Probability and statistics
Psychiatry
Robotics
Scientific naming
Structural engineering
Virology

Authority control: National libraries 
Spain
France (data)
Germany
Israel
United States
Japan

Sources
 This article incorporates text derived from a free content work.  Licensed under C-BY-SA 3.0 IGO Licensed text taken from UNESCO Science Report: the Race Against Time for Smarter Development.,  Schneegans, S., T. Straza and J. Lewis (eds), UNESCO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see Wikipedia's terms of use.




Retrieved from ""https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1086360034""
Categories: Artificial intelligenceCyberneticsFormal sciencesComputational neuroscienceEmerging technologiesUnsolved problems in computer scienceComputational fields of studyHidden categories: Harv and Sfn no-target errorsArticles with short descriptionShort description is different from WikidataWikipedia indefinitely semi-protected pagesUse dmy dates from January 2018All articles with unsourced statementsArticles with unsourced statements from October 2021Articles with unsourced statements from September 2021CS1 Finnish-language sources (fi)CS1: Julian–Gregorian uncertaintyCS1: long volume valueWebarchive template wayback linksPages using Sister project links with hidden wikidataArticles with Internet Encyclopedia of Philosophy linksArticles with BNE identifiersArticles with BNF identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersFree-content attributionFree content from UNESCO

"
1,BLockchain,"




BLockchain

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search


Look for BLockchain on one of Wikipedia's sister projects:




Wiktionary (dictionary)



Wikibooks (textbooks)



Wikiquote (quotations)



Wikisource (library)



Wikiversity (learning resources)



Commons (media)



Wikivoyage (travel guide)



Wikinews (news source)



Wikidata (linked database)



Wikispecies (species directory)



Wikipedia does not have an article with this exact name. Please search for BLockchain in Wikipedia to check for alternative titles or spellings.
You need to log in or create an account to create this page.
Search for ""BLockchain"" in existing articles.
Look for pages within Wikipedia that link to this title.


Other reasons this message may be displayed:

If a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.
Titles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.
If the page has been deleted, check the deletion log, and see Why was the page I created deleted?.



Retrieved from ""https://en.wikipedia.org/wiki/BLockchain""


"
2,Internet_of_things,"




Internet of things

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Not to be confused with Web of Things.
Internet-like structure connecting everyday physical objects

The Internet of things (IoT) describes physical objects (or groups of such objects)  with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks.[1][2][3][4] Internet of things has been considered a misnomer because devices do not need to be connected to the public internet, they only need to be connected to a network and be individually addressable.[5][6]
The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning.[7]  Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.[8]  In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the ""smart home"", including devices and appliances (such as lighting fixtures, thermostats, home security systems, cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. IoT is also used in healthcare systems.[9]
There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of privacy and security, and consequently, industry and governmental moves to address these concerns have begun, including the development of international and local standards, guidelines, and regulatory frameworks.[10]

Contents

1 History
2 Applications

2.1 Consumer applications

2.1.1 Smart home
2.1.2 Elder care


2.2 Organizational applications

2.2.1 Medical and healthcare
2.2.2 Transportation
2.2.3 V2X communications
2.2.4 Building and home automation


2.3 Industrial applications

2.3.1 Manufacturing
2.3.2 Agriculture
2.3.3 Maritime


2.4 Infrastructure applications

2.4.1 Metropolitan scale deployments
2.4.2 Energy management
2.4.3 Environmental monitoring


2.5 Military applications

2.5.1 Internet of Battlefield Things
2.5.2 Ocean of Things


2.6 Product digitalization


3 Trends and characteristics

3.1 Intelligence
3.2 Architecture

3.2.1 Network architecture

3.2.1.1 Decentralized IoT




3.3 Complexity
3.4 Size considerations
3.5 Space considerations
3.6 A solution to ""basket of remotes""
3.7 Social Internet of things

3.7.1 Social Network for IoT Devices (Not Human)
3.7.2 How SIoT different from IoT?
3.7.3 How SIoT Works?
3.7.4 Social IoT Examples
3.7.5 Social IoT Challenges




4 Enabling technologies for IoT

4.1 Addressability
4.2 Application Layer
4.3 Short-range wireless
4.4 Medium-range wireless
4.5 Long-range wireless
4.6 Wired
4.7 Standards and standards organizations


5 Politics and civic engagement
6 Government regulation on IoT
7 Criticism, problems and controversies

7.1 Platform fragmentation
7.2 Privacy, autonomy, and control
7.3 Data storage
7.4 Security
7.5 Safety
7.6 Design
7.7 Environmental sustainability impact
7.8 Intentional obsolescence of devices
7.9 Confusing terminology


8 IoT adoption barriers

8.1 Lack of interoperability and unclear value propositions
8.2 Privacy and security concerns
8.3 Traditional governance structure
8.4 Business planning and project management


9 See also
10 References
11 Bibliography


History[edit]
The main concept of a network of smart devices was discussed as early as 1982, with a modified Coca-Cola vending machine at Carnegie Mellon University becoming the first ARPANET-connected appliance,[11] able to report its inventory and whether newly loaded drinks were cold or not.[12] Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT.[13][14] In 1994, Reza Raji described the concept in IEEE Spectrum as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories"".[15] Between 1993 and 1997, several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his ""Six Webs"" framework, presented at the World Economic Forum at Davos in 1999.[16]
The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C, published in September 1985.[17] According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.""
The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later MIT's Auto-ID Center, in 1999,[18] though he prefers the phrase ""Internet for things"".[19] At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things,[20] which would allow computers to manage all individual things.[21][22][23] The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.[24]
Defining the Internet of things as ""simply the point in time when more 'things or objects' were connected to the Internet than people"", Cisco Systems estimated that the IoT was ""born"" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.[25]

Applications[edit]
The extensive set of applications for IoT devices[26] is often divided into consumer, commercial, industrial, and infrastructure spaces.[27][28]

Consumer applications[edit]
A growing portion of IoT devices are created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.[29]

Smart home[edit]
IoT devices are a part of the larger concept of home automation, which can include lighting, heating and air conditioning, media and security systems and camera systems.[30][31] Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage.[32]

 A smart toilet seat that measures blood pressure, weight, pulse and oxygen levels.
A smart home or automated home could be based on a platform or hubs that control smart devices and appliances.[33] For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch.[34][35] This could be a dedicated app or iOS native applications such as Siri.[36] This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge.[36] There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products and these include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub.[37] In addition to the commercial systems, there are many non-proprietary, open source ecosystems; including Home Assistant, OpenHAB and Domoticz.[38][39]

Elder care[edit]
One key application of a smart home is to provide assistance to elderly individuals and to those with disabilities. These home systems use assistive technology to accommodate an owner's specific disabilities.[40] Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by hearing-impaired users.[41] They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures.[42] Smart home technology applied in this way can provide users with more freedom and a higher quality of life.[40]
The term ""Enterprise IoT"" refers to devices used in business and corporate settings. By 2019, it is estimated that the EIoT will account for 9.1 billion devices.[27]

Organizational applications[edit]
Medical and healthcare[edit]
The Internet of Medical Things (IoMT) is an application of the IoT for medical and health related purposes, data collection and analysis for research, and monitoring.[43][44][45][46][47] The IoMT has been referenced as ""Smart Healthcare"",[48] as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services.[49][50]
IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids.[51] Some hospitals have begun implementing ""smart beds"" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support is applied to the patient without the manual interaction of nurses.[43] A 2015 Goldman Sachs report indicated that healthcare IoT devices ""can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.""[52] Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used analyzed health statistics.""[53]
Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people to regain lost mobility via therapy as well.[54] These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems.[48] Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT.[55] End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.[56]
Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on paper or e-textiles for wireless powered disposable sensing devices.[57] Applications have been established for point-of-care medical diagnostics, where portability and low system-complexity is essential.[58]
As of 2018[update] IoMT was not only being applied in the clinical laboratory industry,[45] but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others, such as guardians of patients, nurses, families, and similar, to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to patient information.[59] Moreover, IoT-based systems are patient-centered, which involves being flexible to the patient's medical conditions.[citation needed] IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models.[60]
The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and applying complex algorithms in health data analysis.[61]

Transportation[edit]
 Digital variable speed-limit sign
The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e. the vehicle,[62] the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication,[63] smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance.[51][64]

V2X communications[edit]
Main article: V2X
In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle to vehicle communication (V2V), vehicle to infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to autonomous driving and connected road infrastructure.[citation needed]

Building and home automation[edit]
IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential)[51] in home automation and building automation systems. In this context, three main areas are being covered in literature:[65]

The integration of the Internet with building energy management systems in order to create energy-efficient and IOT-driven ""smart buildings"".[65]
The possible means of real-time monitoring for reducing energy consumption[32] and monitoring occupant behaviors.[65]
The integration of smart devices in the built environment and how they might be used in future applications.[65]
Industrial applications[edit]
Main article: Industrial internet of things
Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems. Also, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money.

Manufacturing[edit]
The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities.[66] Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control allow IoT to be used for industrial applications and smart manufacturing.[67] IoT intelligent systems enable rapid manufacturing and optimization of new products, and rapid response to product demands.[51]
Digital control systems to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the IIoT.[68] IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability.[69] Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.[51]
In addition to general manufacturing, IoT is also used for processes in the industrialization of construction.[70]

Agriculture[edit]
There are numerous IoT applications in farming[71] such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar, and even apply IoT-acquired data to precision fertilization programs.[72] The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs.
In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide.[73] The FarmBeats project[74] from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now.[75]

Maritime[edit]
IoT devices are in use monitoring the environments and systems of boats and yachts.[76] Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to a connected Android & Apple applications for example.

Infrastructure applications[edit]
Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind-farms is a key application of the IoT.[68] The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and save money with Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities in an efficient manner, by coordinating tasks between different service providers and users of these facilities.[51] IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. Usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure related areas.[77] Even areas such as waste management can benefit[78] from automation and optimization that could be brought in by the IoT.[citation needed]

Metropolitan scale deployments[edit]
There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first of its kind fully equipped and wired smart city, is gradually being built, with approximately 70 percent of the business district completed as of June 2018[update]. Much of the city is planned to be wired and automated, with little or no human intervention.[79]
Another application is currently undergoing a project in Santander, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search, environmental monitoring, digital city agenda, and more. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.[80]
Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City;[81] work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California;[82] and smart traffic management in western Singapore.[83] Using its RPMA (Random Phase Multiple Access) technology, San Diego-based Ingenu has built a nationwide public network[84] for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's ""Machine Network"" covers more than a third of the US population across 35 major cities including San Diego and Dallas.[85] French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in 2014, the first business to achieve such a deployment in the U.S.[86][87] It subsequently announced it would set up a total of 4000 base stations to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far.[88][89] Cisco also participates in smart cities projects. Cisco has started deploying technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada, India.[90]
Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.[91]

Energy management[edit]
Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance power generation but also helps optimize the energy consumption as a whole.[51] These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.).[51] The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity.[92] Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.[51]

Environmental monitoring[edit]
Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection[93] by monitoring air or water quality,[94] atmospheric or soil conditions,[95] and can even include areas like monitoring the movements of wildlife and their habitats.[96] Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile.[51] It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area.[97]
Living Lab
Another example of integrating the IoT is Living Lab which integrates and combines research and innovation processes, establishing within a public-private-people-partnership.[98] There are currently 320 Living Labs that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop IoT services for smart cities, they need to have incentives. The governments play key roles in smart city projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production process, and transaction costs.[98] The relationship between the technology developers and governments who manage the city's assets, is key to provide open access to resources to users in an efficient way.

Military applications[edit]
Main article: Internet of Military Things
The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield.[99]

Internet of Battlefield Things[edit]
The Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhance the capabilities of Army soldiers.[100] In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.[101][102]

Ocean of Things[edit]
The Ocean of Things project is a DARPA-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detect and track military and commercial vessels as part of a cloud-based network.[103]

Product digitalization[edit]
There are several applications of smart or active packaging in which a QR code or NFC tag is affixed on a product or its packaging. The tag itself is passive, however, it contains a unique identifier (typically a URL) which enables a user to access digital content about the product via a smartphone.[104] Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions.[105] The term ""Internet of Packaging"" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content.[106] Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive digital watermark or copy detection pattern for scanning when scanning a QR code,[107] while NFC tags can encrypt communication.[108]

Trends and characteristics[edit]
The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet.[109] The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.
The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.[110][111][112][113]
The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017[114] and it is estimated that there will be 30 billion devices by 2020.[109] The global market value of the IoT is projected to reach $7.1 trillion by 2020.[115]

Intelligence[edit]
Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT.[116] A promising approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment.[117] Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term.
IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and Cloud computing.[118] The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder.[119][118]
In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend,[120] clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.[121]

Architecture[edit]
This section needs attention from an expert in technology. The specific problem is: The information is partially outdated, unclear, and uncited. Requires more details, but not so technical that others won't understand it.. WikiProject Technology may be able to help recruit an expert. (July 2018)
IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud.[122] Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway.[122] The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing.[122] Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL).[122] The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers.[123] Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.[124]
Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.[citation needed]

Network architecture[edit]
The Internet of things requires huge scalability in the network space to handle the surge of devices.[125] IETF 6LoWPAN would be used to connect devices to IP networks. With billions of devices[126] being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT would provide lightweight data transport.
Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet.[127] The edge devices' computation power to analyse and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the internet to a server with sufficient processing power.[128]

Decentralized IoT[edit]
Decentralized Internet of things, or decentralized IoT, is a modified IoT. It utilizes Fog Computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers, and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices.[129]
Conventional IoT is connected via a mesh network and led by a major head node (centralized controller).[130] The head node decides how a data is created, stored, and transmitted.[131] In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions.[132] The head node authorizes partial decision making power to lower level sub-nodes under mutual agreed policy.[133] Performance is improved, especially for huge IoT systems with millions of nodes.[134]
Decentralized IoT attempts to address the limited bandwidth and hashing capacity of battery-powered or wireless IoT devices via lightweight blockchain.[135][136][137]
Cyberattack identification can be done through early detection and mitigation at the edge nodes with traffic monitoring and evaluation.[138]

Complexity[edit]
In semi-open or closed loops (i.e. value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system[139] due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality).
As a practical approach, not all elements in the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network.[140] Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.[141][142]

Size considerations[edit]
The Internet of things would encode 50 to 100 trillion objects, and be able to follow the movement of those objects. Human beings in surveyed urban environments are each surrounded by 1000 to 5000 trackable objects.[143] In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020.[31][144]
The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion.[114]

Space considerations[edit]
In the Internet of things, the precise geographic location of a thing—and also the precise geographic dimensions of a thing—will be critical.[145] Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things in the Internet of things will be sensors, and sensor location is usually important.[146]) The GeoWeb and Digital Earth are promising applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. In the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role in the Internet and the Web, geo-spatial standards will play a key role in the Internet of things.[147][148]

A solution to ""basket of remotes""[edit]
Many IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note,[149] where he predicts that the most likely problem will be what he calls the ""basket of remotes"" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another.[149] For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, ""where collected data is used to predict and trigger actions on the specific devices"" while making them work together.[150]

Social Internet of things[edit]
Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices.[151] SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services,[152] and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering.[153]

Social Network for IoT Devices (Not Human)[edit]
IoT defines a device with an identity like a citizen in a community, and connect them to the internet to provide services to its users.[154] SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human.[155]

How SIoT different from IoT?[edit]
SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users.[156]

How SIoT Works?[edit]
IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, navigates and groups with other IoT devices in the same or nearby network for useful service compositions in order to help its users proactively in every day's life especially during emergency.[157]

Social IoT Examples[edit]
IoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed.[158] In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action.[159]
IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help.[160]
Social IoT Challenges[edit]
Internet of things is multifaceted and complicated.[161] One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity.[162] Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation.[163]
System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place.[153]
Another critical challenge for SIoT is the accuracy and reliability of the sensors. At most of the circumstances, IoT sensors would need to respond in nanoseconds to avoid accidents, injury, and loss of life.[153]
Enabling technologies for IoT[edit]
There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:[164][165][166]

Addressability[edit]
The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI.[167] An alternative view, from the world of the Semantic Web[168] focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners.[169] Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required.[170][171][172]
Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6,[173] as it reduces the configuration overhead on the hosts,[171] and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.[172]

Application Layer[edit]
ADRC[174] defines an application layer protocol and supporting framework for implementing IoT applications.
Short-range wireless[edit]
Bluetooth mesh networking – Specification providing a mesh networking variant to Bluetooth low energy (BLE) with an increased number of nodes and standardized application layer (Models).
Light-Fidelity (Li-Fi) – Wireless communication technology similar to the Wi-Fi standard, but using visible light communication for increased bandwidth.
Near-field communication (NFC) – Communication protocols enabling two electronic devices to communicate within a 4 cm range.
Radio-frequency identification (RFID) – Technology using electromagnetic fields to read data stored in tags embedded in other items.
Wi-Fi – Technology for local area networking based on the IEEE 802.11 standard, where devices may communicate through a shared access point or directly between individual devices.
ZigBee – Communication protocols for personal area networking based on the IEEE 802.15.4 standard, providing low power consumption, low data rate, low cost, and high throughput.
Z-Wave – Wireless communications protocol used primarily for home automation and security applications
Medium-range wireless[edit]
LTE-Advanced – High-speed communication specification for mobile networks. Provides enhancements to the LTE standard with extended coverage, higher throughput, and lower latency.
5G - 5G wireless networks can be used to achieve the high communication requirements of the IoT and connect a large number of IoT devices, even when they are on the move.[175]
Long-range wireless[edit]
Low-power wide-area networking (LPWAN) – Wireless networks designed to allow long-range communication at a low data rate, reducing power and cost for transmission. Available LPWAN technologies and protocols: LoRaWan, Sigfox, NB-IoT, Weightless, RPMA.
Very small aperture terminal (VSAT) – Satellite communication technology using small dish antennas for narrowband and broadband data.
Wired[edit]
Ethernet – General purpose networking standard using twisted pair and fiber optic links in conjunction with hubs or switches.
Power-line communication (PLC) – Communication technology using electrical wiring to carry power and data. Specifications such as HomePlug or G.hn utilize PLC for networking IoT devices.
Standards and standards organizations[edit]
This is a list of technical standards for the IoT, most of which are open standards, and the standards organizations that aspire to successfully setting them.[176][177]



Short name

Long name

Standards under development

Other notes


Auto-ID Labs
Auto Identification Center
Networked RFID (radiofrequency identification) and emerging sensing technologies



Connected Home over IP
Project Connected Home over IP
Connected Home over IP (or Project Connected Home over IP) is an open-sourced, royalty-free home automation connectivity standard project which features compatibility among different smart home and Internet of things (IoT) products and software
The Connected Home over IP project group was launched and introduced by Amazon, Apple, Google,[178] Comcast and the Zigbee Alliance on December 18, 2019.[179] The project is backed by big companies and by being based on proven Internet design principles and protocols it aims to unify the currently fragmented systems.[180]


EPCglobal
Electronic Product code Technology
Standards for adoption of EPC (Electronic Product Code) technology



FDA
U.S. Food and Drug Administration
UDI (Unique Device Identification) system for distinct identifiers for medical devices



GS1
Global Standards One
Standards for UIDs (""unique"" identifiers) and RFID of fast-moving consumer goods (consumer packaged goods), health care supplies, and other things
The GS1 digital link standard,[181] first released in August 2018, allows the use QR Codes, GS1 Datamatrix, RFID and NFC to enable various types of business-to-business, as well as business-to-consumers interactions.


Parent organization comprises member organizations such as GS1 US


IEEE
Institute of Electrical and Electronics Engineers
Underlying communication technology standards such as IEEE 802.15.4, IEEE P1451-99[182] (IoT Harmonization), and IEEE P1931.1 (ROOF Computing).



IETF
Internet Engineering Task Force
Standards that comprise TCP/IP (the Internet protocol suite)



MTConnect Institute
—
MTConnect is a manufacturing industry standard for data exchange with machine tools and related industrial equipment. It is important to the IIoT subset of the IoT.



O-DF
Open Data Format
O-DF is a standard published by the Internet of Things Work Group of The Open Group in 2014, which specifies a generic information model structure that is meant to be applicable for describing any ""Thing"", as well as for publishing, updating and querying information when used together with O-MI (Open Messaging Interface).



O-MI
Open Messaging Interface
O-MI is a standard published by the Internet of Things Work Group of The Open Group in 2014, which specifies a limited set of key operations needed in IoT systems, notably different kinds of subscription mechanisms based on the Observer pattern.



OCF
Open Connectivity Foundation
Standards for simple devices using CoAP (Constrained Application Protocol)
OCF (Open Connectivity Foundation) supersedes OIC (Open Interconnect Consortium)


OMA

Open Mobile Alliance

OMA DM and OMA LWM2M for IoT device management, as well as GotAPI, which provides a secure framework for IoT applications




XSF
XMPP Standards Foundation
Protocol extensions of XMPP (Extensible Messaging and Presence Protocol), the open standard of instant messaging



W3C

World Wide Web Consortium

Standards for bringing interoperability between different IoT protocols and platforms such as Thing Description, Discovery , Scripting API and Architecture that explains how they work together.

Homepage of the Web of Things activity at the W3C at https://www.w3.org/WoT/


Politics and civic engagement[edit]
Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the ""ultimate beneficiaries"" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.[183]

Government regulation on IoT[edit]
One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems.[184] The other issues pertain to consumer choice and ownership of data[185] and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop.[186][187][188] IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.[189]
Current regulatory environment:
A report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations:[190]

Data security – At the time of designing IoT companies should ensure that data collection, storage and processing would be secure at all times. Companies should adopt a ""defense in depth"" approach and encrypt data at each stage.[191]
Data consent – users should have a choice as to what data they share with IoT companies and the users must be informed if their data gets exposed.
Data minimisation – IoT companies should collect only the data they need and retain the collected information only for a limited time.
However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.[192]
A resolution passed by the Senate in March 2015, is already being considered by the Congress.[193] This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices.
Approved on 28 September 2018, California Senate Bill No. 327[194] goes into effect on 1 January 2020. The bill requires ""a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,""
Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.[195]
A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT.[196] These include –

Still early days for the IoT in government 
Underdeveloped policy and regulatory frameworks 
Unclear business models, despite strong value proposition 
Clear institutional and capacity gap in government AND the private sector 
Inconsistent data valuation and management 
Infrastructure a major barrier 
Government as an enabler 
Most successful pilots share common characteristics (public-private partnership, local, leadership)
In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.[197]

Criticism, problems and controversies[edit]
Platform fragmentation[edit]
The IoT suffers from platform fragmentation, lack of interoperability and common technical standards[198][199][200][201][202][203][204][excessive citations] a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard.[1] For example, wireless connectivity for IoT devices can be done using Bluetooth, Zigbee, Z-Wave, LoRa, NB-IoT, Cat M1 as well as completely custom proprietary radios – each with its own advantages and disadvantages; and unique support ecosystem.[205]
The IoT's amorphous computing nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices.[206][207][208] One set of researchers say that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable.[209][210]

Privacy, autonomy, and control[edit]
Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation.[211]
Concerns about privacy have led many to consider the possibility that big data infrastructures such as the Internet of things and data mining are inherently incompatible with privacy.[212] Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and cybersecurity which necessitate an adequate response from research and policymakers alike.[213]
Writer Adam Greenfield claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement.
The Internet of Things Council compared the increased prevalence of digital surveillance due to the Internet of things to the conceptual panopticon described by Jeremy Bentham in the 18th Century.[214] The assertion was defended by the works of French philosophers Michel Foucault and Gilles Deleuze. In Discipline and Punish: The Birth of the Prison Foucault asserts that the panopticon was a central element of the discipline society developed during the Industrial Era.[215] Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of panopticism.[215] In his 1992 paper ""Postscripts on the Societies of Control,"" Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism.[216]
Peter-Paul Verbeek, a professor of philosophy of technology at the University of Twente, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent.[217]
Justin Brookman, of the Center for Democracy and Technology, expressed concern regarding the impact of the IoT on consumer privacy, saying that ""There are some people in the commercial space who say, 'Oh, big data – well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.""[218]
Tim O'Reilly believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the ""IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.""[219]
Editorials at WIRED have also expressed concern, one stating ""What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.""[220]
The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that ""There's simply no way to forecast how these immense powers – disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control – will be used. Chances are big data and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.""[221]
In response to rising concerns about privacy and smart technology, in 2007 the British Government stated it would follow formal Privacy by Design principles when implementing their smart metering program. The program would lead to replacement of traditional power meters with smart power meters, which could track and manage energy usage more accurately.[222] However the British Computer Society is doubtful these principles were ever actually implemented.[223] In 2009 the Dutch Parliament rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011.[223]

Data storage[edit]
A challenge for producers of IoT applications is to clean, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks.[224] These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data.[225]
Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. Currently the Internet is already responsible for 5% of the total energy generated,[224] and a ""daunting challenge to power"" IoT devices to collect and even store data still remains.[226]

Security[edit]
Security is the biggest concern in adopting Internet of things technology,[227] with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved[228] and the regulatory changes that might be necessary.[229][230]
Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones.[231] These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, SQL injections, Man-in-the-middle attacks, and poor handling of security updates.[232][233] However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices[234] - and the low price and consumer focus of many devices makes a robust security patching system uncommon.[235]
Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault.[236]
Internet of things devices also have access to new areas of data, and can often control physical devices,[237] so that even by 2014 it was possible to say that many Internet-connected appliances could already ""spy on people in their own homes"" including televisions, kitchen appliances,[238] cameras, and thermostats.[239] Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely.[240] By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps[241] and implantable cardioverter defibrillators.[242]
Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a distributed denial of service attack powered by Internet of things devices running the Mirai malware took down a DNS provider and major web sites.[243] The Mirai Botnet had infected roughly 65,000 IoT devices within the first 20 hours.[244] Eventually the infections increased to around 200,000 to 300,000 infections.[244] Brazil, Colombia and Vietnam made up of 41.5% of the infections.[244] The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers.[244] Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik.[244] In May 2017, Junade Ali, a Computer Scientist at Cloudflare noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the Publish–subscribe pattern.[245][246] These sorts of attacks have caused security experts to view IoT as a real threat to Internet services.[247]
The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny ""access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel sensor fusion may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.""[248] In general, the intelligence community views the Internet of things as a rich source of data.[249]
On 31 January 2019, the Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: ""Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password""[250]
There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway.[251] As per the estimates from KBV Research,[252] the overall IoT security market[253] would grow at 27.9% rate during 2016–2022 as a result of growing infrastructural concerns and diversified usage of Internet of things.[254][255]
Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet – as market incentives to secure IoT devices is insufficient.[256][229][230] It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by Man-in-the-middle attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys.[257]

Safety[edit]
IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation.[258] Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings,[259] Apple's HomeKit,[260] and Amazon's Alexa,[261] among others.
A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., ""unlock the entrance door when no one is at home"" or ""turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night"".[258] Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal ""interaction-level"" flaws by identifying events that can lead the system to unsafe states.[258] They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties).

Design[edit]
Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for ""anarchic scalability.""[262] Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure.[262]
Brown University computer scientist Michael Littman has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: ""If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.""[263]

Environmental sustainability impact[edit]
A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices.[264] Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime.[265]

Intentional obsolescence of devices[edit]
The Electronic Frontier Foundation has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or ""brick"" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of a ""Lifetime Subscription"" were rendered useless after Nest Labs acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate.[266] As Nest is a company owned by Alphabet (Google's parent company), the EFF argues this sets a ""terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.""[267]
Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States DMCA section 1201, which only has an exemption for ""local use"". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own.[267]
Examples of post-sale manipulations include Google Nest Revolv, disabled privacy settings on Android, Sony disabling Linux on PlayStation 3, enforced EULA on Wii U.[267]

Confusing terminology[edit]
Kevin Lonergan at Information Age, a business technology magazine, has referred to the terms surrounding the IoT as a ""terminology zoo"".[268] The lack of clear terminology is not ""useful from a practical point of view"" and a ""source of confusion for the end user"".[268] A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics.[268] According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars,[139] cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT).[268] Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a ""common and reusable vocabulary of terms""[269] to ensure ""consistent terminology""[269][270] across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert[271] to be notified when a new term is published. As of March 2020[update], this database aggregates 807 IoT-related terms, while keeping material ""transparent and comprehensive.""[272][273]

IoT adoption barriers[edit]
 GE Digital CEO William Ruh speaking about GE's attempts to gain a foothold in the market for IoT services at the first IEEE Computer Society TechIgnite conference
Lack of interoperability and unclear value propositions[edit]
Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users.[274] A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle ""to pinpoint exactly where the value of IoT lies for them"".[275]

Privacy and security concerns[edit]
As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the “things” around the user can cooperate to provide better services that fulfill personal preference.[276] When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network.[277]
For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors.[278]
Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorized parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorized parties even with the denial-of-service (DOS) attacks.[279]
Information privacy regulations also require organizations to practice ""reasonable security"". California's SB-327 Information privacy: connected devices ""would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure, as specified.""[280] As each organization's environment is unique, it can prove challenging to demonstrate what ""reasonable security"" is and what potential risks could be involved for the business. Oregon's HB 2395 also ""requires [a] person that manufactures, sells or offers to sell connected device] manufacturer to equip connected device with reasonable security features that protect connected device and information that connected device collects, contains, stores or transmits] stores from access, destruction, modification, use or disclosure that consumer does not authorize.""[281]
According to antivirus provider Kaspersky, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021.[197]

Traditional governance structure[edit]
 Town of Internet of Things in Hangzhou, China
A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a ""clash between IoT and companies' traditional governance structures, as IoT still presents both uncertainties and a lack of historical precedence.""[275] Among the respondents interviewed, 60 percent stated that they ""do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.""[275] This has led to a need to understand organizational culture in order to facilitate organizational design processes and to test new innovation management practices. A lack of digital leadership in the age of digital transformation has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, ""were waiting for the market dynamics to play out"",[275] or further action in regards to IoT ""was pending competitor moves, customer pull, or regulatory requirements.""[275] Some of these companies risk being ""kodaked"" – ""Kodak was a market leader until digital disruption eclipsed film photography with digital photos"" – failing to ""see the disruptive forces affecting their industry""[282] and ""to truly embrace the new business models the disruptive change opens up.""[282] Scott Anthony has written in Harvard Business Review that Kodak ""created a digital camera, invested in the technology, and even understood that photos would be shared online""[282] but ultimately failed to realize that ""online photo sharing was the new business, not just a way to expand the printing business.""[282]

Business planning and project management[edit]
According to 2018 study, 70–75% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning.[283][page needed][284]
Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed. IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects:[285]

A separate research and development phase 
A Proof-of-Concept/Prototype before the actual project begins 
Project managers with interdisciplinary technical knowledge 
Universally defined business and technical jargon
See also[edit]

5G
Artificial intelligence of things
Automotive security
Big Data
Cloud manufacturing
Cyber-physical system
Data Distribution Service
Digital object memory
Digital twin
Edge computing
Four-dimensional product
Home automation
Indoor positioning system
Industry 4.0
Internet of Military Things
IoT Cloud
IoT Simulation
Open Interconnect Consortium
OpenWSN
Quantified self
Responsive computer-aided design
Smart grid
Web of things
Thread (network protocol)
Matter (standard)

References[edit]


^ a b Gillis, Alexander (2021). ""What is internet of things (IoT)?"". IOT Agenda. Retrieved 17 August 2021.

^ Brown, Eric (20 September 2016). ""21 Open Source Projects for IoT"". Linux.com. Retrieved 23 October 2016.

^ ""Internet of Things Global Standards Initiative"". ITU. Retrieved 26 June 2015.

^ Hendricks, Drew. ""The Trouble with the Internet of Things"". London Datastore. Greater London Authority. Retrieved 10 August 2015.

^ Internet of things and big data analytics toward next-generation intelligence. Nilanjan Dey, Aboul Ella Hassanien, Chintan Bhatt, Amira Ashour, Suresh Chandra Satapathy. Cham, Switzerland. 2018. p. 440. ISBN 978-3-319-60435-0. OCLC 1001327784.{{cite book}}:  CS1 maint: others (link)

^ ""Forecast: The Internet of Things, Worldwide, 2013"". Gartner. Retrieved 3 March 2022.

^ Hu, J.; Niu, H.; Carrasco, J.; Lennox, B.; Arvin, F., ""Fault-tolerant cooperative navigation of networked UAV swarms for forest fire monitoring"" Aerospace Science and Technology, 2022.

^ Hu, J.; Lennox, B.; Arvin, F., ""Robust formation control for networked robotic systems using Negative Imaginary dynamics"" Automatica, 2022.

^ Laplante, Phillip A.; Kassab, Mohamad; Laplante, Nancy L.; Voas, Jeffrey M. (2018). ""Building Caring Healthcare Systems in the Internet of Things"". IEEE Systems Journal. 12 (3): 3030–3037. Bibcode:2018ISysJ..12.3030L. doi:10.1109/JSYST.2017.2662602. ISSN 1932-8184. PMC 6506834. PMID 31080541.

^ ""The New York City Internet of Things Strategy"". www1.nyc.gov. Retrieved 6 September 2021.

^ ""The ""Only"" Coke Machine on the Internet"". Carnegie Mellon University. Retrieved 10 November 2014.

^ ""Internet of Things Done Wrong Stifles Innovation"". InformationWeek. 7 July 2014. Retrieved 10 November 2014.

^ Mattern, Friedemann; Floerkemeier, Christian (2010). ""From the Internet of Computer to the Internet of Things"" (PDF). Informatik-Spektrum. 33 (2): 107–121. Bibcode:2009InfSp..32..496H. doi:10.1007/s00287-010-0417-7. hdl:20.500.11850/159645. S2CID 29563772. Retrieved 3 February 2014.

^ Weiser, Mark (1991). ""The Computer for the 21st Century"" (PDF). Scientific American. 265 (3): 94–104. Bibcode:1991SciAm.265c..94W. doi:10.1038/scientificamerican0991-94. Archived from the original (PDF) on 11 March 2015. Retrieved 5 November 2014.

^ Raji, R.S. (1994). ""Smart networks for control"". IEEE Spectrum. 31 (6): 49–55. doi:10.1109/6.284793. S2CID 42364553.

^ Pontin, Jason (29 September 2005). ""ETC: Bill Joy's Six Webs"". MIT Technology Review. Retrieved 17 November 2013.

^ ""CORRECTING THE IOT HISTORY"". CHETAN SHARMA. 14 March 2016. Retrieved 1 June 2021.

^ Ashton, K. (22 June 2009). ""That 'Internet of Things' Thing"". Retrieved 9 May 2017.

^ ""Peter Day's World of Business"". BBC World Service. BBC. Retrieved 4 October 2016.

^ Magrassi, P. (2 May 2002). ""Why a Universal RFID Infrastructure Would Be a Good Thing"". Gartner research report G00106518.

^ Magrassi, P.; Berg, T (12 August 2002). ""A World of Smart Objects"". Gartner research report R-17-2243. Archived from the original on 3 October 2003.

^ Commission of the European Communities (18 June 2009). ""Internet of Things – An action plan for Europe"" (PDF). COM(2009) 278 final.

^ Wood, Alex (31 March 2015). ""The internet of things is revolutionizing our lives, but standards are a must"". The Guardian.

^ Stallings, William (2016). Foundations of modern networking : SDN, NFV, QoE, IoT, and Cloud. Florence Agboma, Sofiene Jelassi. Indianapolis, Indiana. ISBN 978-0-13-417547-8. OCLC 927715441.

^ Dave Evans (April 2011). ""The Internet of Things: How the Next Evolution of the Internet Is Changing Everything"" (PDF). CISCO White Paper.

^ Vongsingthong, S.; Smanchat, S. (2014). ""Internet of Things: A review of applications & technologies"" (PDF). Suranaree Journal of Science and Technology.

^ a b ""The Enterprise Internet of Things Market"". Business Insider. 25 February 2015. Retrieved 26 June 2015.

^ Perera, C.; Liu, C. H.; Jayawardena, S. (December 2015). ""The Emerging Internet of Things Marketplace From an Industrial Perspective: A Survey"". IEEE Transactions on Emerging Topics in Computing. 3 (4): 585–598. arXiv:1502.00134. Bibcode:2015arXiv150200134P. doi:10.1109/TETC.2015.2390034. ISSN 2168-6750. S2CID 7329149.

^ ""How IoT's are Changing the Fundamentals of ""Retailing"""". Trak.in – Indian Business of Tech, Mobile & Startups. 30 August 2016. Retrieved 2 June 2017.

^ Kang, Won Min; Moon, Seo Yeon; Park, Jong Hyuk (5 March 2017). ""An enhanced security framework for home appliances in smart home"". Human-centric Computing and Information Sciences. 7 (6). doi:10.1186/s13673-017-0087-4.

^ a b ""How IoT & smart home automation will change the way we live"". Business Insider. Retrieved 10 November 2017.

^ a b Jussi Karlgren; Lennart Fahlén; Anders Wallberg; Pär Hansson; Olov Ståhl; Jonas Söderberg; Karl-Petter Åkesson (2008). Socially Intelligent Interfaces for Increased Energy Awareness in the Home. The Internet of Things. Lecture Notes in Computer Science. Vol. 4952. Springer. pp. 263–275. arXiv:2106.15297. doi:10.1007/978-3-540-78731-0_17. ISBN 978-3-540-78730-3. S2CID 30983428.

^ Greengard, Samuel (2015). The Internet of Things. Cambridge, MA: MIT Press. p. 90. ISBN 9780262527736.

^ ""HomeKit – Apple Developer"". developer.apple.com. Retrieved 19 September 2018.

^ Wollerton, Megan (3 June 2018). ""Here's everything you need to know about Apple HomeKit"". CNET. Retrieved 19 September 2018.

^ a b Lovejoy, Ben (31 August 2018). ""HomeKit devices getting more affordable as Lenovo announces Smart Home Essentials line"". 9to5Mac. Retrieved 19 September 2018.

^ Prospero, Mike (12 September 2018). ""Best Smart Home Hubs of 2018"". Tom's Guide. Retrieved 19 September 2018.

^ Chinchilla, Chris (26 November 2018). ""What Smart Home IoT Platform Should You Use?"". Hacker Noon. Retrieved 13 May 2019.

^ Baker, Jason (14 December 2017). ""6 open source home automation tools"". opensource.com. Retrieved 13 May 2019.

^ a b Demiris, G; Hensel, K (2008). ""Technologies for an Aging Society: A Systematic Review of 'Smart Home' Applications"". IMIA Yearbook of Medical Informatics 2008. 17: 33–40. doi:10.1055/s-0038-1638580. PMID 18660873. S2CID 7244183.

^ Aburukba, Raafat; Al-Ali, A. R.; Kandil, Nourhan; AbuDamis, Diala (10 May 2016). Configurable ZigBee-based control system for people with multiple disabilities in smart homes. pp. 1–5. doi:10.1109/ICCSII.2016.7462435. ISBN 978-1-4673-8743-9. S2CID 16754386.

^ Mulvenna, Maurice; Hutton, Anton; Martin, Suzanne; Todd, Stephen; Bond, Raymond; Moorhead, Anne (14 December 2017). ""Views of Caregivers on the Ethics of Assistive Technology Used for Home Surveillance of People Living with Dementia"". Neuroethics. 10 (2): 255–266. doi:10.1007/s12152-017-9305-z. PMC 5486509. PMID 28725288.

^ a b da Costa, CA; Pasluosta, CF; Eskofier, B; da Silva, DB; da Rosa Righi, R (July 2018). ""Internet of Health Things: Toward intelligent vital signs monitoring in hospital wards"". Artificial Intelligence in Medicine. 89: 61–69. doi:10.1016/j.artmed.2018.05.005. PMID 29871778. S2CID 46941758.

^ Engineer, A; Sternberg, EM; Najafi, B (21 August 2018). ""Designing Interiors to Mitigate Physical and Cognitive Deficits Related to Aging and to Promote Longevity in Older Adults: A Review"". Gerontology. 64 (6): 612–622. doi:10.1159/000491488. PMID 30130764. S2CID 52056959. 

^ a b Kricka, LJ (2019). ""History of disruptions in laboratory medicine: what have we learned from predictions?"". Clinical Chemistry and Laboratory Medicine. 57 (3): 308–311. doi:10.1515/cclm-2018-0518. PMID 29927745. S2CID 49354315.

^ Gatouillat, Arthur; Badr, Youakim; Massot, Bertrand; Sejdic, Ervin (2018). ""Internet of Medical Things: A Review of Recent Contributions Dealing with Cyber-Physical Systems in Medicine"" (PDF). IEEE Internet of Things Journal. 5 (5): 3810–3822. doi:10.1109/jiot.2018.2849014. ISSN 2327-4662. S2CID 53440449.

^ Topol, Eric (2016). The Patient Will See You Now: The Future of Medicine Is in Your Hands. Basic Books. ISBN 978-0465040025.

^ a b Dey, Nilanjan; Hassanien, Aboul Ella; Bhatt, Chintan; Ashour, Amira S.; Satapathy, Suresh Chandra (2018). Internet of things and big data analytics toward next-generation intelligence (PDF). Springer International Publishing. ISBN 978-3-319-60434-3. Retrieved 14 October 2018.

^ Pratap Singh, R.; Javaid, M.; Haleem, A.; Vaishya, R.; Ali, S. (2020). ""Internet of Medical Things (IoMT) for orthopaedic in COVID-19 pandemic: Roles, challenges, and applications"". Journal of Clinical Orthopaedics and Trauma. 11 (4): 713–717. doi:10.1016/j.jcot.2020.05.011. PMC 7227564. PMID 32425428.

^ ""Deloitte Centre for Health Solutions"" (PDF). Deloitte.

^ a b c d e f g h i j Ersue, M.; Romascanu, D.; Schoenwaelder, J.; Sehgal, A. (4 July 2014). ""Management of Networks with Constrained Devices: Use Cases"". IETF Internet Draft.

^ ""Goldman Sachs Report: How the Internet of Things Can Save the American Healthcare System $305 Billion Annually"". Engage Mobile Blog. Engage Mobile Solutions, LLC. 23 June 2016. Retrieved 26 July 2018.

^ World Health Organization. ""mHealth. New horizons for health through mobile technologies"" (PDF). World Health Organization. Retrieved 3 January 2020.

^ Istepanian, R.; Hu, S.; Philip, N.; Sungoor, A. (2011). ""The potential of Internet of m-health Things ""m-IoT"" for non-invasive glucose level sensing"". 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference. Vol. 2011. pp. 5264–6. doi:10.1109/IEMBS.2011.6091302. ISBN 978-1-4577-1589-1. PMID 22255525. S2CID 995488.

^ Swan, Melanie (8 November 2012). ""Sensor Mania! The Internet of Things, Wearable Computing, Objective Metrics, and the Quantified Self 2.0"". Journal of Sensor and Actuator Networks. 1 (3): 217–253. doi:10.3390/jsan1030217.

^ Taiwan Information Strategy, Internet and E-commerce Development Handbook - Strategic Information, Regulations, Contacts. IBP, Inc. USA. 2016. p. 79. ISBN 978-1514521021.

^ Grell, Max; Dincer, Can; Le, Thao; Lauri, Alberto; Nunez Bajo, Estefania; Kasimatis, Michael; Barandun, Giandrin; Maier, Stefan A.; Cass, Anthony E. G. (2019). ""Autocatalytic Metallization of Fabrics Using Si Ink, for Biosensors, Batteries and Energy Harvesting"". Advanced Functional Materials. 29 (1): 1804798. doi:10.1002/adfm.201804798. ISSN 1616-301X. PMC 7384005. PMID 32733177.

^ Dincer, Can; Bruch, Richard; Kling, André; Dittrich, Petra S.; Urban, Gerald A. (1 August 2017). ""Multiplexed Point-of-Care Testing – xPOCT"". Trends in Biotechnology. 35 (8): 728–742. doi:10.1016/j.tibtech.2017.03.013. ISSN 0167-7799. PMC 5538621. PMID 28456344.

^ ""What is HIE? | HealthIT.gov"". www.healthit.gov. Retrieved 21 January 2020.

^ Amiot, Emmanuel. ""The Internet of Things. Disrupting Traditional Business Models"" (PDF). Oliver Wyman. Retrieved 14 October 2018.

^ Vermesan, Ovidiu, and Peter Friess, eds. Internet of things: converging technologies for smart environments and integrated ecosystems. River Publisher, 2013. https://www.researchgate.net/publication/272943881

^ Mahmud, Khizir; Town, Graham E.; Morsalin, Sayidul; Hossain, M.J. (February 2018). ""Integration of electric vehicles and management in the internet of energy"". Renewable and Sustainable Energy Reviews. 82: 4179–4203. doi:10.1016/j.rser.2017.11.004.

^ Xie, Xiao-Feng; Wang, Zun-Jing (2017). ""Integrated in-vehicle decision support system for driving at signalized intersections: A prototype of smart IoT in transportation"". Transportation Research Board (TRB) Annual Meeting, Washington, DC, USA.

^ ""Key Applications of the Smart IoT to Transform Transportation"". 20 September 2016.

^ a b c d Haase, Jan; Alahmad, Mahmoud; Nishi, Hiroaki; Ploennigs, Joern; Tsang, Kim Fung (2016). ""The IOT mediated built environment: A brief survey"". 2016 IEEE 14th International Conference on Industrial Informatics (INDIN). pp. 1065–1068. doi:10.1109/INDIN.2016.7819322. ISBN 978-1-5090-2870-2. S2CID 5554635.

^ Yang, Chen; Shen, Weiming; Wang, Xianbin (January 2018). ""The Internet of Things in Manufacturing: Key Issues and Potential Applications"". IEEE Systems, Man, and Cybernetics Magazine. 4 (1): 6–15. doi:10.1109/MSMC.2017.2702391. S2CID 42651835.

^ Severi, S.; Abreu, G.; Sottile, F.; Pastrone, C.; Spirito, M.; Berens, F. (23–26 June 2014). ""M2M Technologies: Enablers for a Pervasive Internet of Things"". The European Conference on Networks and Communications (EUCNC2014).

^ a b Gubbi, Jayavardhana; Buyya, Rajkumar; Marusic, Slaven; Palaniswami, Marimuthu (24 February 2013). ""Internet of Things (IoT): A vision, architectural elements, and future directions"". Future Generation Computer Systems. 29 (7): 1645–1660. arXiv:1207.0203. doi:10.1016/j.future.2013.01.010. S2CID 204982032.

^ Tan, Lu; Wang, Neng (20–22 August 2010). Future Internet: The Internet of Things. 3rd International Conference on Advanced Computer Theory and Engineering (ICACTE). Vol. 5. pp. 376–380. doi:10.1109/ICACTE.2010.5579543. ISBN 978-1-4244-6539-2. S2CID 40587.

^ ""Industrialized Construction in Academia"" (PDF). Autodesk.

^ Meola, A. (20 December 2016). ""Why IoT, big data & smart farming are the future of agriculture"". Business Insider. Insider, Inc. Retrieved 26 July 2018.

^ Zhang, Q. (2015). Precision Agriculture Technology for Crop Farming. CRC Press. pp. 249–58. ISBN 9781482251081.

^ ""Google goes bilingual, Facebook fleshes out translation and TensorFlow is dope ~ And, Microsoft is assisting fish farmers in Japan"". The Register.

^ Vasisht, Deepak; Kapetanovic, Zerina; Won, Jongho; Jin, Xinxin; Chandra, Ranveer; Sinha, Sudipta; Kapoor, Ashish; Sudarshan, Madhusudhan; Stratman, Sean (2017). FarmBeats: An IoT Platform for Data-Driven Agriculture. pp. 515–529. ISBN 978-1-931971-37-9.

^ ""FarmBeats: AI, Edge & IoT for Agriculture"". Microsoft Research. Retrieved 28 June 2021.

^ ""Monitoring apps: How the Internet of Things can turn your boat into a smart boat"". Yachting World. 9 March 2020.

^ Chui, Michael; Löffler, Markus; Roberts, Roger. ""The Internet of Things"". McKinsey Quarterly. McKinsey & Company. Retrieved 10 July 2014.

^ ""Smart Trash"". Postscapes. Retrieved 10 July 2014.

^ Poon, L. (22 June 2018). ""Sleepy in Songdo, Korea's Smartest City"". CityLab. Atlantic Monthly Group. Retrieved 26 July 2018.

^ Rico, Juan (22–24 April 2014). ""Going beyond monitoring and actuating in large scale smart cities"". NFC & Proximity Solutions – WIMA Monaco.

^ ""A vision for a city today, a city of vision tomorrow"". Sino-Singapore Guangzhou Knowledge City. Retrieved 11 July 2014.

^ ""San Jose Implements Intel Technology for a Smarter City"". Intel Newsroom. Retrieved 11 July 2014.

^ ""Western Singapore becomes test-bed for smart city solutions"". Coconuts Singapore. 19 June 2014. Retrieved 11 July 2014.

^ Higginbotham, Stacey. ""A group of wireless execs aim to build a nationwide network for the Internet of things"". Fortune.com. Retrieved 8 June 2019.

^ Freeman, Mike (9 September 2015). ""On-Ramp Wireless becomes Ingenu, launches nationwide IoT network"". SanDiegoUnionTribune.com. Retrieved 8 June 2019.

^ Lipsky, Jessica. ""IoT Clash Over 900 MHz Options"". EETimes. Retrieved 15 May 2015.

^ Alleven, Monica. ""Sigfox launches IoT network in 10 UK cities"". Fierce Wireless Tech. Retrieved 13 May 2015.

^ Merritt, Rick. ""13 Views of IoT World"". EETimes. Retrieved 15 May 2015.

^ Fitchard, Kevin (20 May 2014). ""Sigfox brings its internet of things network to San Francisco"". Gigaom. Retrieved 15 May 2015.

^ Ujaley, Mohd (25 July 2018). ""Cisco To Invest in Fiber Grid, IoT, Smart Cities in Andhra Pradesh"". ProQuest 1774166769.

^ ""STE Security Innovation Awards Honorable Mention: The End of the Disconnect"". securityinfowatch.com. Retrieved 12 August 2015.

^ Parello, J.; Claise, B.; Schoening, B.; Quittek, J. (28 April 2014). ""Energy Management Framework"". IETF Internet Draft.

^ Davies, Nicola. ""How the Internet of Things will enable 'smart buildings'"". Extreme Tech.

^ ""Molluscan eye"". Retrieved 26 June 2015.

^ Li, Shixing; Wang, Hong; Xu, Tao; Zhou, Guiping (2011). Application Study on Internet of Things in Environment Protection Field (Submitted manuscript). Lecture Notes in Electrical Engineering. Vol. 133. pp. 99–106. doi:10.1007/978-3-642-25992-0_13. ISBN 978-3-642-25991-3.

^ ""Use case: Sensitive wildlife monitoring"". FIT French Project. Archived from the original on 14 July 2014. Retrieved 10 July 2014.

^ Hart, Jane K.; Martinez, Kirk (1 May 2015). ""Toward an environmental Internet of Things"". Earth and Space Science. 2 (5): 194–200. Bibcode:2015E&SS....2..194H. doi:10.1002/2014EA000044.

^ a b Scuotto, Veronica; Ferraris, Alberto; Bresciani, Stefano (4 April 2016). ""Internet of Things"". Business Process Management Journal. 22 (2): 357–367. doi:10.1108/bpmj-05-2015-0074. ISSN 1463-7154.

^ Cameron, Lori. ""Internet of Things Meets the Military and Battlefield: Connecting Gear and Biometric Wearables for an IoMT and IoBT"". IEEE Computer Society. Retrieved 31 October 2019.

^ ""Army Takes on Wicked Problems With the Internet of Battlefield Things"". MeriTalk. 30 January 2018. Retrieved 31 October 2019.

^ Gudeman, Kim (6 October 2017). ""Next-Generation Internet of Battle things (IoBT) Aims to Help Keep Troops and Civilians Safe"". ECE Illinois. Retrieved 31 October 2019.

^ ""Internet of Battlefield Things (IOBT)"". CCDC Army Research Laboratory. Retrieved 31 October 2019.

^ ""DARPA Floats a Proposal for the Ocean of Things"". MeriTalk. 3 January 2018. Retrieved 31 October 2019.

^ ""How to make smart packaging even smarter"". Packaging Digest. 4 June 2018. Retrieved 28 April 2020.

^ foodnavigator-asia.com. ""Connecting with consumers: The benefits - and dangers - of smart packaging for the F&B industry"". foodnavigator-asia.com. Retrieved 28 April 2020.

^ confectionerynews.com. ""Which smart packaging technologies are readily available in 2018"". confectionerynews.com. Retrieved 28 April 2020.

^ Chen, Changsheng; Li, Mulin; Ferreira, Anselmo; Huang, Jiwu; Cai, Rizhao (2020). ""A Copy-Proof Scheme Based on the Spectral and Spatial Barcoding Channel Models"". IEEE Transactions on Information Forensics and Security. 15: 1056–1071. doi:10.1109/tifs.2019.2934861. ISSN 1556-6013. S2CID 201903693.

^ ""MIT unveils battery-free crypto tag for anti-counterfeit"". www.securingindustry.com. 26 February 2020. Retrieved 28 April 2020.

^ a b Nordrum, Amy (18 August 2016). ""Popular Internet of Things Forecast of 50 Billion Devices by 2020 Is Outdated"". IEEE Spectrum.

^ Vermesan, Ovidiu; Friess, Peter (2013). Internet of Things: Converging Technologies for Smart Environments and Integrated Ecosystems (PDF). Aalborg, Denmark: River Publishers. ISBN 978-87-92982-96-4.

^ Santucci, Gérald. ""The Internet of Things: Between the Revolution of the Internet and the Metamorphosis of Objects"" (PDF). European Commission Community Research and Development Information Service. Retrieved 23 October 2016.

^ Mattern, Friedemann; Floerkemeier, Christian. ""From the Internet of Computers to the Internet of Things"" (PDF). ETH Zurich. Retrieved 23 October 2016.

^ Lindner, Tim (13 July 2015). ""The Supply Chain: Changing at the Speed of Technology"". Connected World. Retrieved 18 September 2015.

^ a b Köhn, Rüdiger. ""Online-Kriminalität: Konzerne verbünden sich gegen Hacker"". Faz.net.

^ Hsu, Chin-Lung; Lin, Judy Chuan-Chuan (2016). ""An empirical examination of consumer adoption of Internet of Things services: Network externalities and concern for information privacy perspectives"". Computers in Human Behavior. 62: 516–527. doi:10.1016/j.chb.2016.04.023.

^ ""Smarter Things: The Autonomous IoT"". GDR Blog. GDR Creative Intelligence. 5 January 2018. Retrieved 26 July 2018.

^ Levine, Sergey; Finn, Chelsea; Darrell, Trevor; Abbeel, Pieter (2016). ""End-to-End Training of Deep Visuomotor Policies"" (PDF). The Journal of Machine Learning Research. 17 (1): 1334–1373. arXiv:1504.00702. Bibcode:2015arXiv150400702L.

^ a b Mohammadi, Mehdi; Al-Fuqaha, Ala; Sorour, Sameh; Guizani, Mohsen (2018). ""Deep Learning for IoT Big Data and Streaming Analytics: A Survey"". IEEE Communications Surveys & Tutorials. 20 (4): 2923–2960. arXiv:1712.04301. doi:10.1109/COMST.2018.2844341. S2CID 9461213.

^ Mahdavinejad, Mohammad Saeid; Rezvan, Mohammadreza; Barekatain, Mohammadamin; Adibi, Peyman; Barnaghi, Payam; Sheth, Amit P. (2018). ""Machine learning for internet of things data analysis: A survey"". Digital Communications and Networks. 4 (3): 161–175. arXiv:1802.06305. Bibcode:2018arXiv180206305S. doi:10.1016/j.dcan.2017.10.002. S2CID 2666574.

^ Alippi, C. (2014). Intelligence for Embedded Systems. Springer Verlag. ISBN 978-3-319-05278-6.

^ Delicato, F.C.; Al-Anbuky, A.; Wang, K., eds. (2018). Smart Cyber-Physical Systems: towards Pervasive Intelligence systems. Future Generation Computer Systems. Elsevier. Retrieved 26 July 2018.

^ a b c d Traukina, Alena; Thomas, Jayant; Tyagi, Prashant; Reddipalli, Kishore (29 September 2018). Industrial Internet Application Development: Simplify IIoT development using the elasticity of Public Cloud and Native Cloud Services (1st ed.). Packt Publishing. p. 18.

^ Hassan, Qusay; Khan, Atta; Madani, Sajjad (2018). Internet of Things: Challenges, Advances, and Applications. Boca Raton, Florida: CRC Press. p. 198. ISBN 9781498778510.

^ Chauhuri, Abhik (2018). Internet of Things, for Things, and by Things. Boca Raton, Florida: CRC Press. ISBN 9781138710443.

^ Pal, Arpan (May–June 2015). ""Internet of Things: Making the Hype a Reality"" (PDF). IT Pro. 17 (3): 2–4. doi:10.1109/MITP.2015.36. Retrieved 10 April 2016.

^ ""Gartner Says 6.4 Billion Connected ""Things"" Will Be in Use in 2016, Up 30 Percent From 2015"". Gartner. 10 November 2015. Archived from the original on 12 November 2015. Retrieved 21 April 2016.

^ Reza Arkian, Hamid (2017). ""MIST: Fog-based Data Analytics Scheme with Cost-Efficient Resource Provisioning for IoT Crowdsensing Applications"". Journal of Network and Computer Applications. 82: 152–165. doi:10.1016/j.jnca.2017.01.012.

^ ""IoT The outer Edge Computing"". June 2019. Retrieved 3 June 2019. {{cite journal}}: Cite journal requires |journal= (help)

^ Cui, Laizhong; Yang, Shu; Chen, Ziteng; Pan, Yi; Ming, Zhong; Xu, Mingwei (May 2020). ""A Decentralized and Trusted Edge Computing Platform for Internet of Things"". IEEE Internet of Things Journal. 7 (5): 3910–3922. doi:10.1109/JIOT.2019.2951619. ISSN 2327-4662. S2CID 209097962.

^ Nguyen, Tien-Dung; Huh, Eui-Nam; Jo, Minho (June 2019). ""Decentralized and Revised Content-Centric Networking-Based Service Deployment and Discovery Platform in Mobile Edge Computing for IoT Devices"". IEEE Internet of Things Journal. 6 (3): 4162–4175. doi:10.1109/JIOT.2018.2875489. ISSN 2327-4662. S2CID 69250756.

^ Xiong, Zehui; Zhang, Yang; Luong, Nguyen Cong; Niyato, Dusit; Wang, Ping; Guizani, Nadra (January 2020). ""The Best of Both Worlds: A General Architecture for Data Management in Blockchain-enabled Internet-of-Things"". IEEE Network. 34 (1): 166–173. doi:10.1109/MNET.001.1900095. ISSN 1558-156X. S2CID 211050783.

^ Alhaizaey, Yousef; Singer, Jeremy; Michala, Anna Lito (June 2021). ""Optimizing Task Allocation for Edge Micro-Clusters in Smart Cities"". 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM): 341–347. doi:10.1109/WoWMoM51794.2021.00062. ISBN 978-1-6654-2263-5. S2CID 235780952.

^ Guo, Hongzhi; Liu, Jiajia; Qin, Huiling (January 2018). ""Collaborative Mobile Edge Computation Offloading for IoT over Fiber-Wireless Networks"". IEEE Network. 32 (1): 66–71. doi:10.1109/MNET.2018.1700139. ISSN 1558-156X. S2CID 12479631.

^ Messaoud, Seifeddine; Bradai, Abbas; Bukhari, Syed Hashim Raza; Quang, Pham Tran Anh; Ahmed, Olfa Ben; Atri, Mohamed (1 December 2020). ""A survey on machine learning in Internet of Things: Algorithms, strategies, and applications"". Internet of Things. 12: 100314. doi:10.1016/j.iot.2020.100314. ISSN 2542-6605. S2CID 228876304.

^ Cherupally, Sumanth Reddy; Boga, Srinivas; Podili, Prashanth; Kataoka, Kotaro (January 2021). ""Lightweight and Scalable DAG based distributed ledger for verifying IoT data integrity"". 2021 International Conference on Information Networking (ICOIN): 267–272. doi:10.1109/ICOIN50884.2021.9334000. ISBN 978-1-7281-9101-0. S2CID 231825899.

^ Fan, Xinxin; Chai, Qi; Xu, Lei; Guo, Dong (6 October 2020). ""DIAM-IoT: A Decentralized Identity and Access Management Framework for Internet of Things"". Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure. BSCI '20. Taipei, Taiwan: Association for Computing Machinery: 186–191. doi:10.1145/3384943.3409436. ISBN 978-1-4503-7610-5. S2CID 222142832.

^ Durand, Arnaud; Gremaud, Pascal; Pasquier, Jacques (22 October 2017). ""Decentralized web of trust and authentication for the internet of things"". Proceedings of the Seventh International Conference on the Internet of Things. IoT '17. Linz, Austria: Association for Computing Machinery: 1–2. doi:10.1145/3131542.3140263. ISBN 978-1-4503-5318-2. S2CID 3645848.

^ Rathore, Shailendra; Wook Kwon, Byung; Park, Jong Hyuk (1 October 2019). ""BlockSecIoTNet: Blockchain-based decentralized security architecture for IoT network"". Journal of Network and Computer Applications. 143: 167–177. doi:10.1016/j.jnca.2019.06.019. ISSN 1084-8045. S2CID 198365021.

^ a b Gautier, Philippe; Gonzalez, Laurent (2011). L'Internet des Objets... Internet, mais en mieux (PDF). Foreword by Gérald Santucci (European commission), postword by Daniel Kaplan (FING) and Michel Volle. Paris: AFNOR editions. ISBN 978-2-12-465316-4.

^ Marginean, M.-T.; Lu, C. (2016). ""sDOMO communication protocol for home robotic systems in the context of the internet of things"". Computer Science, Technology And Application. World Scientific. pp. 151–60. ISBN 9789813200432.

^ Montazerolghaem, Ahmadreza (2021). ""Software-defined Internet of Multimedia Things: Energy-efficient and Load-balanced Resource Management"". IEEE Internet of Things Journal. 9 (3): 2432–2442. doi:10.1109/JIOT.2021.3095237. ISSN 2327-4662. S2CID 237801052.

^ Rowayda, A. Sadek (May 2018). ""– An Agile Internet of Things (IoT) based Software Defined Network (SDN) Architecture"" (PDF). Egyptian Computer Science Journal.

^ Waldner, Jean-Baptiste (2007). Nanoinformatique et intelligence ambiante. Inventer l'Ordinateur du XXIeme Siècle. London: Hermes Science. p. 254. ISBN 978-2-7462-1516-0.

^ Montazerolghaem, Ahmadreza; Yaghmaee, Mohammad Hossein (April 2020). ""Load-Balanced and QoS-Aware Software-Defined Internet of Things"". IEEE Internet of Things Journal. 7 (4): 3323–3337. doi:10.1109/JIOT.2020.2967081. ISSN 2327-4662. S2CID 214551067.

^ ""OGC SensorThings API standard specification"". OGC. Retrieved 15 February 2016.

^ ""OGC Sensor Web Enablement: Overview And High Level Architecture"". OGC. Retrieved 15 February 2016.

^ Minteer, A. (2017). ""Chapter 9: Applying Geospatial Analytics to IoT Data"". Analytics for the Internet of Things (IoT). Packt Publishing. pp. 230–57. ISBN 9781787127579.

^ van der Zee, E.; Scholten, H. (2014). ""Spatial Dimensions of Big Data: Application of Geographical Concepts and Spatial Technology to the Internet of Things"".  In Bessis, N.; Dobre, C. (eds.). Big Data and Internet of Things: A Roadmap for Smart Environments. Springer. pp. 137–68. ISBN 9783319050294.

^ a b Gassée, J.-L. (12 January 2014). ""Internet of Things: The ""Basket of Remotes"" Problem"". Monday Note. Retrieved 26 June 2015.

^ de Sousa, M. (2015). ""Chapter 10: Integrating with Muzzley"". Internet of Things with Intel Galileo. Packt Publishing. p. 163. ISBN 9781782174912.

^ ""Social IoT"". Enabling the Internet of Things. ieeexplore.ieee.org. 2021. pp. 195–211. doi:10.1002/9781119701460.ch9. ISBN 9781119701255. S2CID 240696468. Retrieved 9 July 2021.

^ Saleem, Yasir; Crespi, Noel; Pace, Pasquale (April 2018). ""SCDIoT: Social Cross-Domain IoT Enabling Application-to-Application Communications"". 2018 IEEE International Conference on Cloud Engineering (IC2E). Orlando, FL: IEEE: 346–350. doi:10.1109/IC2E.2018.00068. ISBN 978-1-5386-5008-0. S2CID 21720322.

^ a b c Afzal, Bilal; Umair, Muhammad; Asadullah Shah, Ghalib; Ahmed, Ejaz (March 2019). ""Enabling IoT platforms for social IoT applications: Vision, feature mapping, and challenges"". Future Generation Computer Systems. 92: 718–731. doi:10.1016/j.future.2017.12.002. S2CID 57379503.

^ Bhatia, Munish; Sood, Sandeep K. (June 2020). ""Quantum Computing-Inspired Network Optimization for IoT Applications"". IEEE Internet of Things Journal. 7 (6): 5590–5598. doi:10.1109/JIOT.2020.2979887. ISSN 2327-4662. S2CID 215845606.

^ Cheng, Wai Khuen; Ileladewa, Adeoye Abiodun; Tan, Teik Boon (January 2019). ""A Personalized Recommendation Framework for Social Internet of Things (SIoT)"". 2019 International Conference on Green and Human Information Technology (ICGHIT): 24–29. doi:10.1109/ICGHIT.2019.00013. ISBN 978-1-7281-0627-4. S2CID 204702019.

^ Atzori, Luigi; Iera, Antonio; Morabito, Giacomo; Nitti, Michele (14 November 2012). ""The Social Internet of Things (SIoT) – When social networks meet the Internet of Things: Concept, architecture and network characterization"". Computer Networks. 56 (16): 3594–3608. doi:10.1016/j.comnet.2012.07.010. ISSN 1389-1286.

^ Khelloufi, Amar; Ning, Huansheng; Dhelim, Sahraoui; Qiu, Tie; Ma, Jianhua; Huang, Runhe; Atzori, Luigi (1 February 2021). ""A Social-Relationships-Based Service Recommendation System for SIoT Devices"". IEEE Internet of Things Journal. 8 (3): 1859–1870. doi:10.1109/JIOT.2020.3016659. ISSN 2327-4662. S2CID 226476576.

^ Miori, Vittorio; Russo, Dario (June 2017). ""Improving life quality for the elderly through the Social Internet of Things (SIoT)"". 2017 Global Internet of Things Summit (GIoTS). Geneva, Switzerland: IEEE: 1–6. doi:10.1109/GIOTS.2017.8016215. ISBN 978-1-5090-5873-0. S2CID 7475703.

^ Udawant, Omkar; Thombare, Nikhil; Chauhan, Devanand; Hadke, Akash; Waghole, Dattatray (December 2017). ""Smart ambulance system using IoT"". 2017 International Conference on Big Data, IoT and Data Science (BID). Pune, India: IEEE: 171–176. doi:10.1109/BID.2017.8336593. ISBN 978-1-5090-6593-6. S2CID 4865714.

^ Saleem, Yasir; Crespi, Noel; Rehmani, Mubashir Husain; Copeland, Rebecca; Hussein, Dina; Bertin, Emmanuel (December 2016). ""Exploitation of social IoT for recommendation services"". 2016 IEEE 3rd World Forum on Internet of Things (WF-IoT). Reston, VA, USA: IEEE: 359–364. doi:10.1109/WF-IoT.2016.7845500. ISBN 978-1-5090-4130-5. S2CID 206866361.

^ Andrade, Rossana M.C.; Aragão, Belmondo R.; Oliveira, Pedro Almir M.; Maia, Marcio E.F.; Viana, Windson; Nogueira, Tales P. (April 2021). ""Multifaceted infrastructure for self-adaptive IoT systems"". Information and Software Technology. 132: 106505. doi:10.1016/j.infsof.2020.106505. S2CID 231731945.

^ Farahbakhsh, Bahareh; Fanian, Ali; Manshaei, Mohammad Hossein (March 2021). ""TGSM: Towards trustworthy group-based service management for social IoT"". Internet of Things. 13: 100312. doi:10.1016/j.iot.2020.100312. ISSN 2542-6605. S2CID 228806944.

^ Iqbal, Muhammad Azhar; Hussain, Sajjad; Xing, Huanlai; Imran, Muhammad (February 2021). Enabling the Internet of Things: Fundamentals, Design, and Applications (1 ed.). Wiley. doi:10.1002/9781119701460.ch9. ISBN 978-1-119-70125-5. S2CID 240696468.

^ Want, Roy; Schilit, Bill N.; Jenson, Scott (2015). ""Enabling the Internet of Things"". Computer. 48: 28–35. doi:10.1109/MC.2015.12. S2CID 17384656.

^ ""The Internet of Things: a jumbled mess or a jumbled mess?"". The Register. Retrieved 5 June 2016.

^ ""Can we talk? Internet of Things vendors face a communications 'mess'"". Computerworld. 18 April 2014. Retrieved 5 June 2016.

^ Hassan, Q.F. (2018). Internet of Things A to Z: Technologies and Applications. John Wiley & Sons. pp. 27–8. ISBN 9781119456759.

^ Dan Brickley et al., c. 2001

^ Sheng, M.; Qun, Y.; Yao, L.; Benatallah, B. (2017). Managing the Web of Things: Linking the Real World to the Web. Morgan Kaufmann. pp. 256–8. ISBN 9780128097656.

^ Waldner, Jean-Baptiste (2008). Nanocomputers and Swarm Intelligence. London: ISTE. pp. 227–231. ISBN 978-1-84704-002-2.

^ a b Kushalnagar, N.; Montenegro, G.; Schumacher, C. (August 2007). IPv6 over Low-Power Wireless Personal Area Networks (6LoWPANs): Overview, Assumptions, Problem Statement, and Goals. IETF. doi:10.17487/RFC4919. RFC 4919.

^ a b Sun, Charles C. (1 May 2014). ""Stop using Internet Protocol Version 4!"". Computerworld.

^ Thomson, S.; Narten, T.; Jinmei, T. (September 2007). IPv6 Stateless Address Autoconfiguration. IETF. doi:10.17487/RFC4862. RFC 4862.

^ Xped Limited, ADRC Overview"", from Wikipedia

^ Alsulami, M. M.; Akkari, N. (April 2018). ""The role of 5G wireless networks in the internet-of- things (IoT)"". 2018 1st International Conference on Computer Applications Information Security (ICCAIS): 1–8. doi:10.1109/CAIS.2018.8471687. ISBN 978-1-5386-4427-0. S2CID 52897932.

^ Jing, J.; Li, H. (2012). ""Research on the Relevant Standards of Internet of Things"".  In Wang, Y.; Zhang, X. (eds.). Internet of Things: International Workshop, IOT 2012. Springer. pp. 627–32. ISBN 9783642324277.

^ Mahmood, Z. (2018). Connected Environments for the Internet of Things: Challenges and Solutions. Springer. pp. 89–90. ISBN 9783319701028.

^ ""Project Connected Home over IP"". Google Developers Blog. Retrieved 16 September 2020.

^ Mihalcik, Carrie. ""Apple, Amazon, Google, and others want to create a new standard for smart home tech"". CNET. Retrieved 24 December 2019.

^ Strategy, Moor Insights and. ""CHIP Shot: Will Project Connected Home Over IP Get Us Onto The IoT Green?"". Forbes. Retrieved 3 September 2020.

^ ""Digital Link - Standards | GS1"". www.gs1.org. 12 November 2018. Retrieved 28 April 2020.

^ ""P1451-99 - Standard for Harmonization of Internet of Things (IoT) Devices and Systems"". IEEE. Retrieved 26 July 2021.

^ Howard, Philip N. (1 June 2015). ""The Internet of Things is Posed to Change Democracy Itself"". Politico. Retrieved 8 August 2017.

^ Thompson, Kirsten; Mattalo, Brandon (24 November 2015). ""The Internet of Things: Guidance, Regulation and the Canadian Approach"". CyberLex. Retrieved 23 October 2016.

^ ""The Question of Who Owns the Data Is About to Get a Lot Trickier"". Fortune. 6 April 2016. Retrieved 23 October 2016.

^ Weber, R.H.; Weber, R. (2010). Internet of Things: Legal Perspectives. Springer Science & Business Media. pp. 59–64. ISBN 9783642117107.

^ Hassan, Q.F. (2018). Internet of Things A to Z: Technologies and Applications. John Wiley & Sons. pp. 41–4. ISBN 9781119456759.

^ Hassan, Q.F.; Khan, A. ur R.; Madani, S.A. (2017). Internet of Things: Challenges, Advances, and Applications. CRC Press. pp. 41–2. ISBN 9781498778534.

^ Lopez, Javier; Rios, Ruben; Bao, Feng; Wang, Guilin (2017). ""Evolving privacy: From sensors to the Internet of Things"". Future Generation Computer Systems. 75: 46–57. doi:10.1016/j.future.2017.04.045.

^ ""The 'Internet of Things': Legal Challenges in an Ultra-connected World"". Mason Hayes & Curran. 22 January 2016. Retrieved 23 October 2016.

^ Brown, Ian (2015). ""Regulation and the Internet of Things"" (PDF). Oxford Internet Institute. Retrieved 23 October 2016.

^ ""FTC Report on Internet of Things Urges Companies to Adopt Best Practices to Address Consumer Privacy and Security Risks"". Federal Trade Commission. 27 January 2015. Retrieved 23 October 2016.

^ Lawson, Stephen (2 March 2016). ""IoT users could win with a new bill in the US Senate"". Tech Barrista. Retrieved 9 December 2019.

^ ""California Legislative Information – SB-327 Information privacy: connected devices"".

^ Pittman, F. Paul (2 February 2016). ""Legal Developments in Connected Car Arena Provide Glimpse of Privacy and Data Security Regulation in Internet of Things"". Lexology. Retrieved 23 October 2016.

^ Rasit, Yuce, Mehmet; Claus, Beisswenger, Stefan; Mangalam, Srikanth; Das, Prasanna, Lal; Martin, Lukac (2 November 2017). ""Internet of things : the new government to business platform – a review of opportunities, practices, and challenges"": 1–112. {{cite journal}}: Cite journal requires |journal= (help)

^ a b Page, Carly (4 December 2021). ""Is the UK government's new IoT cybersecurity bill fit for purpose?"". TechCrunch. Retrieved 4 December 2021.

^ Wieland, Ken (25 February 2016). ""IoT experts fret over fragmentation"". Mobile World.

^ Wallace, Michael (19 February 2016). ""Fragmentation is the enemy of the Internet of Things"". Qualcomm.com.

^ Bauer, Harald; Patel, Mark; Veira, Jan (October 2015). ""Internet of Things: Opportunities and challenges for semiconductor companies"". McKinsey & Co.

^ Ardiri, Aaron (8 July 2014). ""Will fragmentation of standards only hinder the true potential of the IoT industry?"". evothings.com.

^ ""IOT Brings Fragmentation in Platform"" (PDF). arm.com.

^ Raggett, Dave (27 April 2016). ""Countering Fragmentation with the Web of Things: Interoperability across IoT platforms"" (PDF). W3C.

^ Kovach, Steve (30 July 2013). ""Android Fragmentation Report"". Business Insider. Retrieved 19 October 2013.

^ ""Ultimate Guide to Internet of Things (IoT) Connectivity"".

^ Piedad, Floyd N. ""Will Android fragmentation spoil its IoT appeal?"". TechBeacon.

^ Franceschi-Bicchierai, Lorenzo (29 July 2015). ""Goodbye, Android"". Motherboard. Vice.

^ Kingsley-Hughes, Adrian. ""The toxic hellstew survival guide"". ZDnet. Retrieved 2 August 2015.

^ Tung, Liam (13 October 2015). ""Android security a 'market for lemons' that leaves 87 percent vulnerable"". ZDNet. Retrieved 14 October 2015.

^ Thomas, Daniel R.; Beresford, Alastair R.; Rice, Andrew (2015). Proceedings of the 5th Annual ACM CCS Workshop on Security and Privacy in Smartphones and Mobile Devices – SPSM '15 (PDF). Computer Laboratory, University of Cambridge. pp. 87–98. doi:10.1145/2808117.2808118. ISBN 9781450338196. S2CID 14832327. Retrieved 14 October 2015.

^ Howard, Philip N. (2015). Pax Technica: How the internet of things May Set Us Free, Or Lock Us Up. New Haven, CT: Yale University Press. ISBN 978-0-30019-947-5.

^ McEwan, Adrian (2014). ""Designing the Internet of Things"" (PDF). Retrieved 1 June 2016.

^ Moy de Vitry, Matthew; Schneider, Mariane; Wani, Omar; Liliane, Manny; Leitao, João P.; Eggimann, Sven (2019). ""Smart urban water systems: what could possibly go wrong?"". Environmental Research Letters. 14 (8): 081001. Bibcode:2019ERL....14h1001M. doi:10.1088/1748-9326/ab3761.

^ ""Panopticon as a metaphor for the internet of things"" (PDF). The Council of the Internet of Things. Retrieved 6 June 2016.

^ a b ""Foucault"" (PDF). UCLA.

^ ""Deleuze – 1992 – Postscript on the Societies of Control"" (PDF). UCLA.

^ Verbeek, Peter-Paul (2011). Moralizing Technology: Understanding and Designing the Morality of Things. Chicago: The University of Chicago Press. ISBN 978-0-22685-291-1.

^ Cardwell, Diane (18 February 2014). ""At Newark Airport, the Lights Are On, and They're Watching You"". The New York Times.

^ Hardy, Quentin (4 February 2015). ""Tim O'Reilly Explains the Internet of Things"". The New York Times.

^ Webb, Geoff (5 February 2015). ""Say Goodbye to Privacy"". WIRED. Retrieved 15 February 2015.

^ Crump, Catherine; Harwood, Matthew (25 March 2014). ""The Net Closes Around Us"". TomDispatch.

^ Brown, Ian (12 February 2013). ""Britain's Smart Meter Programme: A Case Study in Privacy by Design"". International Review of Law, Computers & Technology. 28 (2): 172–184. doi:10.1080/13600869.2013.801580. S2CID 62756630. SSRN 2215646.

^ a b ""The Societal Impact of the Internet of Things"" (PDF). British Computer Society. 14 February 2013. Retrieved 23 October 2016.

^ a b Gubbi, Jayavardhana; Buyya, Rajkumar; Marusic, Slaven; Palaniswami, Marimuthu (1 September 2013). ""Internet of Things (IoT): A vision, architectural elements, and future directions"". Future Generation Computer Systems. Including Special sections: Cyber-enabled Distributed Computing for Ubiquitous Cloud and Network Services & Cloud Computing and Scientific Applications – Big Data, Scalable Analytics, and Beyond. 29 (7): 1645–1660. arXiv:1207.0203. doi:10.1016/j.future.2013.01.010. S2CID 204982032.

^ Acharjya, D.P.; Ahmed, N.S.S. (2017). ""Recognizing Attacks in Wireless Sensor Network in View of Internet of Things"".  In Acharjya, D.P.; Geetha, M.K. (eds.). Internet of Things: Novel Advances and Envisioned Applications. Springer. pp. 149–50. ISBN 9783319534725.

^ Hussain, A. (June 2017). ""Energy Consumption of Wireless IoT Nodes"" (PDF). Norwegian University of Science and Technology. Retrieved 26 July 2018.

^ ""We Asked Executives About The Internet of Things And Their Answers Reveal That Security Remains A Huge Concern"". Business Insider. Retrieved 26 June 2015.

^ Singh, Jatinder; Pasquier, Thomas; Bacon, Jean; Ko, Hajoon; Eyers, David (2015). ""Twenty Cloud Security Considerations for Supporting the Internet of Things"". IEEE Internet of Things Journal. 3 (3): 1. doi:10.1109/JIOT.2015.2460333. S2CID 4732406.

^ a b Clearfield, Chris. ""Why The FTC Can't Regulate The Internet of Things"". Forbes. Retrieved 26 June 2015.

^ a b Feamster, Nick (18 February 2017). ""Mitigating the Increasing Risks of an Insecure Internet of Things"". Freedom to Tinker. Retrieved 8 August 2017.

^ Li, S. (2017). ""Chapter 1: Introduction: Securing the Internet of Things"".  In Li, S.; Xu, L.D. (eds.). Securing the Internet of Things. Syngress. p. 4. ISBN 9780128045053.

^ Bastos, D.; Shackleton, M.; El-Moussa, F. (2018). ""Internet of Things: A Survey of Technologies and Security Risks in Smart Home and City Environments"". Living in the Internet of Things: Cybersecurity of the IoT - 2018. pp. 30 (7 pp.). doi:10.1049/cp.2018.0030. ISBN 9781785618437.

^ Harbi, Yasmine; Aliouat, Zibouda; Harous, Saad; Bentaleb, Abdelhak; Refoufi, Allaoua (September 2019). ""A Review of Security in Internet of Things"". Wireless Personal Communications. 108 (1): 325–344. doi:10.1007/s11277-019-06405-y. ISSN 0929-6212. S2CID 150181134.

^ Liu, Ximeng; Yang, Yang; Choo, Kim-Kwang Raymond; Wang, Huaqun (24 September 2018). ""Security and Privacy Challenges for Internet-of-Things and Fog Computing"". Wireless Communications and Mobile Computing. 2018: 1–3. doi:10.1155/2018/9373961. ISSN 1530-8669.

^ Morrissey, Janet (22 January 2019). ""In the Rush to Join the Smart Home Crowd, Buyers Should Beware"". The New York Times. ISSN 0362-4331. Retrieved 26 February 2020.

^ Ahmadi, Mohsen; Kiaei, Pantea; Emamdoost, Navid (2021). SN4KE: Practical Mutation Testing at Binary Level (PDF) (MSc). NDSS Symposium 2021.

^ Clearfield, Christopher (26 June 2013). ""Rethinking Security for the Internet of Things"". Harvard Business Review Blog.

^ Witkovski, Adriano; Santin, Altair; Abreu, Vilmar; Marynowski, Joao (2014). ""An IdM and Key-Based Authentication Method for Providing Single Sign-On in IoT"". 2015 IEEE Global Communications Conference (GLOBECOM). pp. 1–6. doi:10.1109/GLOCOM.2014.7417597. ISBN 978-1-4799-5952-5. S2CID 8108114.

^ Steinberg, Joseph (27 January 2014). ""These Devices May Be Spying on You (Even in Your Own Home)"". Forbes. Retrieved 27 May 2014.

^ Greenberg, Andy (21 July 2015). ""Hackers Remotely Kill a Jeep on the Highway—With Me in It"". Wired. Retrieved 21 July 2015.

^ Scientific American, April 2015, p.68.

^ Loukas, George (June 2015). Cyber-Physical Attacks A growing invisible threat. Oxford, UK: Butterworh-Heinemann (Elsevier). p. 65. ISBN 9780128012901.

^ Woolf, Nicky (26 October 2016). ""DDoS attack that disrupted internet was largest of its kind in history, experts say"". The Guardian.

^ a b c d e Antonakakis, Manos; April, Tim; Bailey, Michael; Bernhard, Matt; Bursztein, Elie; Cochran, Jaime; Durumeric, Zakir; Halderman, J. Alex; Invernizzi, Luca (18 August 2017). Understanding the Mirai Botnet (PDF). Usenix. ISBN 978-1-931971-40-9. Retrieved 13 May 2018.

^ ""The ""anti-patterns"" that turned the IoT into the Internet of Shit / Boing Boing"". boingboing.net. 3 May 2017.

^ Ali, Junade (2 May 2017). ""IoT Security Anti-Patterns"". Cloudflare Blog.

^ Schneier, Bruce (6 October 2016). ""We Need to Save the Internet from the Internet of Things"". Motherboard.

^ ""Disruptive Technologies Global Trends 2025"" (PDF). National Intelligence Council (NIC). April 2008. p. 27.

^ Ackerman, Spencer (15 March 2012). ""CIA Chief: We'll Spy on You Through Your Dishwasher"". WIRED. Retrieved 26 June 2015.

^ https://www.facebook.com/geoffreyfowler. ""The doorbells have eyes: The privacy battle brewing over home security cameras"". Washington Post. Retrieved 3 February 2019. {{cite news}}: |last= has generic name (help); External link in |last= (help)

^ ""Building the Web of Things – Mozilla Hacks – the Web developer blog"". Mozilla Hacks – the Web developer blog.

^ ""The Step Towards Innovation"".

^ ""Global IoT Security Market to reach a market size of $29.2 billion by 2022"".

^ Ward, Mark (23 September 2015). ""Smart devices to get security tune-up"". BBC News.

^ ""Executive Steering Board"". IoT Security Foundation.

^ Schneier, Bruce (1 February 2017). ""Security and the Internet of Things"".

^ Alfandi, Omar; Hasan, Musaab; Balbahaith, Zayed (2019), ""Assessment and Hardening of IoT Development Boards"", Lecture Notes in Computer Science, Springer International Publishing, pp. 27–39, doi:10.1007/978-3-030-30523-9_3, ISBN 978-3-030-30522-2, S2CID 202550425

^ a b c Nguyen, Dang Tu; Song, Chengyu; Qian, Zhiyun; V. Krishnamurthy, Srikanth; J. M. Colbert, Edward; McDaniel, Patrick (2018). IoTSan: Fortifying the Safety of IoT Systems. Proc. of the 14th International Conference on emerging Networking EXperiments and Technologies (CoNEXT '18). Heraklion, Greece. arXiv:1810.09551. doi:10.1145/3281411.3281440. arXiv:1810.09551.

^ ""SmartThings"". SmartThings.com.

^ ""HomeKit – Apple Developer"". developer.apple.com.

^ ""Amazon Alexa"". developer.amazon.com.

^ a b Fielding, Roy Thomas (2000). ""Architectural Styles and the Design of Network-based Software Architectures"" (PDF). University of California, Irvine.

^ Littman, Michael; Kortchmar, Samuel (11 June 2014). ""The Path to a Programmable World"". Footnote. Retrieved 14 June 2014.

^ Finley, Klint (6 May 2014). ""The Internet of Things Could Drown Our Environment in Gadgets"". Wired.

^ Light, A.; Rowland, C. (2015). ""Chapter 11: Responsible IoT Design"".  In Rowland, C.; Goodman, E.; Charlier, M.;  et al. (eds.). Designing Connected Products: UX for the Consumer Internet of Things. O'Reilly Media. pp. 457–64. ISBN 9781449372569.

^ Gilbert, Arlo (3 April 2016). ""The time that Tony Fadell sold me a container of hummus"". Retrieved 7 April 2016.

^ a b c Walsh, Kit (5 April 2016). ""Nest Reminds Customers That Ownership Isn't What It Used to Be"". Electronic Frontier Foundation. Retrieved 7 April 2016.

^ a b c d ""Taming the IoT terminology zoo: what does it all mean?"". Information Age. Vitesse Media Plc. 30 July 2015.

^ a b ""Technology Working Group"". The Industrial Internet Consortium. Retrieved 21 March 2017.

^ ""Vocabulary Technical Report"". The Industrial Internet Consortium. Retrieved 21 March 2017.

^ ""Acceleration Sensing"". IoT One. Retrieved 21 March 2017.

^ ""IoT Terms Database"". IoT One. Retrieved 21 March 2017.

^ ""Quick Guide"". IoT ONE. Retrieved 26 July 2018.

^ ""Why The Consumer Internet of Things Is Stalling"". Forbes. Retrieved 24 March 2017.

^ a b c d e ""Every. Thing. Connected. A study of the adoption of 'Internet of Things' among Danish companies"" (PDF). Ericsson. Retrieved 2 May 2020.

^ Zhang, Zhi-Kai; Cho, Michael Cheng Yi; Wang, Chia-Wei; Hsu, Chia-Wei; Chen, Chong-Kuan; Shieh, Shiuhpyng (2014). ""IoT Security: Ongoing Challenges and Research Opportunities"". 2014 IEEE 7th International Conference on Service-Oriented Computing and Applications. pp. 230–234. doi:10.1109/SOCA.2014.58. ISBN 978-1-4799-6833-6. S2CID 18445510.

^ Khan, Minhaj Ahmad; Salah, Khaled (2018). ""IoT security: Review, blockchain solutions, and open challenges"". Future Generation Computer Systems. 82: 395–411. doi:10.1016/j.future.2017.11.022.

^ Zhou, Wei; Jia, Yan; Peng, Anni; Zhang, Yuqing; Liu, Peng (2019). ""The Effect of IoT New Features on Security and Privacy: New Threats, Existing Solutions, and Challenges Yet to be Solved"". IEEE Internet of Things Journal. 6 (2): 1606–1616. arXiv:1802.03110. doi:10.1109/JIOT.2018.2847733. S2CID 31057653.

^ Supriya, S.; Padaki, Sagar (2016). ""Data Security and Privacy Challenges in Adopting Solutions for IOT"". 2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData). pp. 410–415. doi:10.1109/iThings-GreenCom-CPSCom-SmartData.2016.97. ISBN 978-1-5090-5880-8. S2CID 34661195.

^ ""California Legislative Information"".

^ ""Oregon State Legislature"".

^ a b c d Anthony, Scott (15 July 2016). ""Disruptive Innovation: Kodak's Downfall Wasn't About Technology"". Harvard Business Review. Harvard Business Publishing. Retrieved 30 March 2017.

^ ""World Economic Forum: The Next Economic Growth Engine – Scaling Fourth Industrial Revolution Technologies in Production"" (PDF). World Economic Forum. January 2018. p. 4.

^ at 11:15, Kat Hall 23 May 2017. ""Three-quarters of IoT projects are failing, says Cisco"". www.theregister.co.uk. Retrieved 29 January 2020.

^ Prasher, V. S.; Onu, Stephen (15 September 2020). ""The Internet of Things (IoT) upheaval: overcoming management challenges"". The Journal of Modern Project Management. 8 (2). doi:10.19255/JMPM02402 (inactive 28 February 2022). ISSN 2317-3963.{{cite journal}}:  CS1 maint: DOI inactive as of February 2022 (link)


Bibliography[edit]



Wikimedia Commons has media related to Internet of things.

Acharjya, D.P.; Geetha, M.K., eds. (2017). Internet of Things: Novel Advances and Envisioned Applications. Springer. p. 311. ISBN 9783319534725.
Li, S.; Xu, L.D., eds. (2017). Securing the Internet of Things. Syngress. p. 154. ISBN 9780128045053.
Rowland, C.; Goodman, E.; Charlier, M.;  et al., eds. (2015). Designing Connected Products: UX for the Consumer Internet of Things. O'Reilly Media. p. 726. ISBN 9781449372569.
Thomas, Jayant; Traukina, Alena (2018). Industrial Internet Application Development: Simplify IIoT development using the elasticity of Public Cloud and Native Cloud Services. Packt Publishing. p. 25. ISBN 978-1788298599.
Stephenson, W. David (2018). The Future Is Smart: how your company can capitalize on the Internet of Things--and win in a connected economy. HarperCollins Leadership. p. 250. ISBN 9780814439777.
vteAmbient intelligenceConcepts
Context awareness
Internet of things
Object hyperlinking
Profiling
Spime
Supranet
Ubiquitous computing
Web of Things
Wireless sensor networks
Technologies
6LoWPAN
ANT+
DASH7
IEEE 802.15.4
Internet 0
Machine to machine
Radio-frequency identification
Smartdust
XBee
Platforms
Arduino
Contiki
Gadgeteer
ioBridge
Netduino
Raspberry Pi
TinyOS
Wiring
Xively
NodeMCU
Applications
Ambient device
CeNSE
Connected car
Home automation
HomeOS
Internet refrigerator
Nabaztag
Smart city
Smart TV
Smarter Planet
Pioneers
Kevin Ashton
Gaetano Borriello
Adam Dunkels
Stefano Marzano
Don Norman
Roel Pieper
Josef Preishuber-Pflügl
John Seely Brown
Bruce Sterling
Mark Weiser
Other
Ambient Devices
AmbieSense
Ebbits project
IPSO Alliance

vteEmbedded systemsGeneral terms
ASIC
Board support package
Bootloader
Consumer electronics
Cross compiler
Embedded database
Embedded hypervisor
Embedded OS
Embedded software
FPGA
IoT
Memory footprint
Microcontroller
Single-board computer
Raspberry Pi
SoC
Firmware and controls
Closed platform
Crippleware
Custom firmware
Defective by Design
Hacking of consumer electronics
Homebrew (video games)
iOS jailbreaking
PlayStation 3 Jailbreak
Proprietary firmware
Rooting (Android)
Vendor lock-in
Boot loaders
U-Boot
Barebox
Software libraries
uClibc
dietlibc
Embedded GLIBC
lwIP
musl
Programming tools
Almquist shell
BitBake
Buildroot
BusyBox
OpenEmbedded
Stand-alone shell
Toybox
Yocto Project
Operating systems
Linux on embedded systems
Linux for mobile devices
Light-weight Linux distribution
Real-time operating system
Windows IoT
Win CE
Programming languages
Ada
Assembly language
CAPL
Embedded C
Embedded C++
Embedded Java
MISRA C
MicroPython

Lightweight browsers
List of open-source computing hardware
Open-source robotics

vteEmerging technologiesFieldsInformation andcommunications
Ambient intelligence
Internet of things
Artificial intelligence
Applications of artificial intelligence
Progress in artificial intelligence
Machine translation
Mobile translation
Machine vision
Semantic Web
Speech recognition
Atomtronics
Carbon nanotube field-effect transistor
Cybermethodology
Fourth-generation optical discs
3D optical data storage
Holographic data storage
GPGPU
Memory
CBRAM
FRAM
Millipede
MRAM
NRAM
PRAM
Racetrack memory
RRAM
SONOS
ECRAM
UltraRAM
Optical computing
RFID
Chipless RFID
Software-defined radio
Three-dimensional integrated circuit
Topics
Collingridge dilemma
Differential technological development
Disruptive innovation
Ephemeralization
Ethics
Bioethics
Cyberethics
Neuroethics
Robot ethics
Exploratory engineering
Fictional technology
Proactionary principle
Technological change
Technological unemployment
Technological convergence
Technological evolution
Technological paradigm
Technology forecasting
Accelerating change
Horizon scanning
Moore's law
Technological singularity
Technology scouting
Technology readiness level
Technology roadmap
Transhumanism

 Category
 List

vteSelf-driving cars and enabling technologiesOverview and context
History of self-driving cars
Impact of self-driving cars
Intelligent transportation system
Context-aware pervasive systems
Mobile computing
Smart, connected products
Ubiquitous computing
Ambient intelligence
Internet of things
SAE LevelsHuman driver monitors the driving environment(Levels 0,1,2)
Lane departure warning system
Automatic parking
Collision avoidance system
Cruise control
Adaptive cruise control
Advanced driver-assistance systems
Driver drowsiness detection
Intelligent speed adaptation
Blind spot monitor
System monitors the driving environment(Levels 3,4,5)
Vehicular ad hoc network (V2V)
Connected car
Automotive navigation system
VehiclesCars
VaMP (1994)
Spirit of Berlin (2007)
General Motors EN-V (2010)
MadeInGermany (2011)
Waymo, formerly Google Car (2012)
Tesla Model S with Autopilot (2015)
LUTZ Pathfinder (2015)
Yandex self-driving car (2017)
Honda Legend (2021)
Buses and commercial vehicles
Automated guideway transit
ParkShuttle
Navia shuttle
NuTonomy taxi
Freightliner Inspiration
Driverless tractor
Mobility as a service
Regulation
Legislation
IEEE 802.11p
 Safe speed automotive common law
Automated lane keeping system (unece regulation 157)
Regulation (EU) 2019/2144
Enabling technologies
Radar
Laser
LIDAR
Artificial neural network
Computer stereo vision
Image recognition
Dedicated short-range communications
Real-time Control System
rFpro
Eye tracking
Radio-frequency identification
Automotive navigation system
Organizations, Projects & PeopleOrganizations, projects and events
American Center for Mobility
DAVI
European Land-Robot Trial
Navlab
DARPA Grand Challenge
VisLab Intercontinental Autonomous Challenge
Eureka Prometheus Project
IEEE Intelligent Transportation Systems Society
People
Harold Goddijn
Alberto Broggi
Anthony Levandowski

Authority control: National libraries 
France (data)
Germany
Israel
United States





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Internet_of_things&oldid=1085510691""
Categories: Internet of thingsAmbient intelligenceEmerging technologiesTechnology assessmentsComputing and societyDigital technology21st-century inventionsHidden categories: CS1 maint: othersCS1 errors: missing periodicalCS1 errors: external linksCS1 errors: generic nameCS1 maint: DOI inactive as of February 2022Articles with short descriptionShort description is different from WikidataUse dmy dates from October 2019Use American English from December 2020All Wikipedia articles written in American EnglishArticles containing potentially dated statements from 2018All articles containing potentially dated statementsAll articles with unsourced statementsArticles with unsourced statements from November 2019Articles with unsourced statements from April 2019Articles with unsourced statements from June 2020Articles containing potentially dated statements from June 2018Articles needing expert attention from July 2018All articles needing expert attentionTechnology articles needing expert attentionArticles with unsourced statements from May 2017Citation overkillArticles tagged with the inline citation overkill template from April 2019Articles containing potentially dated statements from March 2020Wikipedia articles needing page number citations from August 2018Commons category link is on WikidataArticles with BNF identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiers

"
3,Cloud_computing,"




Cloud computing

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Form of shared Internet-based computing
Not to be confused with Cloud Computing (horse).
This article may be confusing or unclear to readers. In particular, it is a poorly-written and -sourced article full of inaccuracies about a high-profile topic. Please help clarify the article. There is a discussion about this on Talk:Cloud computing § Ungrammatical and Uninterpretable Language, Incoherent Scope/Purpose, Full of Misinformation. (March 2021) (Learn how and when to remove this template message)


 Cloud computing metaphor: the group of networked elements providing services need not be individually addressed or managed by users; instead, the entire provider-managed suite of hardware and software can be thought of as an amorphous cloud.
Cloud computing[1] is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user.[2] Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and typically using a ""pay-as-you-go"" model which can help in reducing capital expenses but may also lead to unexpected operating expenses for unaware users.[3]

Contents

1 Value proposition
2 History

2.1 Early history
2.2 2000s
2.3 2010s


3 Similar concepts
4 Characteristics
5 Service models

5.1 Infrastructure as a service (IaaS)
5.2 Platform as a service (PaaS)
5.3 Software as a service (SaaS)
5.4 Mobile ""backend"" as a service (MBaaS)
5.5 Serverless computing or Function-as-a-Service (FaaS)


6 Deployment models

6.1 Private cloud
6.2 Public cloud
6.3 Hybrid cloud
6.4 Others

6.4.1 Community cloud
6.4.2 Distributed cloud
6.4.3 Multicloud
6.4.4 Poly cloud
6.4.5 Big data cloud
6.4.6 HPC cloud




7 Architecture

7.1 Cloud engineering


8 Security and privacy
9 Limitations and disadvantages
10 Emerging trends
11 Digital forensics in the cloud
12 See also
13 References
14 Further reading
15 External links


Value proposition[edit]
Advocates of public and hybrid clouds claim that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand,[4][5][6] providing burst computing capability: high computing power at certain periods of peak demand.[7]

History[edit]
This section may be confusing or unclear to readers. Please help clarify the section. There might be a discussion about this on the talk page. (January 2021) (Learn how and when to remove this template message)
The term cloud was used to refer to platforms for distributed computing as early as 1993, when Apple spin-off General Magic and AT&T used it in describing their (paired) Telescript and PersonaLink technologies.[8]  In Wired's April 1994 feature ""Bill and Andy's Excellent Adventure II"", Andy Hertzfeld commented on Telescript, General Magic's distributed programming language:

""The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.""[9]
Early history[edit]
During the 1960s, the initial concepts of time-sharing became popularized via RJE (Remote Job Entry);[10] this terminology was mostly associated with large vendors such as IBM and DEC.  Full-time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the ""data center"" model where users submitted jobs to operators to run on IBM's mainframes was overwhelmingly predominant.
In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.[citation needed]  They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the network infrastructure.[11] As computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.[citation needed] They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.[12]
The use of the cloud metaphor for virtualized services dates at least to General Magic in 1994, where it was used to describe the universe of ""places"" that mobile agents in the Telescript environment could go. As described by 
Andy Hertzfeld:

""The beauty of Telescript,"" says Andy, ""is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service.""[13]
The use of the cloud metaphor is credited to General Magic communications employee David Hoffman, based on long-standing use in networking and telecom. In addition to use by General Magic itself, it was also used in promoting AT&T's associated PersonaLink Services.[14]

2000s[edit]
In July 2002, Amazon created subsidiary Amazon Web Services, with the goal to ""enable developers to build innovative and entrepreneurial applications on their own."" In March 2006 Amazon introduced its Simple Storage Service (S3), followed by Elastic Compute Cloud (EC2) in August of the same year.[15][16] These products pioneered the usage of server virtualization to deliver IaaS at a cheaper and on-demand pricing basis.
In April 2008, Google released the beta version of Google App Engine.[17] The App Engine was a PaaS (one of the first of its kind) which provided fully maintained infrastructure and a deployment platform for users to create web applications using common languages/technologies such as Python, Node.js and PHP. The goal was to eliminate the need for some administrative tasks typical of an IaaS model, while creating a platform where users could easily deploy such applications and scale them to demand.[18]
In early 2008, NASA's Nebula,[19] enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.[20]
By mid-2008, Gartner saw an opportunity for cloud computing ""to shape the relationship among consumers of IT services, those who use IT services and those who sell them""[21] and observed that ""organizations are switching from company-owned hardware and software assets to per-use service-based models"" so that the ""projected shift to computing ... will result in dramatic growth in IT products in some areas and significant reductions in other areas.""[22]
In 2008, the U.S. National Science Foundation began the Cluster Exploratory program to fund academic research using Google-IBM cluster technology to analyze massive amounts of data.[23]
In 2009, the government of France announced Project Andromède to create a ""sovereign cloud"" or national cloud computing, with the government to spend €285 million.[24][25]  The initiative failed badly and Cloudwatt was shut down on 1 February 2020.[26][27]

2010s[edit]
In February 2010, Microsoft released Microsoft Azure, which was announced in October 2008.[28]
In July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. The OpenStack project intended to help organizations offering cloud-computing services running on standard hardware. The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform. As an open-source offering and along with other open-source solutions such as CloudStack, Ganeti, and OpenNebula, it has attracted attention by several key communities. Several studies aim at comparing these open source offerings based on a set of criteria.[29][30][31][32][33][34][35]
On March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.[36] Among the various components of the Smarter Computing foundation, cloud computing is a critical part. On June 7, 2012, Oracle announced the Oracle Cloud.[37] This cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.[38][39][40]
In May 2012, Google Compute Engine was released in preview, before being rolled out into General Availability in December 2013.[41]
In 2019, Linux was the most common OS used on Microsoft Azure.[42] In December 2019, Amazon announced AWS Outposts, which is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any customer datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience[43]

Similar concepts[edit]
The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.[44] The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more ""virtual"" devices, each of which can be easily used and managed to perform computing tasks. With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[44]
Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.[44]
Cloud computing shares characteristics with:

Client–server model—Client–server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).[45]
Computer bureau—A service bureau providing computer services, particularly from the 1960s to 1980s.
Grid computing—A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks.
Fog computing—Distributed computing paradigm that provides data, compute, storage and application services closer to the client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client-side (e.g. mobile devices), instead of sending data to a remote location for processing.
Utility computing—The ""packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity.""[46][47]
Peer-to-peer—A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client-server model).
Cloud sandbox—A live, isolated computer environment in which a program, code or file can run without affecting the application in which it runs.
Characteristics[edit]
Cloud computing exhibits the following key characteristics:

Agility for organizations may be improved, as cloud computing may increase users' flexibility with re-provisioning, adding, or expanding technological infrastructure resources.
Cost reductions are claimed by cloud providers. A public-cloud delivery model converts capital expenditures (e.g., buying servers) to operational expenditure.[48] This purportedly lowers barriers to entry, as infrastructure is typically provided by a third party and need not be purchased for one-time or infrequent intensive computing tasks. Pricing on a utility computing basis is ""fine-grained"", with usage-based billing options. As well, less in-house IT skills are required for implementation of projects that use cloud computing.[49] The e-FISCAL project's state-of-the-art repository[50] contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available in-house.
Device and location independence[51] enable users to access systems using a web browser regardless of their location or what device they use (e.g., PC, mobile phone). As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect to it from anywhere.[49]
Maintenance of cloud environment is easier because the data is hosted on an outside server maintained by a provider without the need to invest in data center hardware. IT maintenance of cloud computing is managed and updated by the cloud provider's IT maintenance team that reduces cloud computing costs compared with the on-premises data centers.
Multitenancy enables sharing of resources and costs across a large pool of users thus allowing for:
centralization of infrastructure in locations with lower costs (such as real estate, electricity, etc.)
peak-load capacity increases (users need not engineer and pay for the resources and equipment to meet their highest possible load-levels)
utilisation and efficiency improvements for systems that are often only 10–20% utilised.[52][53]
Performance is monitored by IT experts from the service provider, and consistent and loosely coupled architectures are constructed using web services as the system interface.[49][54]
Productivity may be increased when multiple users can work on the same data simultaneously, rather than waiting for it to be saved and emailed. Time may be saved as information does not need to be re-entered when fields are matched, nor do users need to install application software upgrades to their computer.[55]
Availability improves with the use of multiple redundant sites, which makes well-designed cloud computing suitable for business continuity and disaster recovery.[56]
Scalability and elasticity via dynamic (""on-demand"") provisioning of resources on a fine-grained, self-service basis in near real-time[57][58] (Note, the VM startup time varies by VM type, location, OS and cloud providers[57]), without users having to engineer for peak loads.[59][60][61] This gives the ability to scale up when the usage need increases or down if resources are not being used.[62] The time-efficient benefit of cloud scalability also means faster time to market, more business flexibility, and adaptability, as adding new resources doesn’t take as much time as it used to.[63] Emerging approaches for managing elasticity include the use of machine learning techniques to propose efficient elasticity models.[64]
Security can improve due to centralization of data, increased security-focused resources, etc., but concerns can persist about loss of control over certain sensitive data, and the lack of security for stored kernels. Security is often as good as or better than other traditional systems, in part because service providers are able to devote resources to solving security issues that many customers cannot afford to tackle or which they lack the technical skills to address.[65] However, the complexity of security is greatly increased when data is distributed over a wider area or over a greater number of devices, as well as in multi-tenant systems shared by unrelated users. In addition, user access to security audit logs may be difficult or impossible. Private cloud installations are in part motivated by users' desire to retain control over the infrastructure and avoid losing control of information security.
The National Institute of Standards and Technology's definition of cloud computing identifies ""five essential characteristics"":

On-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.
Broad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).
Resource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. 
Rapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.

Measured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.— National Institute of Standards and Technology[66]
Service models[edit]
 Cloud computing service models arranged as layers in a stack
Though service-oriented architecture advocates ""Everything as a service"" (with the acronyms EaaS or XaaS,[67] or simply aas), cloud-computing providers offer their ""services"" according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).[66] These models offer increasing abstraction; they are thus often portrayed as layers in a stack: infrastructure-, platform- and software-as-a-service, but these need not be related. For example, one can provide SaaS implemented on physical machines (bare metal), without using underlying PaaS or IaaS layers, and conversely one can run a program on IaaS and access it directly, without wrapping it as SaaS.

Infrastructure as a service (IaaS)[edit]
Main article: Infrastructure as a service
""Infrastructure as a service"" (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization because there is no hypervisor overhead.  IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[68]
The NIST's definition of cloud computing describes IaaS as ""where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).""[66]
IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.[citation needed]

Platform as a service (PaaS)[edit]
Main article: Platform as a service
The NIST's definition of cloud computing defines Platform as a Service as:[66]

The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment.
PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.[69][need quotation to verify]
Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows.[70] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[71] dPaaS delivers integration—and data-management—products as a fully managed service.[72] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.[73]

Software as a service (SaaS)[edit]
Main article: Software as a service
The NIST's definition of cloud computing defines Software as a Service as:[66]

The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.
In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as ""on-demand software"" and is usually priced on a pay-per-use basis or using a subscription fee.[74] In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[75] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.
The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[76] so prices become scalable and adjustable if users are added or removed at any point. It may also be free.[77] Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data.[78] Examples of applications offered as SaaS are games and productivity software like Google Docs and Word Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive and Word Online being integrated with Onedrive.[citation needed]

Mobile ""backend"" as a service (MBaaS)[edit]
Main article: Mobile backend as a service
In the mobile ""backend"" as a service (m) model, also known as backend as a service (BaaS), web app and mobile app developers are provided with a way to link their applications to cloud storage and cloud computing services with application programming interfaces (APIs) exposed to their applications and custom software development kits (SDKs). Services include user management, push notifications, integration with social networking services[79] and more. This is a relatively recent model in cloud computing,[80] with most BaaS startups dating from 2011 or later[81][82][83] but trends indicate that these services are gaining significant mainstream traction with enterprise consumers.[84]

Serverless computing or Function-as-a-Service (FaaS)[edit]
Main article: Serverless computing
Serverless computing is a cloud computing code execution model in which the cloud provider fully manages starting and stopping virtual machines as necessary to serve requests, and requests are billed by an abstract measure of the resources required to satisfy the request, rather than per virtual machine, per hour.[85] Despite the name, it does not actually involve running code without servers.[85] Serverless computing is so named because the business or person that owns the system does not have to purchase, rent or provide servers or virtual machines for the back-end code to run on.
Function as a service (FaaS) is a service-hosted remote procedure call that leverages serverless computing to enable the deployment of individual functions in the cloud that run in response to events.[86] FaaS is considered by some to come under the umbrella of serverless computing, while some others use the terms interchangeably.[87]

Deployment models[edit]
 Cloud computing types
Private cloud[edit]
Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally.[66] Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[88] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users ""still have to buy, build, and manage them"" and thus do not benefit from less hands-on management,[89] essentially ""[lacking] the economic model that makes cloud computing such an intriguing concept"".[90][91]

Public cloud[edit]
For a comparison of cloud-computing software and providers, see Cloud-computing comparison
Cloud services are considered ""public"" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge.[92] Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.[49][93]
Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.[94]

Hybrid cloud[edit]
Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources,[95][96] that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[66] Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[97] A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[98] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[99]
Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[100] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[66] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and ""bursts"" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed.[101] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[102] The specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called ""Cross-platform Hybrid Cloud"". A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.[103] This kind of cloud emerges from the rise of ARM-based system-on-chip for server-class computing.
Hybrid cloud infrastructure essentially serves to eliminate limitations inherent to the multi-access relay characteristics of private cloud networking. The advantages include enhanced runtime flexibility and adaptive memory processing unique to virtualized interface models.[104]

Others[edit]
Community cloud[edit]
Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.[66]

Distributed cloud[edit]
A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.

Public-resource computing—This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. Nonetheless, it is considered a sub-class of cloud computing.
Volunteer cloud—Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. Many challenges arise from this type of infrastructure, because of the volatility of the resources used to build it and the dynamic environment it operates in. It can also be called peer-to-peer clouds, or ad-hoc clouds. An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.[105]
Multicloud[edit]
Main article: Multicloud
Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).[106][107][108]

Poly cloud[edit]
Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more that could be done with a single provider.[109]

Big data cloud[edit]
The issues of transferring large amounts of data to the cloud as well as data security once the data is in the cloud initially hampered adoption of cloud for big data, but now that much data originates in the cloud and with the advent of bare-metal servers, the cloud has become[110] a solution for use cases including business analytics and geospatial analysis.[111]

HPC cloud[edit]
HPC cloud refers to the use of cloud computing services and infrastructure to execute high-performance computing (HPC) applications.[112] These applications consume considerable amount of computing power and memory and are traditionally executed on clusters of computers. In 2016 a handful of companies, including R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Sabalcore, Gomput, and Penguin Computing offered a high performance computing cloud. The Penguin On Demand (POD) cloud was one of the first non-virtualized remote HPC services offered on a pay-as-you-go basis.[113][114] Penguin Computing launched its HPC cloud in 2016 as alternative to Amazon's EC2 Elastic Compute Cloud, which uses virtualized computing nodes.[115][116]

Architecture[edit]
 Cloud computing sample architecture
Cloud architecture,[117] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.

Cloud engineering[edit]
Cloud engineering is the application of engineering disciplines of cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information technology engineering, security, platform, risk, and quality engineering.

Security and privacy[edit]
 Cloud suppliers security and privacy agreements must be alligned to the demand(s) requirements
Main article: Cloud computing security
Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information.[118] Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored.[118] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[119][118] Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity.[120] The systems work by creating and describing identities, recording activities, and getting rid of unused identities.
According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. ""There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into"". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called ""hyperjacking"". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[121] Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[121]
There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[122] Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[123] Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click ""Accept"" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[124]

Limitations and disadvantages[edit]
According to Bruce Schneier, ""The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and—like any outsourced task—you tend to get what you want. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug."" He also suggests that ""the cloud provider might not meet your legal needs"" and that businesses need to weigh the benefits of cloud computing against the risks.[125]
In cloud computing, the control of the back end infrastructure is limited to the cloud vendor only. Cloud providers often decide on the management policies, which moderates what the cloud users are able to do with their deployment.[126] Cloud users are also limited to the control and management of their applications, data and services.[127] This includes data caps, which are placed on cloud users by the cloud vendor allocating a certain amount of bandwidth for each customer and are often shared among other cloud users.[127]
Privacy and confidentiality are big concerns in some activities. For instance, sworn translators working under the stipulations of an NDA, might face problems regarding sensitive data that are not encrypted.[128] Due to the use of the internet, confidential information such as employee data and user data can be easily available to third-party organisations and people in Cloud Computing.[129]
Cloud computing has some limitations for smaller business operations, particularly regarding security and downtime. Technical outages are inevitable and occur sometimes when cloud service providers (CSPs) become overwhelmed in the process of serving their clients. This may result in temporary business suspension. Since this technology's systems rely on the Internet, an individual cannot access their applications, server, or data from the cloud during an outage.[130]

Emerging trends[edit]
This section needs expansion. You can help by adding to it.  (September 2021)
Cloud computing is still a subject of research.[131] A driving factor in the evolution of cloud computing has been chief technology officers seeking to minimize risk of internal outages and mitigate the complexity of housing network and computing hardware in-house.[132] They are also looking to share information to workers located in diverse areas in near and real-time, to enable teams to work seamlessly, no matter where they are located. Since the global pandemic of 2020, cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees, notably remote workers. For example, Zoom grew over 160% in 2020 alone.[133]

Digital forensics in the cloud[edit]
The issue of carrying out investigations where the cloud storage devices cannot be physically accessed has generated a number of changes to the way that digital evidence is located and collected.[134] New process models have been developed to formalize collection.[135]
In some scenarios existing digital forensics tools can be employed to access cloud storage as networked drives (although this is a slow process generating a large amount of internet traffic).[citation needed]
An alternative approach is to deploy a tool that processes in the cloud itself.[136]
For organizations using Office 365 with an 'E5' subscription, there is the option to use Microsoft's built-in e-discovery resources, although these do not provide all the functionality that is typically required for a forensic process.[137]

See also[edit]

Block-level storage
Category:Cloud computing providers
Category:Cloud platforms
Communication protocol
Communications system
Cloud collaboration
Cloud native computing
Cloud computing security
Cloud computing comparison
Cloud management
Cloud research
Cloud robotics
Cloud gaming
Cloud storage
Cloudlet
Computer cluster
Cooperative storage cloud
Dew computing
Data cluster
Directory
Distributed data store
Distributed database
Distributed computing
Distributed networking
Decentralized computing
Edge computing
Edge device
eScience
File system
Clustered file system
Distributed file system
Distributed file system for cloud
Fog computing
Fog robotics
Green computing (environmentally sustainable computing)
Grid computing
In-memory database
In-memory processing
Internet of things
Microservices
Mobile cloud computing
Mobile edge computing
Peer-to-peer
Personal cloud
Robot as a service
As a service
Service-oriented architecture
Time-sharing
Ubiquitous computing
VDI
Virtual private cloud
Web computing

References[edit]


^ Ray, Partha Pratim (2018). ""An Introduction to Dew Computing: Definition, Concept and Implications - IEEE Journals & Magazine"". IEEE Access. 6: 723–737. doi:10.1109/ACCESS.2017.2775042. S2CID 3324933.

^ Montazerolghaem, Ahmadreza; Yaghmaee, Mohammad Hossein; Leon-Garcia, Alberto (September 2020). ""Green Cloud Multimedia Networking: NFV/SDN Based Energy-Efficient Resource Allocation"". IEEE Transactions on Green Communications and Networking. 4 (3): 873–889. doi:10.1109/TGCN.2020.2982821. ISSN 2473-2400. S2CID 216188024.

^ ""Where's The Rub: Cloud Computing's Hidden Costs"". Forbes. 2014-02-27. Retrieved 2014-07-14.

^ ""What is Cloud Computing?"". Amazon Web Services. 2013-03-19. Retrieved 2013-03-20.

^ Baburajan, Rajani (2011-08-24). ""The Rising Cloud Storage Market Opportunity Strengthens Vendors"". It.tmcnet.com. Retrieved 2011-12-02.

^ Oestreich, Ken (2010-11-15). ""Converged Infrastructure"". CTO Forum. Thectoforum.com. Archived from the original on 2012-01-13. Retrieved 2011-12-02.

^ Ted Simpson, Jason Novak, Hands on Virtual Computing,  2017, ISBN 1337515744, p. 451

^ AT&T (1993). ""What Is The Cloud?"". YouTube. Archived from the original on 2021-10-27. Retrieved 2017-10-26. You can think of our electronic meeting place as the Cloud. PersonaLink was built from the ground up to give handheld communicators and other devices easy access to a variety of services.  [...]  Telescript is the revolutionary software technology that makes intelligent assistance possible.  Invented by General Magic, AT&T is the first company to harness Telescript, and bring its benefits to people everywhere.  [...]  Very shortly, anyone with a computer, a personal communicator, or television will be able to use intelligent assistance in the Cloud.  And our new meeting place is open, so that anyone, whether individual, entrepreneur, or a multinational company, will be able to offer information, goods, and services.

^ Steven Levy (April 1994). ""Bill and Andy's Excellent Adventure II"". Wired.

^ White, J.E. ""Network Specifications for Remote Job Entry and Remote Job Output Retrieval at UCSB"". tools.ietf.org. Retrieved 2016-03-21.

^ ""July, 1993 meeting report from the IP over ATM working group of the IETF"". CH: Switch. Archived from the original on 2012-07-10. Retrieved 2010-08-22.

^ Corbató, Fernando J. ""An Experimental Time-Sharing System"". SJCC Proceedings. MIT. Archived from the original on 6 September 2009. Retrieved 3 July 2012.

^ Levy, Steven (April 1994). ""Bill and Andy's Excellent Adventure II"". Wired.

^ Levy, Steven (2014-05-23). ""Tech Time Warp of the Week: Watch AT&T Invent Cloud Computing in 1994"". Wired. AT&T and the film's director, David Hoffman, pulled out the cloud metaphor–something that had long been used among networking and telecom types. [...]  ""You can think of our electronic meeting place as the cloud,"" says the film's narrator, [...] David Hoffman, the man who directed the film and shaped all that cloud imagery, was a General Magic employee.

^ ""Announcing Amazon Elastic Compute Cloud (Amazon EC2) – beta"". 24 August 2006. Retrieved 31 May 2014.

^ Qian, Ling; Lou, Zhigou; Du, Yujian; Gou, Leitao. ""Cloud Computing: An Overview"". researchgate.net. Retrieved 19 April 2021.

^ ""Introducing Google App Engine + our new blog"". Google Developer Blog. 2008-04-07. Retrieved 2017-03-07.

^ ""App Engine"". cloud.google.com. Retrieved 19 April 2021.

^ ""Nebula Cloud Computing Platform: NASA"". Open Government at NASA. 2012-11-20. Retrieved 2020-11-15.

^ Rochwerger, B.; Breitgand, D.; Levy, E.; Galis, A.; Nagin, K.; Llorente, I. M.; Montero, R.; Wolfsthal, Y.; Elmroth, E.; Caceres, J.; Ben-Yehuda, M.; Emmerich, W.; Galan, F. (2009). ""The Reservoir model and architecture for open federated cloud computing"". IBM Journal of Research and Development. 53 (4): 4:1–4:11. doi:10.1147/JRD.2009.5429058.

^ Keep an eye on cloud computing, Amy Schurr, Network World, 2008-07-08, citing the Gartner report, ""Cloud Computing Confusion Leads to Opportunity"". Retrieved 2009-09-11.

^ Gartner (2008-08-18). ""Gartner Says Worldwide IT Spending on Pace to Surpass Trillion in 2008"". Archived from the original on December 4, 2008.

^ ""Cluster Exploratory (CluE) nsf08560"". www.nsf.gov.

^ ""285 millions d'euros pour Andromède, le cloud souverain français - le Monde Informatique"". Archived from the original on 2011-10-23.

^ Hicks, Jacqueline. ""'Digital colonialism': why some countries want to take control of their people's data from Big Tech"". The Conversation.

^ ""Orange enterre Cloudwatt, qui fermera ses portes le 31 janvier 2020"". www.nextinpact.com. August 30, 2019.

^ ""Cloudwatt : Vie et mort du premier "" cloud souverain "" de la France"". 29 August 2019.

^ ""Windows Azure General Availability"". The Official Microsoft Blog. Microsoft. 2010-02-01. Archived from the original on 2014-05-11. Retrieved 2015-05-03.

^ Milita Datta (August 9, 2016). ""Apache CloudStack vs. OpenStack: Which Is the Best?"". DZone · Cloud Zone.

^ ""OpenNebula vs OpenStack"". SoftwareInsider.[dead link]

^ Kostantos, Konstantinos, et al. ""OPEN-source IaaS fit for purpose: a comparison between OpenNebula and OpenStack."" International Journal of Electronic Business Management 11.3 (2013)

^ L. Albertson, ""OpenStack vs. Ganeti"", LinuxFest Northwest 2017

^ Qevani, Elton, et al. ""What can OpenStack adopt from a Ganeti-based open-source IaaS?."" Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on. IEEE, 2014

^ Von Laszewski, Gregor, et al. ""Comparison of multiple cloud frameworks."", IEEE 5th International Conference on Cloud Computing (CLOUD), 2012.

^ Diaz, Javier et al. "" Abstract Image Management and Universal Image Registration for Cloud and HPC Infrastructures "", IEEE 5th International Conference on Cloud Computing (CLOUD), 2012

^ ""Launch of IBM Smarter Computing"". Archived from the original on 20 April 2013. Retrieved 1 March 2011.

^ ""Launch of Oracle Cloud"". The Register. Retrieved 28 February 2014.

^ ""Oracle Cloud, Enterprise-Grade Cloud Solutions: SaaS, PaaS, and IaaS"". Retrieved 12 October 2014.

^ ""Larry Ellison Doesn't Get the Cloud: The Dumbest Idea of 2013"". Forbes.com. Retrieved 12 October 2014.

^ ""Oracle Disrupts Cloud Industry with End-to-End Approach"". Forbes.com. Retrieved 12 October 2014.

^ ""Google Compute Engine is now Generally Available with expanded OS support, transparent maintenance, and lower prices"". Google Developers Blog. 2013-12-02. Retrieved 2017-03-07.

^ Vaughan-Nichols, Steven J. ""Microsoft developer reveals Linux is now more used on Azure than Windows Server"". ZDNet. Retrieved 2019-07-02.

^ ""Announcing General Availability of AWS Outposts"". Amazon Web Services, Inc.

^ a b c HAMDAQA, Mohammad (2012). Cloud Computing Uncovered: A Research Landscape (PDF). Elsevier Press. pp. 41–85. ISBN 978-0-12-396535-6.

^ ""Distributed Application Architecture"" (PDF). Sun Microsystem. Retrieved 2009-06-16.

^ Vaquero, Luis M.; Rodero-Merino, Luis; Caceres, Juan; Lindner, Maik (December 2008). ""It's probable that you've misunderstood 'Cloud Computing' until now"". Sigcomm Comput. Commun. Rev. TechPluto. 39 (1): 50–55. doi:10.1145/1496091.1496100. S2CID 207171174.

^ Danielson, Krissi (2008-03-26). ""Distinguishing Cloud Computing from Utility Computing"". Ebizq.net. Retrieved 2010-08-22.

^ ""Recession Is Good For Cloud Computing – Microsoft Agrees"". CloudAve. 2009-02-12. Retrieved 2010-08-22.

^ a b c d ""Defining 'Cloud Services' and ""Cloud Computing"""". IDC. 2008-09-23. Archived from the original on 2010-07-22. Retrieved 2010-08-22.

^ ""State of the Art | e-FISCAL project"". www.efiscal.eu.

^ Farber, Dan (2008-06-25). ""The new geek chic: Data centers"". CNET News. Retrieved 2010-08-22.

^ ""Jeff Bezos' Risky Bet"". Business Week.

^ He, Sijin; Guo, L.; Guo, Y.; Ghanem, M. (June 2012). Improving Resource Utilisation in the Cloud Environment Using Multivariate Probabilistic Models. 2012 2012 IEEE 5th International Conference on Cloud Computing (CLOUD). pp. 574–581. doi:10.1109/CLOUD.2012.66. ISBN 978-1-4673-2892-0. S2CID 15374752.

^ He, Qiang, et al. ""Formulating Cost-Effective Monitoring Strategies for Service-based Systems."" (2013): 1–1.

^ Heather Smith (23 May 2013). Xero For Dummies. John Wiley & Sons. pp. 37–. ISBN 978-1-118-57252-8.

^ King, Rachael (2008-08-04). ""Cloud Computing: Small Companies Take Flight"". Bloomberg BusinessWeek. Retrieved 2010-08-22.

^ a b Mao, Ming; M. Humphrey (2012). A Performance Study on the VM Startup Time in the Cloud. Proceedings of 2012 IEEE 5th International Conference on Cloud Computing (Cloud2012). p. 423. doi:10.1109/CLOUD.2012.103. ISBN 978-1-4673-2892-0. S2CID 1285357.

^ Bruneo, Dario; Distefano, Salvatore; Longo, Francesco; Puliafito, Antonio; Scarpa, Marco (2013). ""Workload-Based Software Rejuvenation in Cloud Systems"". IEEE Transactions on Computers. 62 (6): 1072–1085. doi:10.1109/TC.2013.30. S2CID 23981532.

^ Kuperberg, Michael; Herbst, Nikolas; Kistowski, Joakim Von; Reussner, Ralf (2011). ""Defining and Measuring Cloud Elasticity"". KIT Software Quality Departement. doi:10.5445/IR/1000023476. Retrieved 13 August 2011. {{cite journal}}: Cite journal requires |journal= (help)

^ ""Economies of Cloud Scale Infrastructure"". Cloud Slam 2011. Archived from the original on 2021-10-27. Retrieved 13 May 2011.

^ He, Sijin; L. Guo; Y. Guo; C. Wu; M. Ghanem; R. Han (March 2012). Elastic Application Container: A Lightweight Approach for Cloud Resource Provisioning. 2012 IEEE 26th International Conference on Advanced Information Networking and Applications (AINA). pp. 15–22. doi:10.1109/AINA.2012.74. ISBN 978-1-4673-0714-7. S2CID 4863927.

^ Marston, Sean; Li, Zhi; Bandyopadhyay, Subhajyoti; Zhang, Juheng; Ghalsasi, Anand (2011-04-01). ""Cloud computing – The business perspective"". Decision Support Systems. 51 (1): 176–189. doi:10.1016/j.dss.2010.12.006.

^ Why Cloud computing scalability matters for business growth, Symphony Solutions, 2021

^ Nouri, Seyed; Han, Li; Srikumar, Venugopal; Wenxia, Guo; MingYun, He; Wenhong, Tian (2019). ""Autonomic decentralized elasticity based on a reinforcement learning controller for cloud applications"". Future Generation Computer Systems. 94: 765–780. doi:10.1016/j.future.2018.11.049. S2CID 59284268.

^ Mills, Elinor (2009-01-27). ""Cloud computing security forecast: Clear skies"". CNET News. Retrieved 2019-09-19.

^ a b c d e f g h i Peter Mell; Timothy Grance (September 2011). The NIST Definition of Cloud Computing (Technical report). National Institute of Standards and Technology: U.S. Department of Commerce. doi:10.6028/NIST.SP.800-145. Special publication 800-145.

^ Duan, Yucong; Fu, Guohua; Zhou, Nianjun; Sun, Xiaobing; Narendra, Nanjangud; Hu, Bo (2015). ""Everything as a Service (XaaS) on the Cloud: Origins, Current and Future Trends"". 2015 IEEE 8th International Conference on Cloud Computing. IEEE. pp. 621–628. doi:10.1109/CLOUD.2015.88. ISBN 978-1-4673-7287-9. S2CID 8201466.

^ 
Amies, Alex; Sluiman, Harm; Tong, Qiang Guo; Liu, Guo Ning (July 2012). ""Infrastructure as a Service Cloud Concepts"". Developing and Hosting Applications on the Cloud. IBM Press. ISBN 978-0-13-306684-5.

^ Boniface, M.;  et al. (2010). Platform-as-a-Service Architecture for Real-Time Quality of Service Management in Clouds. 5th International Conference on Internet and Web Applications and Services (ICIW). Barcelona, Spain: IEEE. pp. 155–160. doi:10.1109/ICIW.2010.91.

^ ""Integration Platform as a Service (iPaaS)"". Gartner IT Glossary. Gartner.

^ Gartner; Massimo Pezzini; Paolo Malinverno; Eric Thoo. ""Gartner Reference Model for Integration PaaS"". Retrieved 16 January 2013.

^ Loraine Lawson (3 April 2015). ""IT Business Edge"". Retrieved 6 July 2015.

^ Enterprise CIO Forum; Gabriel Lowy. ""The Value of Data Platform-as-a-Service (dPaaS)"". Archived from the original on 19 April 2015. Retrieved 6 July 2015.

^ ""Definition of: SaaS"". PC Magazine Encyclopedia. Ziff Davis. Retrieved 14 May 2014.

^ Hamdaqa, Mohammad. A Reference Model for Developing Cloud Applications (PDF).

^ 
Chou, Timothy. Introduction to Cloud Computing: Business & Technology.

^ ""HVD: the cloud's silver lining"" (PDF). Intrinsic Technology. Archived from the original (PDF) on 2 October 2012. Retrieved 30 August 2012.

^ Sun, Yunchuan; Zhang, Junsheng; Xiong, Yongping; Zhu, Guangyu (2014-07-01). ""Data Security and Privacy in Cloud Computing"". International Journal of Distributed Sensor Networks. 10 (7): 190903. doi:10.1155/2014/190903. ISSN 1550-1477. S2CID 13213544.

^ Carney, Michael (2013-06-24). ""AnyPresence partners with Heroku to beef up its enterprise mBaaS offering"". PandoDaily. Retrieved 24 June 2013.

^ Alex Williams (11 October 2012). ""Kii Cloud Opens Doors For Mobile Developer Platform With 25 Million End Users"". TechCrunch. Retrieved 16 October 2012.

^ Aaron Tan (30 September 2012). ""FatFractal ups the ante in backend-as-a-service market"". Techgoondu.com. Retrieved 16 October 2012.

^ Dan Rowinski (9 November 2011). ""Mobile Backend As A Service Parse Raises $5.5 Million in Series A Funding"". ReadWrite. Retrieved 23 October 2012.

^ Pankaj Mishra (7 January 2014). ""MobStac Raises $2 Million in Series B To Help Brands Leverage Mobile Commerce"". TechCrunch. Retrieved 22 May 2014.

^ ""built.io Is Building an Enterprise MBaas Platform for IoT"". programmableweb. 2014-03-03. Retrieved 3 March 2014.

^ a b Miller, Ron (24 Nov 2015). ""AWS Lambda Makes Serverless Applications A Reality"". TechCrunch. Retrieved 10 July 2016.

^ ""bliki: Serverless"". martinfowler.com. Retrieved 2018-05-04.

^ Sbarski, Peter (2017-05-04). Serverless Architectures on AWS: With examples using AWS Lambda (1st ed.). Manning Publications. ISBN 9781617293825.

^ ""Self-Run Private Cloud Computing Solution – GovConnection"". govconnection.com. 2014. Retrieved April 15, 2014.

^ ""Private Clouds Take Shape – Services – Business services – Informationweek"". 2012-09-09. Archived from the original on 2012-09-09.

^ Haff, Gordon (2009-01-27). ""Just don't call them private clouds"". CNET News. Retrieved 2010-08-22.

^ ""There's No Such Thing As A Private Cloud – Cloud-computing -"". 2013-01-26. Archived from the original on 2013-01-26.

^ Rouse, Margaret. ""What is public cloud?"". Definition from Whatis.com. Retrieved 12 October 2014.

^ ""FastConnect | Oracle Cloud Infrastructure"". cloud.oracle.com. Retrieved 2017-11-15.

^ Schmidt, Rainer; Möhring, Michael; Keller, Barbara (2017). ""Customer Relationship Management in a Public Cloud environment - Key influencing factors for European enterprises"". HICSS. Proceedings of the 50th Hawaii International Conference on System Sciences (2017). doi:10.24251/HICSS.2017.513. hdl:10125/41673. ISBN 9780998133102.

^ ""What is hybrid cloud? - Definition from WhatIs.com"". SearchCloudComputing. Retrieved 2019-08-10.

^ Butler, Brandon (2017-10-17). ""What is hybrid cloud computing? The benefits of mixing private and public cloud services"". Network World. Retrieved 2019-08-11.

^ ""Mind the Gap: Here Comes Hybrid Cloud – Thomas Bittman"". Thomas Bittman. 24 September 2012. Retrieved 22 April 2015.

^ ""Business Intelligence Takes to Cloud for Small Businesses"". CIO.com. 2014-06-04. Retrieved 2014-06-04.

^ Désiré Athow (24 August 2014). ""Hybrid cloud: is it right for your business?"". TechRadar. Retrieved 22 April 2015.

^ Metzler, Jim; Taylor, Steve. (2010-08-23) ""Cloud computing: Reality vs. fiction"", Network World.

^ Rouse, Margaret. ""Definition: Cloudbursting"", May 2011. SearchCloudComputing.com.

^ ""How Cloudbursting ""Rightsizes"" the Data Center"". 2012-06-22.

^ Kaewkasi, Chanwit (3 May 2015). ""Cross-Platform Hybrid Cloud with Docker"".

^ Qiang, Li (2009). ""Adaptive management of virtualized resources in cloud computing using feedback control"". First International Conference on Information Science and Engineering.

^ Cunsolo, Vincenzo D.; Distefano, Salvatore; Puliafito, Antonio; Scarpa, Marco (2009). ""Volunteer Computing and Desktop Cloud: The Cloud@Home Paradigm"". 2009 Eighth IEEE International Symposium on Network Computing and Applications. pp. 134–139. doi:10.1109/NCA.2009.41. S2CID 15848602.

^ Rouse, Margaret. ""What is a multi-cloud strategy"". SearchCloudApplications. Retrieved 3 July 2014.

^ King, Rachel. ""Pivotal's head of products: We're moving to a multi-cloud world"". ZDnet. Retrieved 3 July 2014.

^ Multcloud manage multiple cloud accounts. Retrieved on 06 August 2014

^ Gall, Richard (2018-05-16). ""Polycloud: a better alternative to cloud agnosticism"". Packt Hub. Retrieved 2019-11-11.

^ Roh, Lucas (31 August 2016). ""Is the Cloud Finally Ready for Big Data?"". dataconomy.com. Retrieved 29 January 2018.

^ Yang, C.; Huang, Q.; Li, Z.; Liu, K.; Hu, F. (2017). ""Big Data and cloud computing: innovation opportunities and challenges"". International Journal of Digital Earth. 10 (1): 13–53. Bibcode:2017IJDE...10...13Y. doi:10.1080/17538947.2016.1239771. S2CID 8053067.

^ Netto, M.; Calheiros, R.; Rodrigues, E.; Cunha, R.; Buyya, R. (2018). ""HPC Cloud for Scientific and Business Applications: Taxonomy, Vision, and Research Challenges"". ACM Computing Surveys. 51 (1): 8:1–8:29. arXiv:1710.08731. doi:10.1145/3150224. S2CID 3604131.

^ Eadline, Douglas. ""Moving HPC to the Cloud"". Admin Magazine. Admin Magazine. Retrieved 30 March 2019.

^ ""Penguin Computing On Demand (POD)"". Retrieved 23 January 2018.

^ Niccolai, James (11 August 2009). ""Penguin Puts High-performance Computing in the Cloud"". PCWorld. IDG Consumer & SMB. Retrieved 6 June 2016.

^ ""HPC in AWS"". Retrieved 23 January 2018.

^ ""Building GrepTheWeb in the Cloud, Part 1: Cloud Architectures"". Developer.amazonwebservices.com. Archived from the original on 5 May 2009. Retrieved 22 August 2010.

^ a b c Ryan, Mark D. ""Cloud Computing Privacy Concerns on Our Doorstep"". cacm.acm.org.

^ Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). ""CloudID: Trustworthy cloud-based and cross-enterprise biometric identification"". Expert Systems with Applications. 42 (21): 7905–7916. doi:10.1016/j.eswa.2015.06.025.

^ Indu, I.; Anand, P.M. Rubesh; Bhaskar, Vidhyacharan (August 1, 2018). ""Identity and access management in cloud environment: Mechanisms and challenges"". Engineering Science and Technology. 21 (4): 574–588. doi:10.1016/j.jestch.2018.05.010 – via www.sciencedirect.com.

^ a b ""Google Drive, Dropbox, Box and iCloud Reach the Top 5 Cloud Storage Security Breaches List"". psg.hitachi-solutions.com. Archived from the original on 2015-11-23. Retrieved 2015-11-22.

^ Maltais, Michelle (26 April 2012). ""Who owns your stuff in the cloud?"". Los Angeles Times. Retrieved 2012-12-14.

^ ""Security of virtualization, cloud computing divides IT and security pros"". Network World. 2010-02-22. Retrieved 2010-08-22.

^ ""The Bumpy Road to Private Clouds"". 2010-12-20. Retrieved 8 October 2014.

^ ""Should Companies Do Most of Their Computing in the Cloud? (Part 1) – Schneier on Security"". www.schneier.com. Retrieved 2016-02-28.

^ 
""Disadvantages of Cloud Computing (Part 1) – Limited control and flexibility"". www.cloudacademy.com. Retrieved 2016-11-03.

^ a b 
""The real limits of cloud computing"". www.itworld.com. 2012-05-14. Retrieved 2016-11-03.

^ Karra, Maria. ""Cloud solutions for translation, yes or no?"". IAPTI.org. Retrieved 16 February 2021.

^ Pradhan, Sayam (2021). ""Cloud Computing"". Transitioning from Traditional Data Centers to Cloud Computing: Pros and Cons (1 ed.). India. p. 14. ISBN 9798528758633.

^ Seltzer, Larry. ""Your infrastructure's in the cloud and the Internet goes down. Now, what?"". ZDNet. Retrieved 2020-06-01.

^ Smith, David Mitchell. ""Hype Cycle for Cloud Computing, 2013"". Gartner. Retrieved 3 July 2014.

^ ""The evolution of Cloud Computing"". Archived from the original on 29 March 2017. Retrieved 22 April 2015.

^ ""Remote work helps Zoom grow 169% in one year, posting $328.2M in Q1 revenue"". TechCrunch. Retrieved 2021-04-27.

^ Ruan, Keyun; Carthy, Joe; Kechadi, Tahar; Crosbie, Mark (2011-01-01). Cloud forensics: An overview.

^ R., Adams (2013). The emergence of cloud storage and the need for a new digital forensic process model. researchrepository.murdoch.edu.au. ISBN 9781466626621. Retrieved 2018-03-18.

^ Richard, Adams; Graham, Mann; Valerie, Hobbs (2017). ""ISEEK, a tool for high speed, concurrent, distributed forensic data acquisition"". Research Online. doi:10.4225/75/5a838d3b1d27f.

^ ""Office 365 Advanced eDiscovery"". Retrieved 2018-03-18.


Further reading[edit]
Millard, Christopher (2013). Cloud Computing Law. Oxford University Press. ISBN 978-0-19-967168-7.
Weisser, Alexander (2020). International Taxation of Cloud Computing. Editions Juridiques Libres, ISBN 978-2-88954-030-3.
Singh, Jatinder; Powles, Julia; Pasquier, Thomas; Bacon, Jean (July 2015). ""Data Flow Management and Compliance in Cloud Computing"". IEEE Cloud Computing. 2 (4): 24–32. doi:10.1109/MCC.2015.69. S2CID 9812531.
Armbrust, Michael; Stoica, Ion; Zaharia, Matei; Fox, Armando; Griffith, Rean; Joseph, Anthony D.; Katz, Randy; Konwinski, Andy; Lee, Gunho; Patterson, David; Rabkin, Ariel (1 April 2010). ""A view of cloud computing"". Communications of the ACM. 53 (4): 50. doi:10.1145/1721654.1721672. S2CID 1673644.
Hu, Tung-Hui (2015). A Prehistory of the Cloud. MIT Press. ISBN 978-0-262-02951-3.
Mell, P. (2011, September 31). The NIST Definition of Cloud Computing. Retrieved November 1, 2015, from National Institute of Standards and Technology website
External links[edit]
 Media related to Cloud computing at Wikimedia Commons



Wikiquote has quotations related to: Cloud computing

vteCloud computingAs a service
Content as a service
Data as a service
Desktop as a service
Function as a service
Infrastructure as a service
Integration platform as a service
Mobile backend as a service
Network as a service
Platform as a service
Security as a service
Software as a service
Technologies
Cloud database
Cloud storage
Data centers
Distributed file system for cloud
Hardware virtualization
Internet
Native cloud application
Networking
Security
Structured storage
Virtual appliance
Web APIs
Virtual private cloud
Applications
Box
Dropbox
Google
Workspace
Drive
HP Cloud (closed)
IBM Cloud
Microsoft
Office 365
OneDrive
Nextcloud
Oracle Cloud
Rackspace
Salesforce
Workday
Zoho
Platforms
Alibaba Cloud
Amazon Web Services
AppScale
Box
Bluemix
CloudBolt
Cloud Foundry
Cocaine (PaaS)
Creatio
Engine Yard
Helion
GE Predix
Google App Engine
GreenQloud
Heroku
IBM Cloud
Inktank
Jelastic
Microsoft Azure
MindSphere
Netlify
Oracle Cloud
OutSystems
openQRM
OpenShift
PythonAnywhere
RightScale
Scalr
Force.com
SAP Cloud Platform
Splunk
VMware vCloud Air
WaveMaker
Infrastructure
Alibaba Cloud
Amazon Web Services
Abiquo Enterprise Edition
CloudStack
Citrix Cloud
CtrlS
DigitalOcean
EMC Atmos
Eucalyptus
Fujitsu
Google Cloud Platform
GreenButton
GreenQloud
IBM Cloud
iland
Joyent
Linode
Lunacloud
Microsoft Azure
Mirantis
Netlify
Nimbula
Nimbus
OpenIO
OpenNebula
OpenStack
Oracle Cloud
OrionVM
Rackspace Cloud
Safe Swiss Cloud
Zadara
libvirt
libguestfs
OVirt
Virtual Machine Manager
Wakame-vdc
Virtual Private Cloud OnDemand

 Category
 Commons

vteParallel computingGeneral
Distributed computing
Parallel computing
Massively parallel
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Speculative (SpMT)
Preemptive
Cooperative
Clustered multi-thread (CMT)
Hardware scout
Theory
PRAM model
PEM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
Karp–Flatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Array data structure
Coordination
Multiprocessing
Memory coherency
Cache coherency
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Grid computer
Hardware acceleration
APIs
Ateji PX
Boost
Chapel
HPX
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
GPUOpen
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
Parallel Extensions
PVM
POSIX Threads
RaftLib
ROCm
UPC
TBB
ZPL
Problems
Automatic parallelization
Deadlock
Deterministic algorithm
Embarrassingly parallel
Parallel slowdown
Race condition
Software lockout
Scalability
Starvation

 Category: Parallel computing

Authority control National libraries
Spain
France (data)
Germany
Israel
United States
Japan
Other
Faceted Application of Subject Terminology
SUDOC (France)
1
2





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Cloud_computing&oldid=1085788015""
Categories: Cloud computingCloud infrastructureHidden categories: All articles with dead external linksArticles with dead external links from July 2018CS1 errors: missing periodicalArticles with short descriptionShort description is different from WikidataWikipedia indefinitely move-protected pagesWikipedia articles needing clarification from March 2021All Wikipedia articles needing clarificationArticles prone to spam from January 2017Use American English from April 2020All Wikipedia articles written in American EnglishWikipedia articles needing clarification from January 2021All articles with unsourced statementsArticles with unsourced statements from May 2021Wikipedia articles needing factual verification from July 2015Articles with unsourced statements from July 2015Articles with unsourced statements from May 2020Articles to be expanded from September 2021All articles to be expandedArticles using small message boxesArticles with unsourced statements from June 2018Commons category link from WikidataArticles with BNE identifiersArticles with BNF identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with FAST identifiersArticles with SUDOC identifiersArticles with multiple identifiers

"
4,Amazon,"




Amazon

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search



Look up Amazon or amazon in Wiktionary, the free dictionary.

Amazon most often refers to:

Amazons, a tribe of woman warriors in Greek mythology
Amazon rainforest, a rainforest covering most of the Amazon basin
Amazon River, in South America
Amazon (company), an American multinational technology company
Amazon or Amazone may also refer to:

Contents

1 Places

1.1 South America
1.2 Elsewhere


2 People
3 Art and entertainment

3.1 Fictional characters
3.2 Film and television
3.3 Games
3.4 Literature
3.5 Music


4 Military units
5 Organizations
6 Transportation

6.1 Land vehicles
6.2 Ships


7 Other uses
8 See also



Places[edit]
South America[edit]
Amazon Basin (sedimentary basin), a sedimentary basin at the middle and lower course of the river
Amazon basin, the part of South America drained by the river and its tributaries
Amazon Reef, at the mouth of the Amazon basin
Elsewhere[edit]
1042 Amazone, an asteroid
Amazon Creek, a stream in Oregon, US
People[edit]
Amazon Eve (born 1979), American model, fitness trainer, and actress
Lesa Lewis (born 1967), American professional bodybuilder nicknamed ""Amazon""
Art and entertainment[edit]
Fictional characters[edit]
Amazon (Amalgam Comics)
Amazon, an alias of the Marvel supervillain Man-Killer
Amazons (DC Comics), a group of superhuman characters
The Amazon, a Diablo II character
The Amazon, a Pro Wrestling character
Kamen Rider Amazon, title character in the fourth installment of the Kamen Rider series
Film and television[edit]


The Amazons (1917 film), an American silent tragedy
The Amazon (film), a 1921 German silent film
War Goddess, also known as The Amazons, a 1973 Italian adventure fantasy drama
Amazons (1984 film), an American thriller
Amazons (1986 film), an Argentine adventure fantasy
Amazon (1990 film), a 1990 drama
Amazon (1997 film), a short documentary
Amazon (1999 TV series), a Canadian drama
Amazon (2000 film), a 2000 French film
Amazon (2008 TV series), a British documentary series
Games[edit]
Amazon (chess), a fairy chess piece
Amazons (solitaire), a card game
Amazon (video game), a 1984 interactive fiction graphic adventure game
Amazon: Guardians of Eden, a 1991 video game
Game of the Amazons, a board game
Literature[edit]
Amazons (novel), a 1980 novel co-written by Don DeLillo, published under the pseudonym Cleo Birdwell
Amazons!, a fantasy anthology edited by Jessica Amanda Salmonson
Swallows and Amazons series, a series of twelve children's books by Arthur Ransome
The Amazon (novella), by Nikolai Leskov
The Amazons (play), by Arthur Wing Pinero
Music[edit]
The Amazons (band), a British indie band from Reading, Berkshire
Military units[edit]
Amazonian Guard, or ""the Amazons"", a bodyguard unit of Muammar Gaddafi
Amazons Company, a Greek ceremonial female battalion
Dahomey Amazons, a Fon regiment
Organizations[edit]
Amazon Bookstore Cooperative, a former feminist bookstore
Amazonen-Werke, a German agricultural machinery manufacturer
Los Angeles Amazons, an American football team
Takembeng, or les Amazones des SDF, a women's social movement in Cameroon
Transportation[edit]
Land vehicles[edit]
Amazon (automobile), a 1920s British cyclecar
Amazon, a GWR 3031 Class locomotive operating 1892–1908
Amazon, a GWR Iron Duke Class locomotive operating 1851–1877
Volvo Amazon, a 1956–1970 mid-size car
Ships[edit]
Amazon (1780 ship), launched in France in 1775 under another name
Amazon (brigantine), a Canadian brigantine launched 1861
Amazon (yacht), a British screw schooner built 1885
Amazon-class frigate, four classes of frigate of the British Royal Navy
Amazon-class sloop, of the British Royal Navy
French submarine Amazone (1916), an Armide-class diesel-electric attack submarine
HMS Amazon, nine ships of the Royal Navy
RMS Amazon, two ships of the Royal Mail Steam Packet Company
SMS Amazone (1843), a 3-masted sail corvette of the Prussian Navy
SMS Amazone, a 1900 2,700 ton Gazelle-class light cruiser
USS Amazon (1861), a US Navy bark
Other uses[edit]
Amazon (color), a variant of jungle green
Amazon parrot
See also[edit]
All pages with titles beginning with Amazon
All pages with titles containing Amazon
Amason (disambiguation)
Amazonas (disambiguation)
Amazonia (disambiguation)
Amazonian (disambiguation)
Amazonka, a 2008 album by Ruslana
Amyzon (disambiguation)
Topics referred to by the same term

 This disambiguation page lists  articles associated with the title Amazon.If an internal link led you here, you may wish to change the link to point directly to the intended article. 





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Amazon&oldid=1084655201""
Categories: Disambiguation pagesShip disambiguation pagesPlace name disambiguation pagesHidden categories: Disambiguation pages with short descriptionsShort description is different from WikidataAll article disambiguation pagesAll disambiguation pages

"
5,Laptop,"




Laptop

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Personal computer for mobile use
For other uses, see Laptop (disambiguation).
This article needs to be updated. Please help update this article to reflect recent events or newly available information. (November 2021)


 A Chromebook 11 laptop by Acer Inc.
 A MacBook Air laptop by Apple Inc.
A laptop, laptop computer, or notebook computer is a small, portable personal computer (PC) with a screen and alphanumeric keyboard. Laptops typically have a clam shell form factor with the screen mounted on the inside of the upper lid and the keyboard on the inside of the lower lid, although 2-in-1 PCs with a detachable keyboard are often marketed as laptops or as having a laptop mode. Laptops are folded shut for transportation, and thus are suitable for mobile use.[1] Its name comes from lap, as it was deemed practical to be placed on a person's lap when being used. Today, laptops are used in a variety of settings, such as at work, in education, for playing games, web browsing, for personal multimedia, and for general home computer use.
As of 2021, in American English, the terms laptop computer and notebook computer are used interchangeably;[2] in other dialects of English, one or the other may be preferred. Although the terms notebook computers or notebooks originally referred to a specific size of laptop (originally smaller and lighter than mainstream laptops of the time),[3] the terms have come to mean the same thing and notebook no longer refers to any specific size.
Laptops combine all the input/output components and capabilities of a desktop computer, including the display screen, small speakers, a keyboard, data storage device, sometimes an optical disc drive, pointing devices (such as a touch pad or pointing stick), with an operating system, a processor and memory into a single unit. Most modern laptops feature integrated webcams and built-in microphones, while many also have touchscreens. Laptops can be powered either from an internal battery or by an external power supply from an AC adapter. Hardware specifications, such as the processor speed and memory capacity, significantly vary between different types, models and price points.
Design elements, form factor and construction can also vary significantly between models depending on the intended use. Examples of specialized models of laptops include rugged notebooks for use in construction or military applications, as well as low production cost laptops such as those from the One Laptop per Child (OLPC) organization, which incorporate features like solar charging and semi-flexible components not found on most laptop computers. Portable computers, which later developed into modern laptops, were originally considered to be a small niche market, mostly for specialized field applications, such as in the military, for accountants, or traveling sales representatives. As portable computers evolved into modern laptops, they became widely used for a variety of purposes.[4]

Contents

1 History
2 Etymology
3 Types

3.1 Smaller and larger laptops
3.2 Convertible, hybrid, 2-in-1
3.3 Rugged laptop


4 Hardware

4.1 Display

4.1.1 Sizes
4.1.2 Resolution
4.1.3 Refresh rates and 3D


4.2 Central processing unit
4.3 Graphical processing unit
4.4 Memory
4.5 Internal storage
4.6 Removable media drive
4.7 Inputs
4.8 Input/output (I/O) ports
4.9 Expansion cards
4.10 Battery and power supply
4.11 Power connectors
4.12 Cooling
4.13 Docking station
4.14 Charging trolleys
4.15 Solar panels
4.16 Accessories
4.17 Modularity
4.18 Obsolete features


5 Comparison with desktops

5.1 Advantages
5.2 Disadvantages

5.2.1 Performance
5.2.2 Upgradeability
5.2.3 Ergonomics and health effects

5.2.3.1 Wrists
5.2.3.2 Neck and spine
5.2.3.3 Possible effect on fertility


5.2.4 Thighs
5.2.5 Durability

5.2.5.1 Equipment wear
5.2.5.2 Heating and cooling
5.2.5.3 Battery life


5.2.6 Security and privacy




6 Sales

6.1 Manufacturers
6.2 Market share


7 Disposal
8 Extreme use
9 See also
10 Notes
11 References



History[edit]
Main article: History of laptops
 Alan Kay holding the mockup of his Dynabook concept in 2008
 The Epson HX-20, the first ""laptop computer"", was invented in 1980 and introduced in 1981
As the personal computer (PC) became feasible in 1971, the idea of a portable personal computer soon followed. A ""personal, portable information manipulator"" was imagined by Alan Kay at Xerox PARC in 1968,[5] and described in his 1972 paper as the ""Dynabook"".[6] The IBM Special Computer APL Machine Portable (SCAMP) was demonstrated in 1973.[7] This prototype was based on the IBM PALM processor.[8] The IBM 5100, the first commercially available portable computer, appeared in September 1975, and was based on the SCAMP prototype.[9]
As 8-bit CPU machines became widely accepted, the number of portables increased rapidly. The first ""laptop-sized notebook computer"" was the Epson HX-20,[10][11] invented (patented) by Suwa Seikosha's Yukio Yokozawa in July 1980,[12] introduced at the COMDEX computer show in Las Vegas by Japanese company Seiko Epson in 1981,[13][11] and released in July 1982.[11][14] It had an LCD screen, a rechargeable battery, and a calculator-size printer, in a 1.6 kg (3.5 lb) chassis, the size of an A4 notebook.[11] It was described as a ""laptop"" and ""notebook"" computer in its patent.[12]
The portable micro computer Portal of the French company R2E Micral CCMC officially appeared in September 1980 at the Sicob show in Paris. It was a portable microcomputer designed and marketed by the studies and developments department of R2E Micral at the request of the company CCMC specializing in payroll and accounting. It was based on an Intel 8085 processor, 8-bit, clocked at 2 MHz. It was equipped with a central 64 KB RAM, a keyboard with 58 alphanumeric keys and 11 numeric keys (separate blocks), a 32-character screen, a floppy disk: capacity = 140,000 characters, of a thermal printer: speed = 28 characters / second, an asynchronous channel, asynchronous channel, a 220 V power supply. It weighed 12 kg and its dimensions were 45 × 45 × 15 cm. It provided total mobility. Its operating system was aptly named Prologue.

 A Siemens PCD-3Psx laptop, released in 1989
The Osborne 1, released in 1981, was a luggable computer that used the Zilog Z80 and weighed 24.5 pounds (11.1 kg).[15] It had no battery, a 5 in (13 cm) cathode-ray tube (CRT) screen, and dual 5.25 in (13.3 cm) single-density floppy drives. Both Tandy/RadioShack and Hewlett Packard (HP) also produced portable computers of varying designs during this period.[16][17] The first laptops using the flip form factor appeared in the early 1980s. The Dulmont Magnum was released in Australia in 1981–82, but was not marketed internationally until 1984–85. The US$8,150 (equivalent to $22,880 in 2021) GRiD Compass 1101, released in 1982, was used at NASA and by the military, among others. The Sharp PC-5000,[18] Ampere[19] and Gavilan SC released in 1983. The Gavilan SC was described as a ""laptop"" by its manufacturer,[20] while the Ampere had a modern clamshell design.[19][21] The Toshiba T1100 won acceptance not only among PC experts but the mass market as a way to have PC portability.[22]
From 1983 onward, several new input techniques were developed and included in laptops, including the touch pad (Gavilan SC, 1983), the pointing stick (IBM ThinkPad 700, 1992), and handwriting recognition (Linus Write-Top,[23] 1987). Some CPUs, such as the 1990 Intel i386SL, were designed to use minimum power to increase battery life of portable computers and were supported by dynamic power management features such as Intel SpeedStep and AMD PowerNow! in some designs.
Displays reached 640x480 (VGA) resolution by 1988 (Compaq SLT/286), and color screens started becoming a common upgrade in 1991,[24] with increases in resolution and screen size occurring frequently until the introduction of 17"" screen laptops in 2003. Hard drives started to be used in portables, encouraged by the introduction of 3.5"" drives in the late 1980s, and became common in laptops starting with the introduction of 2.5"" and smaller drives around 1990; capacities have typically lagged behind physically larger desktop drives.
Common resolutions of laptop webcams are 720p (HD), and in lower-end laptops 480p.[25] The earliest known laptops with 1080p (Full HD) webcams like the Samsung 700G7C were released in the early 2010s.[26]
Optical disc drives became common in full-size laptops around 1997; this initially consisted of CD-ROM drives, which were supplanted by CD-R, DVD, and Blu-ray drives with writing capability over time. Starting around 2011, the trend shifted against internal optical drives, and as of 2021, they have largely disappeared; they are still readily available as external peripherals.

Etymology[edit]
While the terms laptop and notebook are used interchangeably today, there is some question as to the original etymology and specificity of either term. The term laptop appears to have been coined in the early 1980s to describe a mobile computer which could be used on one's lap and to distinguish these devices from earlier and much heavier portable computers (informally called ""luggables""). The term notebook appears to have gained currency somewhat later as manufacturers started producing even smaller portable devices, further reducing their weight and size and incorporating a display roughly the size of A4 paper;[3] these were marketed as notebooks to distinguish them from bulkier mainstream or desktop replacement laptops.

Types[edit]
 Compaq Armada laptop from the late 1990s
 Apple MacBook Air, an ""ultraportable"" laptop weighing under 3.0 lb (1.36 kg)
 Lenovo's IdeaPad laptop
 Lenovo's ThinkPad business laptop, originally an IBM product
 Acer Aspire laptop
 Asus Transformer Pad, a hybrid tablet, powered by Android OS
 Microsoft Surface Pro 3, 2-in-1 detachable
 Alienware gaming laptop with backlit keyboard and touch pad
 Samsung Sens laptop
 Samsung Notebook 3
 Panasonic Toughbook CF-M34, a rugged laptop/subnotebook
Since the introduction of portable computers during the late 1970s, their form has changed significantly, spawning a variety of visually and technologically differing subclasses. Except where there is a distinct legal trademark around a term (notably, Ultrabook), there are rarely hard distinctions between these classes and their usage has varied over time and between different sources. Since the late 2010s, the use of more specific terms has become less common, with sizes distinguished largely by the size of the screen.

Smaller and larger laptops[edit]
Main articles: Subnotebook and Desktop replacement computer
There were in the past a number of marketing categories for smaller and larger laptop computers; these included ""subnotebook"" models, low cost ""netbooks"", and ""ultra-mobile PCs"" where the size class overlapped with devices like smartphone and handheld tablets, and ""Desktop replacement"" laptops for machines notably larger and heavier than typical to operate more powerful processors or graphics hardware.[27] All of these terms have fallen out of favor as the size of mainstream laptops has gone down and their capabilities have gone up; except for niche models, laptop sizes tend to be distinguished by the size of the screen, and for more powerful models, by any specialized purpose the machine is intended for, such as a ""gaming laptop"" or a ""mobile workstation"" for professional use.See also: Gaming computer § Gaming laptop computers, and Workstation
Convertible, hybrid, 2-in-1[edit]
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2015) (Learn how and when to remove this template message)



Main article: 2-in-1 PC
The latest trend of technological convergence in the portable computer industry spawned a broad range of devices, which combined features of several previously separate device types. The hybrids, convertibles, and 2-in-1s emerged as crossover devices, which share traits of both tablets and laptops. All such devices have a touchscreen display designed to allow users to work in a tablet mode, using either multi-touch gestures or a stylus/digital pen.
Convertibles are devices with the ability to conceal a hardware keyboard. Keyboards on such devices can be flipped, rotated, or slid behind the back of the chassis, thus transforming from a laptop into a tablet. Hybrids have a keyboard detachment mechanism, and due to this feature, all critical components are situated in the part with the display. 2-in-1s can have a hybrid or a convertible form, often dubbed 2-in-1 detachable and 2-in-1 convertibles respectively, but are distinguished by the ability to run a desktop OS, such as Windows 10. 2-in-1s are often marketed as laptop replacement tablets.[28]
2-in-1s are often very thin, around 10 millimetres (0.39 in), and light devices with a long battery life. 2-in-1s are distinguished from mainstream tablets as they feature an x86-architecture CPU (typically a low- or ultra-low-voltage model), such as the Intel Core i5, run a full-featured desktop OS like Windows 10, and have a number of typical laptop I/O ports, such as USB 3 and Mini DisplayPort.
2-in-1s are designed to be used not only as a media consumption device but also as valid desktop or laptop replacements, due to their ability to run desktop applications, such as Adobe Photoshop. It is possible to connect multiple peripheral devices, such as a mouse, keyboard, and several external displays to a modern 2-in-1.
Microsoft Surface Pro-series devices and Surface Book are examples of modern 2-in-1 detachable, whereas Lenovo Yoga-series computers are a variant of 2-in-1 convertibles. While the older Surface RT and Surface 2 have the same chassis design as the Surface Pro, their use of ARM processors and Windows RT do not classify them as 2-in-1s, but as hybrid tablets.[29] Similarly, a number of hybrid laptops run a mobile operating system, such as Android. These include Asus's Transformer Pad devices, examples of hybrids with a detachable keyboard design, which do not fall in the category of 2-in-1s.

Rugged laptop[edit]
Main article: Rugged computer
A rugged laptop is designed to reliably operate in harsh usage conditions such as strong vibrations, extreme temperatures, and wet or dusty environments. Rugged laptops are bulkier, heavier, and much more expensive than regular laptops,[30] and thus are seldom seen in regular consumer use.

Hardware[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2016) (Learn how and when to remove this template message)
Main article: Personal computer hardware
 Miniaturization: a comparison of a desktop computer motherboard (ATX form factor) to a motherboard from a 13"" laptop (2008 unibody MacBook)
 Inner view of a Sony VAIO laptop
 A SODIMM memory module
The basic components of laptops function identically to their desktop counterparts. Traditionally they were miniaturized and adapted to mobile use, although desktop systems increasingly use the same smaller, lower-power parts which were originally developed for mobile use. The design restrictions on power, size, and cooling of laptops limit the maximum performance of laptop parts compared to that of desktop components, although that difference has increasingly narrowed.[31]
In general, laptop components are not intended to be replaceable or upgradable by the end-user, except for components that can be detached; in the past, batteries and optical drives were commonly exchangeable. This restriction is one of the major differences between laptops and desktop computers, because the large ""tower"" cases used in desktop computers are designed so that new motherboards, hard disks, sound cards, RAM, and other components can be added. Memory and storage can often be upgraded with some disassembly, but with the most compact laptops, there may be no upgradeable components at all.[32]
Intel, Asus, Compal, Quanta, and some other laptop manufacturers have created the Common Building Block standard for laptop parts to address some of the inefficiencies caused by the lack of standards and inability to upgrade components.[33]
The following sections summarizes the differences and distinguishing features of laptop components in comparison to desktop personal computer parts.[34]

Display[edit]
Internally, a display is usually an LCD panel, although occasionally OLEDs are used. These interface to the laptop using the LVDS or embedded DisplayPort protocol, while externally, it can be a glossy screen or a matte (anti-glare) screen. As of 2021, mainstream consumer laptops tend to come with either 13"" or 15""-16"" screens; 14"" models are more popular among business machines. Larger and smaller models are available, but less common – there is no clear dividing line in minimum or maximum size. Machines small enough to be handheld (screens in the 6–8"" range) can be marketed either as very small laptops or ""handheld PCs,"" while the distinction between the largest laptops and ""All-in-One"" desktops is whether they fold for travel.

Sizes[edit]
In the past, there was a broader range of marketing terms (both formal and informal) to distinguish between different sizes of laptops. These included Netbooks, subnotebooks, Ultra-mobile PC, and Desktop replacement computers; these are sometimes still used informally, although they are essentially dead in terms of manufacturer marketing.

Resolution[edit]
Having a higher resolution display allows more items to fit onscreen at a time, improving the user's ability to multitask, although at the higher resolutions on smaller screens, the resolution may only serve to display sharper graphics and text rather than increasing the usable area. Since the introduction of the MacBook Pro with Retina display in 2012, there have been an increase in the availability of ""HiDPI"" (or high Pixel density) displays; as of 2021, this is generally considered to be anything higher than 1920 pixels wide. This has increasingly converged around 4K (3840-pixel-wide) resolutions.
External displays can be connected to most laptops, and models with a Mini DisplayPort can handle up to three.[35]

Refresh rates and 3D[edit]
The earliest laptops known to feature a display with doubled 120 Hz of refresh rate and active shutter 3D system were released in 2011 by Dell (M17x) and Samsung (700G7A).[36][37]

Central processing unit[edit]
A laptop's central processing unit (CPU) has advanced power-saving features and produces less heat than one intended purely for desktop use. Mainstream laptop CPUs made after 2018 have four processor cores, although some inexpensive models still have 2-core CPUs, and 6-core and 8-core models are also available.
For the low price and mainstream performance, there is no longer a significant performance difference between laptop and desktop CPUs, but at the high end, the fastest desktop CPUs still substantially outperform the fastest laptop processors, at the expense of massively higher power consumption and heat generation; the fastest laptop processors top out at 56 watts of heat, while the fastest desktop processors top out at 150 watts.
There has been a wide range of CPUs designed for laptops available from both Intel, AMD, and other manufacturers. On non-x86 architectures, Motorola and IBM produced the chips for the former PowerPC-based Apple laptops (iBook and PowerBook). Between around 2000 to 2014, most full-size laptops had socketed, replaceable CPUs; on thinner models, the CPU was soldered on the motherboard and was not replaceable or upgradable without replacing the motherboard. Since 2015, Intel has not offered new laptop CPU models with pins to be interchangeable, preferring ball grid array chip packages which have to be soldered;[38]and as of 2021, only a few rare models using desktop parts.
In the past, some laptops have used a desktop processor instead of the laptop version and have had high-performance gains at the cost of greater weight, heat, and limited battery life; this is not unknown as of 2021, but since around 2010, the practice has been restricted to small-volume gaming models. Laptop CPUs are rarely able to be overclocked; most use locked processors. Even on gaming models where unlocked processors are available, the cooling system in most laptops is often very close to its limits and there is rarely headroom for an overclocking–related operating temperature increase.

Graphical processing unit[edit]
On most laptops, a graphical processing unit (GPU) is integrated into the CPU to conserve power and space. This was introduced by Intel with the Core i-series of mobile processors in 2010, and similar accelerated processing unit (APU) processors by AMD later that year.
Before that, lower-end machines tended to use graphics processors integrated into the system chipset, while higher-end machines had a separate graphics processor. In the past, laptops lacking a separate graphics processor were limited in their utility for gaming and professional applications involving 3D graphics, but the capabilities of CPU-integrated graphics have converged with the low-end of dedicated graphics processors since the mid-2010s.
Higher-end laptops intended for gaming or professional 3D work still come with dedicated and in some cases even dual, graphics processors on the motherboard or as an internal expansion card. Since 2011, these almost always involve switchable graphics so that when there is no demand for the higher performance dedicated graphics processor, the more power-efficient integrated graphics processor will be used. Nvidia Optimus and AMD Hybrid Graphics are examples of this sort of system of switchable graphics.

Memory[edit]
Since around the year 2000, most laptops have used SO-DIMM RAM,[34] although, as of 2021, an increasing number of models use memory soldered to the motherboard. Before 2000, most laptops used proprietary memory modules if their memory was upgradable.
In the early 2010s, high end laptops such as the 2011 Samsung 700G7A have passed the 10 GB RAM barrier, featuring 16 GB of RAM.[39]
When upgradeable, memory slots are sometimes accessible from the bottom of the laptop for ease of upgrading; in other cases, accessing them requires significant disassembly. Most laptops have two memory slots, although some will have only one, either for cost savings or because some amount of memory is soldered. Some high-end models have four slots; these are usually mobile engineering workstations, although a few high-end models intended for gaming do as well.
As of 2021, 8 GB RAM is most common, with lower-end models occasionally having 4GB. Higher-end laptops may come with 16 GB of RAM or more.

Internal storage[edit]
The earliest laptops most often used floppy disk for storage, although a few used either RAM disk or tape, by the late 1980s hard disk drives had become the standard form of storage.
Between 1990 and 2009, almost all laptops typically had a hard disk drive (HDD) for storage; since then, solid-state drives (SSD) have gradually come to supplant hard drives in all but some inexpensive consumer models. Solid-state drives are faster and more power-efficient, as well as eliminating the hazard of drive and data corruption caused by a laptop's physical impacts, as they use no mechanical parts such as a rotational platter.[40] In many cases, they are more compact as well. Initially, in the late 2000s, SSDs were substantially more expensive than HDDs, but as of 2021 prices on smaller capacity (under 1 terabyte) drives have converged; larger capacity drives remain more expensive than comparable-sized HDDs.
Since around 1990, where a hard drive is present it will typically be a 2.5-inch drive; some very compact laptops support even smaller 1.8-inch HDDs, and a very small number used 1"" Microdrives. Some SSDs are built to match the size/shape of a laptop hard drive, but increasingly they have been replaced with smaller mSATA or M.2 cards. SSDs using the newer and much faster NVM Express standard for connecting are only available as cards.
As of 2021, many laptops no longer contain space for a 2.5"" drive, accepting only M.2 cards; a few of the smallest have storage soldered to the motherboard. For those that can, they can typically contain a single 2.5-inch drive, but a small number of laptops with a screen wider than 15 inches can house two drives.
A variety of external HDDs or NAS data storage servers with support of RAID technology can be attached to virtually any laptop over such interfaces as USB, FireWire, eSATA, or Thunderbolt, or over a wired or wireless network to further increase space for the storage of data. Many laptops also incorporate a card reader which allows for use of memory cards, such as those used for digital cameras, which are typically SD or microSD cards. This enables users to download digital pictures from an SD card onto a laptop, thus enabling them to delete the SD card's contents to free up space for taking new pictures.

Removable media drive[edit]
Optical disc drives capable of playing CD-ROMs, compact discs (CD), DVDs, and in some cases, Blu-ray discs (BD), were nearly universal on full-sized models between the mid-1990s and the early 2010s. As of 2021, drives are uncommon in compact or premium laptops; they remain available in some bulkier models, but the trend towards thinner and lighter machines is gradually eliminating these drives and players – when needed they can be connected via USB instead.

Inputs[edit]
 Closeup of a touchpad on an Acer laptop, where buttons and the touch-sensitive surface are shared
 Closeup of a TrackPoint cursor and UltraNav buttons on a ThinkPad laptop
 Interfaces on a ThinkPad laptop (2011): Ethernet network port (center), VGA (left), DisplayPort (top right) and USB 2.0 (bottom right). Due to the trend towards very flat laptops and the widespread use of WLAN, the relatively high Ethernet socket is no longer mandatory in today's devices, as is the technically outdated VGA.
An alphanumeric keyboard is used to enter text, data, and other commands (e.g., function keys). A touchpad (also called a trackpad), a pointing stick, or both, are used to control the position of the cursor on the screen, and an integrated keyboard[41] is used for typing. Some touchpads have buttons separate from the touch surface, while others share the surface. A quick double-tap is typically registered as a click, and operating systems may recognize multi-finger touch gestures.
An external keyboard and mouse may be connected using a USB port or wirelessly, via Bluetooth or similar technology. Some laptops have multitouch touchscreen displays, either available as an option or standard. Most laptops have webcams and microphones, which can be used to communicate with other people with both moving images and sound, via web conferencing or video-calling software.
Laptops typically have USB ports and a combined headphone/microphone jack, for use with headphones, a combined headset, or an external mic. Many laptops have a card reader for reading digital camera SD cards.

Input/output (I/O) ports[edit]
On a typical laptop there are several USB ports; if they use only the older USB connectors instead of USB-C, they will typically have an external monitor port (VGA, DVI, HDMI or Mini DisplayPort or occasionally more than one), an audio in/out port (often in form of a single socket) is common. It is possible to connect up to three external displays to a 2014-era laptop via a single Mini DisplayPort, using multi-stream transport technology.[35]
Apple, in a 2015 version of its MacBook, transitioned from a number of different I/O ports to a single USB-C port.[42] This port can be used both for charging and connecting a variety of devices through the use of aftermarket adapters. Google, with its updated version of Chromebook Pixel, shows a similar transition trend towards USB-C, although keeping older USB Type-A ports for a better compatibility with older devices.[43] Although being common until the end of the 2000s decade, Ethernet network port are rarely found on modern laptops, due to widespread use of wireless networking, such as Wi-Fi. Legacy ports such as a PS/2 keyboard/mouse port, serial port, parallel port, or FireWire are provided on some models, but they are increasingly rare. On Apple's systems, and on a handful of other laptops, there are also Thunderbolt ports, but Thunderbolt 3 uses USB-C. Laptops typically have a headphone jack, so that the user can connect external headphones or amplified speaker systems for listening to music or other audio.

Expansion cards[edit]
In the past, a PC Card (formerly PCMCIA) or ExpressCard slot for expansion was often present on laptops to allow adding and removing functionality, even when the laptop is powered on; these are becoming increasingly rare since the introduction of USB 3.0. Some internal subsystems such as Ethernet, Wi-Fi, or a wireless cellular modem can be implemented as replaceable internal expansion cards, usually accessible under an access cover on the bottom of the laptop. The standard for such cards is PCI Express, which comes in both mini and even smaller M.2 sizes. In newer laptops, it is not uncommon to also see Micro SATA (mSATA) functionality on PCI Express Mini or M.2 card slots allowing the use of those slots for SATA-based solid-state drives.[44]

Battery and power supply[edit]
Main article: Smart battery
 Almost all laptops use smart batteries
Since the late 1990s, laptops have typically used lithium ion or lithium polymer batteries, These replaced the older nickel metal-hydride typically used in the 1990s, and nickel–cadmium batteries used in most of the earliest laptops. A few of the oldest laptops used non-rechargeable batteries, or lead–acid batteries.
Battery life is highly variable by model and workload and can range from one hour to nearly a day. A battery's performance gradually decreases over time; a substantial reduction in capacity is typically evident after one to three years of regular use, depending on the charging and discharging pattern and the design of the battery. Innovations in laptops and batteries have seen situations in which the battery can provide up to 24 hours of continued operation, assuming average power consumption levels. An example is the HP EliteBook 6930p when used with its ultra-capacity battery.[45]

 A 2011 laptop with extended capacity replacement battery inserted.
Laptops with removable batteries may support larger replacement batteries with extended capacity.
A laptop's battery is charged using an external power supply, which is plugged into a wall outlet. The power supply outputs a DC voltage typically in the range of 7.2—24 volts. The power supply is usually external and connected to the laptop through a DC connector cable. In most cases, it can charge the battery and power the laptop simultaneously. When the battery is fully charged, the laptop continues to run on power supplied by the external power supply, avoiding battery use. If the used power supply is not strong enough to power computing components and charge the battery simultaneously, the battery may charge in a shorter period of time if the laptop is turned off or sleeping. The charger typically adds about 400 grams (0.88 lb) to the overall transporting weight of a laptop, although some models are substantially heavier or lighter. Most 2016-era laptops use a smart battery, a rechargeable battery pack with a built-in battery management system (BMS). The smart battery can internally measure voltage and current, and deduce charge level and State of Health (SoH) parameters, indicating the state of the cells.[citation needed]

Power connectors[edit]
 Laptop power supply with cylindrical coaxial DC power connector
Historically, DC connectors, typically cylindrical/barrel-shaped coaxial power connectors have been used in laptops. Some vendors such as Lenovo made intermittent use of a rectangular connector.
Some connector heads feature a center pin to allow the end device to determine the power supply type by measuring the resistance between it and the connector's negative pole (outer surface). Vendors may block charging if a power supply is not recognized as original part, which could deny the legitimate use of universal third-party chargers.[46]

 USB-C connector
With the advent of USB-C, portable electronics made increasing use of it for both power delivery and data transfer. Its support for 20 V (common laptop power supply voltage) and 5 A typically suffices for low to mid-end laptops, but some with higher power demands such as gaming laptops depend on dedicated DC connectors to handle currents beyond 5 A without risking overheating, some even above 10 A. Additionally, dedicated DC connectors are more durable and less prone to wear and tear from frequent reconnection, as their design is less delicate.[47]

Cooling[edit]
Waste heat from the operation is difficult to remove in the compact internal space of a laptop. The earliest laptops used passive cooling; this gave way to heat sinks placed directly on the components to be cooled, but when these hot components are deep inside the device, a large space-wasting air duct is needed to exhaust the heat. Modern laptops instead rely on heat pipes to rapidly move waste heat towards the edges of the device, to allow for a much smaller and compact fan and heat sink cooling system. Waste heat is usually exhausted away from the device operator towards the rear or sides of the device. Multiple air intake paths are used since some intakes can be blocked, such as when the device is placed on a soft conforming surface like a chair cushion. Secondary device temperature monitoring may reduce performance or trigger an emergency shutdown if it is unable to dissipate heat, such as if the laptop were to be left running and placed inside a carrying case. Aftermarket cooling pads with external fans can be used with laptops to reduce operating temperatures.

Docking station[edit]
 Docking station and laptop
A docking station (sometimes referred to simply as a dock) is a laptop accessory that contains multiple ports and in some cases expansion slots or bays for fixed or removable drives. A laptop connects and disconnects to a docking station, typically through a single large proprietary connector. A docking station is an especially popular laptop accessory in a corporate computing environment, due to a possibility of a docking station transforming a laptop into a full-featured desktop replacement, yet allowing for its easy release. This ability can be advantageous to ""road warrior"" employees who have to travel frequently for work, and yet who also come into the office. If more ports are needed, or their position on a laptop is inconvenient, one can use a cheaper passive device known as a port replicator. These devices mate to the connectors on the laptop, such as through USB or FireWire.

Charging trolleys[edit]
Laptop charging trolleys, also known as laptop trolleys or laptop carts, are mobile storage containers to charge multiple laptops, netbooks, and tablet computers at the same time. The trolleys are used in schools that have replaced their traditional static computer labs[48] suites of desktop equipped with ""tower"" computers, but do not have enough plug sockets in an individual classroom to charge all of the devices. The trolleys can be wheeled between rooms and classrooms so that all students and teachers in a particular building can access fully charged IT equipment.[49]
Laptop charging trolleys are also used to deter and protect against opportunistic and organized theft. Schools, especially those with open plan designs, are often prime targets for thieves who steal high-value items. Laptops, netbooks, and tablets are among the highest–value portable items in a school. Moreover, laptops can easily be concealed under clothing and stolen from buildings. Many types of laptop–charging trolleys are designed and constructed to protect against theft. They are generally made out of steel, and the laptops remain locked up while not in use. Although the trolleys can be moved between areas from one classroom to another, they can often be mounted or locked to the floor, support pillars, or walls to prevent thieves from stealing the laptops, especially overnight.[48]

Solar panels[edit]
Main article: Solar notebook
In some laptops, solar panels are able to generate enough solar power for the laptop to operate.[50] The One Laptop Per Child Initiative released the OLPC XO-1 laptop which was tested and successfully operated by use of solar panels.[51] Presently, they are designing an OLPC XO-3 laptop with these features. The OLPC XO-3 can operate with 2 watts of electricity because its renewable energy resources generate a total of 4 watts.[52][53] Samsung has also designed the NC215S solar–powered notebook that will be sold commercially in the U.S. market.[54]

Accessories[edit]
A common accessory for laptops is a laptop sleeve, laptop skin, or laptop case, which provides a degree of protection from scratches. Sleeves, which are distinguished by being relatively thin and flexible, are most commonly made of neoprene, with sturdier ones made of low-resilience polyurethane. Some laptop sleeves are wrapped in ballistic nylon to provide some measure of waterproofing. Bulkier and sturdier cases can be made of metal with polyurethane padding inside and may have locks for added security. Metal, padded cases also offer protection against impacts and drops. Another common accessory is a laptop cooler, a device that helps lower the internal temperature of the laptop either actively or passively. A common active method involves using electric fans to draw heat away from the laptop, while a passive method might involve propping the laptop up on some type of pad so it can receive more airflow. Some stores sell laptop pads that enable a reclining person on a bed to use a laptop.

Modularity[edit]
 Opened bottom covers allow replacement of RAM and storage modules (Lenovo G555)
Some of the components of earlier models of laptops can easily be replaced without opening completely its bottom part, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc.
Some of the components of recent models of laptops reside inside. Replacing most of its components, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc., requires removal of its either top or bottom part, removal of the motherboard, and returning them.
In some types, solder and glue are used to mount components such as RAM, storage, and batteries, making repairs additionally difficult.[55][56]

Obsolete features[edit]
 A modem PCMCIA card on an old ThinkPad. The card would normally fully insert into the socket.
Features that certain early models of laptops used to have that are not available in most current laptops include:

Reset (""cold restart"") button in a hole (needed a thin metal tool to press)
Instant power off button in a hole (needed a thin metal tool to press)
Integrated charger or power adapter inside the laptop
Floppy disk drive
Serial port
Parallel port
Modem
Shared PS/2 input device port
IrDA
S-video port[note 1]
S/PDIF audio port
PC Card / PCMCIA slot
ExpressCard slot
CD/DVD Drives (starting with 2013 models)
VGA port (starting with 2013 models)
Comparison with desktops[edit]
Advantages[edit]
 A teacher using the laptop as part of a workshop for school children
 Wikipedia co-founder Jimmy Wales using a laptop on a park bench
Portability is usually the first feature mentioned in any comparison of laptops versus desktop PCs.[57] Physical portability allows a laptop to be used in many places—not only at home and the office but also during commuting and flights, in coffee shops, in lecture halls and libraries, at clients' locations or a meeting room, etc. Within a home, portability enables laptop users to move their devices from the living room to the dining room to the family room. Portability offers several distinct advantages:

Productivity: Using a laptop in places where a desktop PC cannot be used can help employees and students to increase their productivity on work or school tasks, such as an office worker reading their work e-mails during an hour-long commute by train, or a student doing their homework at the university coffee shop during a break between lectures, for example.
Immediacy: Carrying a laptop means having instant access to information, including personal and work files. This allows better collaboration between coworkers or students, as a laptop can be flipped open to look at a report, document, spreadsheet, or presentation anytime and anywhere.
Up-to-date information: If a person has more than one desktop PC, a problem of synchronization arises: changes made on one computer are not automatically propagated to the others. There are ways to resolve this problem, including physical transfer of updated files (using a USB flash memory stick or CD-ROMs) or using synchronization software over the Internet, such as cloud computing. However, transporting a single laptop to both locations avoids the problem entirely, as the files exist in a single location and are always up-to-date.
Connectivity: In the 2010s, a proliferation of Wi-Fi wireless networks and cellular broadband data services (HSDPA, EVDO and others) in many urban centers, combined with near-ubiquitous Wi-Fi support by modern laptops[note 2] meant that a laptop could now have easy Internet and local network connectivity while remaining mobile. Wi-Fi networks and laptop programs are especially widespread at university campuses.[58]
Other advantages of laptops:

Size: Laptops are smaller than desktop PCs. This is beneficial when space is at a premium, for example in small apartments and student dorms. When not in use, a laptop can be closed and put away in a desk drawer.
Low power consumption: Laptops are several times more power-efficient than desktops. A typical laptop uses 20–120 W, compared to 100–800 W for desktops. This could be particularly beneficial for large businesses, which run hundreds of personal computers thus multiplying the potential savings, and homes where there is a computer running 24/7 (such as a home media server, print server, etc.).
Quiet: Laptops are typically much quieter than desktops, due both to the components (quieter, slower 2.5-inch hard drives) and to less heat production leading to the use of fewer and slower cooling fans.
Battery: a charged laptop can continue to be used in case of a power outage and is not affected by short power interruptions and blackouts. A desktop PC needs an uninterruptible power supply (UPS) to handle short interruptions, blackouts, and spikes; achieving on-battery time of more than 20–30 minutes for a desktop PC requires a large and expensive UPS.[59]
All-in-One: designed to be portable, most 2010-era laptops have all components integrated into the chassis (however, some small laptops may not have an internal CD/CDR/DVD drive, so an external drive needs to be used). For desktops (excluding all-in-ones) this is usually divided into the desktop ""tower"" (the unit with the CPU, hard drive, power supply, etc.), keyboard, mouse, display screen, and optional peripherals such as speakers.
Disadvantages[edit]
Compared to desktop PCs, laptops have disadvantages in the following areas:

Performance[edit]
Parts of this article (those related to sub-section) need to be updated. The reason given is: info is since 2008, nearly 13 years old. Please help update this article to reflect recent events or newly available information. (April 2021)
While the performance of mainstream desktops and laptops are comparable, and the cost of laptops has fallen less rapidly than desktops, laptops remain more expensive than desktop PCs at the same performance level.[60][needs update] The upper limits of performance of laptops remain much lower than the highest-end desktops (especially ""workstation class"" machines with two processor sockets), and ""leading-edge"" features usually appear first in desktops and only then, as the underlying technology matures, are adapted to laptops.
For Internet browsing and typical office applications, where the computer spends the majority of its time waiting for the next user input, even relatively low-end laptops (such as Netbooks) can be fast enough for some users.[61] Most higher-end laptops are sufficiently powerful for high-resolution movie playback, some 3D gaming and video editing and encoding. However, laptop processors can be disadvantaged when dealing with a higher-end database, maths, engineering, financial software, virtualization, etc. This is because laptops use the mobile versions of processors to conserve power, and these lag behind desktop chips when it comes to performance. Some manufacturers work around this performance problem by using desktop CPUs for laptops.[62]

Upgradeability[edit]
The upgradeability of laptops is very limited compared to thoroughly standardized desktops. In general, hard drives and memory can be upgraded easily. Optical drives and internal expansion cards may be upgraded if they follow an industry standard, but all other internal components, including the motherboard, CPU, and graphics, are not always intended to be upgradeable. Intel, Asus, Compal, Quanta and some other laptop manufacturers have created the Common Building Block standard for laptop parts to address some of the inefficiencies caused by the lack of standards. The reasons for limited upgradeability are both technical and economic. There is no industry-wide standard form factor for laptops; each major laptop manufacturer pursues its own proprietary design and construction, with the result that laptops are difficult to upgrade and have high repair costs. Moreover, starting with 2013 models, laptops have become increasingly integrated (soldered) with the motherboard for most of its components (CPU, SSD, RAM, keyboard, etc.) to reduce size and upgradeability prospects. Devices such as sound cards, network adapters, hard and optical drives, and numerous other peripherals are available, but these upgrades usually impair the laptop's portability, because they add cables and boxes to the setup and often have to be disconnected and reconnected when the laptop is on the move.[citation needed]

Ergonomics and health effects[edit]
Wrists[edit]
 Laptop cooler (silver) under laptop (white), preventing heating of lap and improving laptop airflow
Prolonged use of laptops can cause repetitive strain injury because of their small, flat keyboard and trackpad pointing devices.[63] Usage of separate, external ergonomic keyboards and pointing devices is recommended to prevent injury when working for long periods of time; they can be connected to a laptop easily by USB, Bluetooth or via a docking station. Some health standards require ergonomic keyboards at workplaces.

Neck and spine[edit]
A laptop's integrated screen often requires users to lean over for a better view, which can cause neck or spinal injuries. A larger and higher-quality external screen can be connected to almost any laptop to alleviate this and to provide additional screen space for more productive work. Another solution is to use a computer stand.

Possible effect on fertility[edit]
A study by State University of New York researchers found that heat generated from laptops can increase the temperature of the lap of male users when balancing the computer on their lap, potentially putting sperm count at risk. The study, which included roughly two dozen men between the ages of 21 and 35, found that the sitting position required to balance a laptop can increase scrotum temperature by as much as 2.1 °C (4 °F). However, further research is needed to determine whether this directly affects male sterility.[64] A later 2010 study of 29 males published in Fertility and Sterility found that men who kept their laptops on their laps experienced scrotal hyperthermia (overheating) in which their scrotal temperatures increased by up to 2.0 °C (4 °F). The resulting heat increase, which could not be offset by a laptop cushion, may increase male infertility.[65][66][67][68][69]
A common practical solution to this problem is to place the laptop on a table or desk or to use a book or pillow between the body and the laptop.[citation needed] Another solution is to obtain a cooling unit for the laptop. These are usually USB powered and consist of a hard thin plastic case housing one, two, or three cooling fans – with the entire assembly designed to sit under the laptop in question – which results in the laptop remaining cool to the touch, and greatly reduces laptop heat buildup.

Thighs[edit]
Heat generated from using a laptop on the lap can also cause skin discoloration on the thighs known as ""toasted skin syndrome"".[70][71][72][73]

Durability[edit]
 A clogged heat sink on a laptop after 2.5 years of use
Laptops are less durable than desktops/PCs. However, the durability of the laptop depends on the user if proper maintenance is done then the laptop can work longer. Laptop keyboard with its keys (except the space bar) removed, revealing crumbs, pet hair, and other detritus to be cleaned away.
Equipment wear[edit]
Because of their portability, laptops are subject to more wear and physical damage than desktops. Components such as screen hinges, latches, power jacks, and power cords deteriorate gradually from ordinary use and may have to be replaced. A liquid spill onto the keyboard, a rather minor mishap with a desktop system (given that a basic keyboard costs about US$20), can damage the internals of a laptop and destroy the computer, result in a costly repair or entire replacement of laptops. One study found that a laptop is three times more likely to break during the first year of use than a desktop.[74] To maintain a laptop, it is recommended to clean it every three months for dirt, debris, dust, and food particles. Most cleaning kits consist of a lint-free or microfiber cloth for the LCD screen and keyboard, compressed air for getting dust out of the cooling fan, and a cleaning solution. Harsh chemicals such as bleach should not be used to clean a laptop, as they can damage it.[75]

Heating and cooling[edit]
Laptops rely on extremely compact cooling systems involving a fan and heat sink that can fail from blockage caused by accumulated airborne dust and debris. Most laptops do not have any type of removable dust collection filter over the air intake for these cooling systems, resulting in a system that gradually conducts more heat and noise as the years pass. In some cases, the laptop starts to overheat even at idle load levels. This dust is usually stuck inside where the fan and heat sink meet, where it can not be removed by a casual cleaning and vacuuming. Most of the time, compressed air can dislodge the dust and debris but may not entirely remove it. After the device is turned on, the loose debris is reaccumulated into the cooling system by the fans. Complete disassembly is usually required to clean the laptop entirely. However, preventative maintenance such as regular cleaning of the heat sink via compressed air can prevent dust build-up on the heat sink. Many laptops are difficult to disassemble by the average user and contain components that are sensitive to electrostatic discharge (ESD).

Battery life[edit]
Battery life is limited because the capacity drops with time, eventually requiring replacement after as little as a year. A new battery typically stores enough energy to run the laptop for three to five hours, depending on usage, configuration, and power management settings. Yet, as it ages, the battery's energy storage will dissipate progressively until it lasts only a few minutes. The battery is often easily replaceable and a higher capacity model may be obtained for longer charging and discharging time. Some laptops (specifically ultrabooks) do not have the usual removable battery and have to be brought to the service center of their manufacturer or a third-party laptop service center to have their battery replaced. Replacement batteries can also be expensive.

Security and privacy[edit]
Main article: Laptop theft
Because they are valuable, commonly used, portable, and easy to hide in a backpack or other type of travel bag, laptops are often stolen. Every day, over 1,600 laptops go missing from U.S. airports.[76] The cost of stolen business or personal data, and of the resulting problems (identity theft, credit card fraud, breach of privacy), can be many times the value of the stolen laptop itself. Consequently, the physical protection of laptops and the safeguarding of data contained on them are both of great importance. Most laptops have a Kensington security slot, which can be used to tether them to a desk or other immovable object with a security cable and lock. In addition, modern operating systems and third-party software offer disk encryption functionality, which renders the data on the laptop's hard drive unreadable without a key or a passphrase. As of 2015, some laptops also have additional security elements added, including eye recognition software and fingerprint scanning components.[77]
Software such as LoJack for Laptops, Laptop Cop, and GadgetTrack have been engineered to help people locate and recover their stolen laptops in the event of theft. Setting one's laptop with a password on its firmware (protection against going to firmware setup or booting), internal HDD/SSD (protection against accessing it and loading an operating system on it afterward), and every user account of the operating system are additional security measures that a user should do.[78][79] Fewer than 5% of lost or stolen laptops are recovered by the companies that own them,[80] however, that number may decrease due to a variety of companies and software solutions specializing in laptop recovery. In the 2010s, the common availability of webcams on laptops raised privacy concerns. In Robbins v. Lower Merion School District (Eastern District of Pennsylvania 2010), school-issued laptops loaded with special software enabled staff from two high schools to take secret webcam shots of students at home, via their students' laptops.[81][82][83]

Sales[edit]
Manufacturers[edit]
Major laptop brands
Acer / Gateway / eMachines / Packard Bell: TravelMate, Extensa, Ferrari and Aspire; Easynote; Chromebook

Apple: MacBook Air and MacBook Pro

Asus: TUF, ROG, Pro and ProArt, ZenBook, VivoBook, ExpertBook

Clevo

Dell: Alienware, Inspiron, Latitude, Precision,  Vostro and XPS

Dynabook (former Toshiba): Portege, Tecra, Satellite, Qosmio, Libretto

Falcon Northwest: DRX, TLX, I / O

Fujitsu: Lifebook, Celsius

Gigabyte: AORUS

HCL (India): ME Laptop, ME Netbook, Leaptop and MiLeap

Hewlett-Packard: Pavilion, Envy, ProBook, EliteBook, ZBook

Huawei: Matebook

Lenovo: ThinkPad, ThinkBook, IdeaPad, Yoga, Legion and the Essential B and G Series

LG: Xnote, Gram

Medion: Akoya (OEM version of MSI Wind)

MSI: E, C, P, G, V, A, X, U series, Modern, Prestige and Wind Netbook

Panasonic: Toughbook, Satellite, Let's Note (Japan only)

Samsung: Sens: N, P, Q, R and X series; Chromebook, ATIV Book

TG Sambo (Korea): Averatec, Averatec Buddy

Vaio (former Sony)

Xiaomi: Mi, Mi Gaming and Mi RedmiBook laptops
vte
Main article: List of laptop brands and manufacturers
Further information: Market share of personal computer vendors
There are many laptop brands and manufacturers. Several major brands that offer notebooks in various classes are listed in the adjacent box.
The major brands usually offer good service and support, including well-executed documentation and driver downloads that remain available for many years after a particular laptop model is no longer produced. Capitalizing on service, support, and brand image, laptops from major brands are more expensive than laptops by smaller brands and ODMs. Some brands specialize in a particular class of laptops, such as gaming laptops (Alienware), high-performance laptops (HP Envy), netbooks (EeePC) and laptops for children (OLPC).
Many brands, including the major ones, do not design and do not manufacture their laptops. Instead, a small number of Original Design Manufacturers (ODMs) design new models of laptops, and the brands choose the models to be included in their lineup. In 2006, 7 major ODMs manufactured 7 of every 10 laptops in the world, with the largest one (Quanta Computer) having 30% of the world market share.[84] Therefore, identical models are available both from a major label and from a low-profile ODM in-house brand.

Market share[edit]
Battery-powered portable computers had just 2% worldwide market share in 1986.[85] However, laptops have become increasingly popular, both for business and personal use.[86] Around 109 million notebook PCs shipped worldwide in 2007, a growth of 33% compared to 2006.[87] In 2008 it was estimated that 145.9 million notebooks were sold, and that the number would grow in 2009 to 177.7 million.[88] The third quarter of 2008 was the first time when worldwide notebook PC shipments exceeded desktops, with 38.6 million units versus 38.5 million units.[86][89][90][91]
May 2005 was the first time notebooks outsold desktops in the US over the course of a full month; at the time notebooks sold for an average of $1,131 while desktops sold for an average of $696.[92] When looking at operating systems, for Microsoft Windows laptops the average selling price (ASP) showed a decline in 2008/2009, possibly due to low-cost netbooks, drawing an average US$689 at U.S. retail stores in August 2008. In 2009, ASP had further fallen to $602 by January and to $560 in February. While Windows machines ASP fell $129 in these seven months, Apple macOS laptop ASP declined just $12 from $1,524 to $1,512.[93]

Disposal[edit]
The list of materials that go into a laptop computer is long, and many of the substances used, such as beryllium (used in beryllium-copper alloy contacts in some connectors and sockets), lead (used in lead-tin solder), chromium, and mercury (used in CCFL LCD backlights) compounds, are toxic or carcinogenic to humans. Although these toxins are relatively harmless when the laptop is in use, concerns that discarded laptops cause a serious health risk and toxic environmental damage, were so strong, that the Waste Electrical and Electronic Equipment Directive (WEEE Directive) in Europe specified that all laptop computers must be recycled by law. Similarly, the U.S. Environmental Protection Agency (EPA) has outlawed landfill dumping or the incinerating of discarded laptop computers.
Most laptop computers begin the recycling process with a method known as Demanufacturing, this involves the physical separation of the components of the laptop.[94] These components are then either grouped into materials (e.g. plastic, metal and glass) for recycling or more complex items that require more advanced materials separation (e.g.) circuit boards, hard drives and batteries.
Corporate laptop recycling can require an additional process known as data destruction. The data destruction process ensures that all information or data that has been stored on a laptop hard drive can never be retrieved again. Below is an overview of some of the data protection and environmental laws and regulations applicable for laptop recycling data destruction:

Data Protection Act 1998 (DPA)
EU Privacy Directive (Due 2016)
Financial Conduct Authority
Sarbanes-Oxley Act
PCI-DSS Data Security Standard
Waste, Electronic & Electrical Equipment Directive (WEEE)
Basel Convention
Bank Secrecy Act (BSA)
FACTA Sarbanes-Oxley Act
FDA Security Regulations (21 C.F.R. part 11)
Gramm-Leach-Bliley Act (GLBA)
HIPAA (Health Insurance Portability and Accountability Act)
NIST SP 800–53
Add NIST SP 800–171
Identity Theft and Assumption Deterrence Act
Patriot Act of 2002
PCI Data Security Standard
US Safe Harbor Provisions
Various state laws
JAN 6/3
Gramm-leach-Bliley Act
DCID
Extreme use[edit]
See also: International Space Station § Communications and computers
 ISS laptops in the US lab
The ruggedized Grid Compass computer was used since the early days of the Space Shuttle program. The first commercial laptop used in space was a Macintosh portable in 1991 aboard Space Shuttle mission STS-43.[95][96][97] Apple and other laptop computers continue to be flown aboard crewed spaceflights, though the only long-duration flight certified computer for the International Space Station is the ThinkPad.[98] As of 2011, over 100 ThinkPads were aboard the ISS. Laptops used aboard the International Space Station and other spaceflights are generally the same ones that can be purchased by the general public but needed modifications are made to allow them to be used safely and effectively in a weightless environment such as updating the cooling systems to function without relying on hot air rising and accommodation for the lower cabin air pressure.[99] Laptops operating in harsh usage environments and conditions, such as strong vibrations, extreme temperatures, and wet or dusty conditions differ from those used in space in that they are custom designed for the task and do not use commercial off-the-shelf hardware.

See also[edit]

List of computer size categories
List of laptop brands and manufacturers
Netbook
Smartbook
Chromebook
Ultrabook
Smartphone
Subscriber Identity Module
Mobile broadband
Mobile Internet device (MID)
Personal digital assistant
VIA OpenBook
Tethering
XJACK
Open-source computer hardware
Novena
Portal laptop computer
Mobile modem
Stereoscopy glasses

Notes[edit]


^ Unconfirmed if this exists in most recent models of laptops.

^ Almost all laptops contain a Wi-Fi interface; broadband cellular devices are available widely as extension cards and USB devices, and also as internal cards in select models.


References[edit]


^ Beal, Vangie (September 1996). ""What is Laptop Computer? Webopedia Definition"". www.webopedia.com.

^ ""Laptop vs desktop: which should you buy?"". TechRadar. Retrieved 1 August 2021.

^ a b Naik, Abhijit. ""Notebook Vs. Laptop"". Buzzle.com. Archived from the original on 14 February 2015. Retrieved 23 September 2014.

^ ""U.S. Commercial Channel Computing Device Sales Set to End 2013 with Double-Digit Growth, According to NPD"". NPD Group. Archived from the original on 9 August 2019. Retrieved 23 September 2014.

^ John W. Maxwell (2006). Tracing the Dynabook: A Study of Technocultural Transformations (PDF) (PhD). University of British Columbia. Archived from the original (PDF) on 24 January 2007. Retrieved 17 October 2008.

^ Alan C. Kay (August 1972). A Personal Computer for Children of All Ages (PDF). Proceedings of the ACM National Conference. Boston: Xerox Palo Alto Research Center. Retrieved 17 October 2008.

^ ""IBM Archives: IBM Personal Computer"". www.ibm.com. 23 January 2003. Retrieved 16 May 2021.

^ ""IBM Personal Computer"". IBM Inc. 23 January 2003.

^ ""IBM 5100 computer"". oldcomputers.net. Retrieved 6 July 2009.

^ ""Epson SX-20 Promotional Brochure"" (PDF). Epson America, Inc. 1987. Retrieved 2 November 2008.

^ a b c d ""HC-20-Computer Museum"". museum.ipsj.or.jp.

^ a b ""portable computer system small"". google.com.

^ Epson HX-20, Old Computers

^ Michael R. Peres, The Focal Encyclopedia of Photography, page 306, Taylor & Francis

^ Osborne 1, Old Computers

^ ""Tandy/Radio Shack model 100 portable computer"". oldcomputers.net. Retrieved 6 July 2009.

^ ""Hewlett-Packard model 85"". oldcomputers.net. Retrieved 6 July 2009.

^ Sharp PC-5000 Archived 4 April 2019 at the Wayback Machine, Old Computers

^ a b Bob Armstrong, http://cosy.com/language/cosyhard/cosyhard.htm

^ ""Gavilian SC computer"". oldcomputers.net. Retrieved 7 July 2009.

^ Japanese PCs (1984) (13:13), Computer Chronicles

^ ""Milestones:Toshiba T1100, a Pioneering Contribution to the Development of Laptop PC, 1985 – Engineering and Technology History Wiki"". ethw.org. 3 November 2021.

^ ""Linus Write-Top"". Retrieved 18 October 2008.

^ ""IBM PS/2 CL57SX | Laptop Pics"". Retrieved 5 December 2020.

^ DigitalTrends

^ [https://www.cnet.com/reviews/samsung-700g7c-review/ Cnet

^ ""Laptop Buying Guide"". Cnet. Retrieved 7 November 2008.

^ ""Best 2-in-1 PCs in 2020 for when you need a laptop and tablet in one"". CNet. Retrieved 1 April 2020.

^ ""What Is a Hybrid Laptop? | Advantages & Buying Guide | Lenovo US"". www.lenovo.com. Retrieved 1 April 2020.

^ ""Rugged Laptop: Choices, Pointers & Specs of Buying Rugged Laptops"". Linux-on-laptops.com. Retrieved 27 November 2008.

^ Dé specialist voor smartphone, tablet en laptop reparaties (4 January 2013). ""Laptop reparatie"". Smartrepair Den Bosch, Nijmegen, Tilburg, Almere en Utrecht (in Dutch). www.smart-repair.nl. Retrieved 30 March 2017.

^ ""Microsoft Surface Pro 3 Teardown"". iFixit.com. 23 June 2014. Retrieved 1 October 2014.

^ ""Common Building Blocks Platform"" (PDF). Intel. Archived from the original (PDF) on 16 May 2006. Retrieved 20 November 2013.

^ a b Catherine Roseberry. ""What Makes Laptops Work – The Laptop Motherboard"". About.com. Retrieved 15 November 2008.

^ a b ""Configuration 3-Displays FAQ"". Intel.com. Retrieved 16 September 2014.

^ ""Dell refreshes Alienware M17x, Dell XPS 17 with 120Hz 3D HD screens, Sandy Bridge CPUs"". Engadget. 6 January 2011. Retrieved 28 April 2021.

^ ""Samsung 700G7A GAMER"". www.pocket-lint.com. 11 April 2012. Retrieved 28 April 2021.

^ btarunr (26 November 2012). ""Is Haswell the Last Interchangeable Intel Client Processor?"". TechPowerUp. Retrieved 30 May 2021.

^ Samsung Review Samsung Series 7 Gamer 700G7A Notebook – Florian Glaser (translated by Liala Stieglitz) – November 24th, 2011

^ Edwards, Benj (17 January 2012). ""Evolution of the Solid-State Drive"". PCWorld.com. Archived from the original on 1 October 2012. Retrieved 1 October 2014.

^ Most keyboards are not illuminated. Some models of laptops feature an illuminated keyboard.

^ ""Apple — MacBook — Tech Specs"". apple.com. Retrieved 2 April 2015.

^ ""Chromebook Pixel"". google.com. Retrieved 2 April 2015.

^ Gabriel Torres (25 November 2004). ""Innovations in Notebook Expansion"". Hardware Secrets, LLC. Archived from the original on 28 April 2005. Retrieved 15 November 2008.

^ ""HP EliteBook 6930p Notebook PC specifications – HP Products and Services Products"". HP. 25 May 2009. Archived from the original on 1 June 2012. Retrieved 17 June 2013.

^ Hacking Dell Laptops To Use Off-Brand Chargers

^ PC Mag – What is USB-C - an explainer – April 28, 2021

^ a b Woods, Dough. ""Getting rid of the ICT suite"". Blog. Archived from the original on 6 October 2010.

^ Wilce, Hilary (1 December 2000). ""Welcome to Lapland"". TES Magazine. Archived from the original on 26 May 2015. Retrieved 5 June 2012.

^ Clarke, Gavin. ""The SOLAR-POWERED Ubuntu laptop"". The Register. Retrieved 7 August 2013.

^ Archived at Ghostarchive and the Wayback Machine: OLPC XO laptop powered by a solar panel. 9 January 2012. Retrieved 23 October 2012 – via YouTube.

^ Elizabeth Woyke (18 April 2012). ""A Look at OLPC's XO 3.0 Tablet's Solar And Kinetic Chargers"". Forbes. Retrieved 23 October 2012.

^ ""One Laptop per Child (OLPC): Frequently Asked Questions"". Laptop.org. Archived from the original on 18 March 2017. Retrieved 23 October 2012.

^ ""Samsung's Solar Powered Laptop Will Be First Sun Powered Laptop Sold in US | Inhabitat – Sustainable Design Innovation, Eco Architecture, Green Building"". Inhabitat. 21 June 2011. Retrieved 23 October 2012.

^ RAM upgrade – Melanie Pinola – March 26th, 2017 – LaptopMag

^ DigitalTrends –  MacBook Pro battery replacement: Everything you need to know  –  March 15, 2021 – Tyler Lacoma

^ ""Should I buy a laptop or desktop?"". IT Division – the University of Wisconsin. 19 March 2008. Retrieved 27 November 2008.

^ Josh Fischman (7 August 2008). ""Faster Wi-Fi Predicted for Colleges"". The Chronicle of Higher Education. Retrieved 27 November 2008.

^ A sample line of UPS devices and on-battery power: ""Back-UPS RS"". APC. Retrieved 27 November 2008.

^ In a comparison between laptops and desktops of equal cost, the desktop's System Benchmark Score was twice that of the laptop. ""What to Buy, a Notebook or Desktop PC?"". Tom's Hardware. 11 June 2008. Retrieved 28 November 2008.[needs update]

^ For example, a review of the MSI Wind Netbook says that ""The device is rarely sluggish in general use. It renders Web pages quickly, launches most applications without becoming too bogged down and generally doesn't feel like it's a budget laptop."" Reid, Rory (7 July 2008). ""MSI Wind Review"". CNET Australia. Archived from the original on 5 December 2008. Retrieved 28 November 2008.

^ ""Rock delivers BD / Core i7-equipped Xtreme 790 and Xtreme 840 gaming laptops"". Engadget.

^ Toub, Allegra (23 May 2017). ""Take It Easy on Those Keyboards"". Backlight Resumes. Retrieved 23 May 2017.

^ ""You Asked: Can Using a Laptop Make You Infertile?"". Time. Retrieved 9 November 2021.

^ Yefim Sheynkin; Robert Welliver; Andrew Winer; Farshid Hajimirzaee; Hongshik Ahn; Kyewon Lee (8 November 2010). ""Protection from scrotal hyperthermia in laptop computer users"". Fertility and Sterility. 95 (2): 647–651. doi:10.1016/j.fertnstert.2010.10.013. PMID 21055743.

^ Yin, Sara (8 November 2010). ""Study: Laptop Pads Don't Prevent Male Infertility"". PC Magazine. Retrieved 8 November 2010.

^ ""Men, your laptop may be roasting your testicles"". The Independent. 8 November 2010. Retrieved 8 November 2010.

^ Caulfield, Philip (7 November 2010). ""Study finds men who place laptop computer on lap put testicles at risk of overheating, infertility"". Daily News. Archived from the original on 10 November 2010. Retrieved 8 November 2010.

^ Joelving, Frederik (8 November 2010). ""Is your laptop cooking your testicles?"". Reuters. Retrieved 8 November 2010.

^ Levinbook, WS.; Mallet J; Grant-Kels JM (October 2007). ""Laptop computer—associated erythema ab igne"". Cutis. Quadrant HealthCom. 80 (4): 319–20. PMID 18038695.

^ Diaz, Jesus (7 October 2010). ""What Is Toasted Skin Syndrome?"". Gizmodo. Retrieved 8 November 2010.

^ Hendrick, Bill (4 October 2010). ""Laptop Risk: 'Toasted Skin Syndrome'"". WebMD. Retrieved 8 November 2010.

^ Tanner, Lindsey (10 April 2010). ""Laptops lead to 'toasted skin syndrome'"". Associated Press. Retrieved 8 November 2010.

^ ""Gartner: Notebook PCs still prone to hardware failure"". IDG News Service / ITWorld. 27 June 2006. Retrieved 27 November 2008.

^ Geier, Eric (6 August 2012). ""Zen and the Art of Laptop Maintenance"". PC World. Retrieved 25 January 2014.

^ [1], Ponemon Institute, Airport Insecurity: The Case of Lost Laptops, June 2008

^ ""Secure File Sharing"". Biometric Devices and Laptop Security. Laptop Security Pro. Retrieved 7 February 2015.

^ Hoffman, Chris. ""How to Secure Your Computer With a BIOS or UEFI Password"". How-To Geek.

^ Hoffman, Chris. ""Hard Disk Passwords Explained: Should You Set One to Secure Your Files?"". How-To Geek.

^ [2] Archived 6 June 2013 at the Wayback Machine, Ponemon Institute, The Billion Dollar Lost Laptop Problem, September 2010

^ Holmes, Kristin E. (31 August 2010). ""Lower Merion School District ordered to pay plaintiff's lawyer $260,000"". The Philadelphia Inquirer. Retrieved 20 September 2010.

^ ""Main Line Media News"". Main Line Media News. 18 September 2010. Archived from the original on 5 March 2016. Retrieved 20 September 2010.

^ ""A lawyer in the Lower Merion webcam case wants to be paid now"", Philly.com Archived 1 September 2010 at the Wayback Machine

^ ""Identical Laptops, Different Prices: Don't Be Fooled by Branding"". Info-Tech Research Group. 10 October 2006. Retrieved 11 November 2011.

^ ""Lap-top computers gain stature as power grows"". Daily News of Los Angeles (CA). 12 April 1987. Retrieved 1 November 2008.

^ a b ""The Falling Costs of Mobile Computing"". Falling Costs of Mobile Computing Drive Corporate Adoption. Computer Economics, Inc. December 2005. Retrieved 1 November 2008.

^ Worldwide notebook shipments grow 33% on year in 2007, says IDC, 31 January 2008, Yen Ting Chen, DigiTimes, retrieved at 12 September 2011

^ Analysis: Did Intel underestimate netbook success?, Accessed at 10 January 2009

^ Notebook PC Shipments Exceed Desktops for First Time in Q3, isuppli.com, accessed at 13 January 2009

^ Randall Stross (18 April 2008). ""The PC Doesn't Have to Be an Anchor"". The New York Times. Retrieved 20 April 2009.

^ ""Intel: laptop/desktop crossover coming sooner than expected"". The Register, UK. Archived from the original on 7 October 2008. Retrieved 10 October 2008.

^ Michael Singer. ""PC milestone—notebooks outsell desktops"". 2005.

^ ""Netbooks Are Destroying the Laptop Market and Microsoft Needs to Act Now"". eWEEK. 16 April 2009.

^ Recycling, Newtech. ""Laptop Disposal Ewaste Recycling and IT asset disposition (ITAD)"". www.newtechrecycling.com. Retrieved 11 June 2018.

^ ""Macintosh Portable: Used in Space Shuttle"". Support.apple.com. Retrieved 23 October 2012.

^ Linzmayer, Owen W. (2004). Apple confidential 2.0 : the definitive history of the world's most colorful company ([Rev. 2. ed.]. ed.). San Francisco, Calif.: No Starch Press. ISBN 1-59327-010-0.

^ ""This Week in Apple History – August 22–31: ""Welcome, IBM. Seriously"", Too Late to License"". The Mac Observer. 31 October 2004. Retrieved 23 October 2012.

^ IBM Archives: IBM ThinkPads in space Archived 20 July 2011 at the Wayback Machine

^ ""2001: A Space Laptop – SpaceRef – Your Space Reference"". www.spaceref.com.


 Media related to Laptops at Wikimedia Commons

vteComputer sizesClasses of computersmicrocomputer,personalcomputerstatic
Appliances:
Smart speaker
Smart TV
Interactive kiosk
Arcade cabinet
Home console
Microconsole
thin client/computer terminal
Computers:
By use:
Home
Home server
Workstation
Personal supercomputer
By size:
Portable
Small form factor
Nettop
Plug
Desktop
Deskside
All-in-One
Tabletop
mobilelaptop
Desktop replacement
2-in-1
Subnotebook
Netbook
Smartbook
Ultrabook
Ultra-mobile PC
tablet
Ultra-mobile PC
2-in-1
Phablet
Tabletop
handheld(informationappliance)
Handheld PC
Palmtop PC
Pocket computer
Personal digital assistant
Electronic organizer
Mobile phone
Feature phone
Smartphone
Rugged Phone
Phablet
Portable media player
E-reader
Handheld game console
Portable/Mobile data terminal
calculator
Scientific
Programmable
Graphing
wearable
Digital wristwatch
Calculator watch
Smartwatch
Sportwatch
Smartband
Smartglasses
Smart ring
midrange
Minicomputer
Supermini
large
Super
Mainframe
Minisuper
others
Microcontroller
Nanocomputer
Single-board computer
Smartdust
Wireless sensor network
Server (size independent)

 Category
 Portal

vteElectronicsBranches
Analog electronics
Digital electronics
Electronic instrumentation
Electronics engineering
Microelectronics
Optoelectronics
Power electronics
Printed electronics
Semiconductor
Schematic capture
Thermal management
Advanced topics
Atomtronics
Bioelectronics
Failure of electronic components
Flexible electronics
Low-power electronics
Molecular electronics
Nanoelectronics
Organic electronics
Photonics
Piezotronics
Quantum electronics
Spintronics
Electronic equipment
Air conditioner
Central heating
Clothes dryer
Computer/Notebook
Camera
Dishwasher
Freezer
Home robot
Home cinema
Home theater PC
Information technologies
Cooker
Microwave oven
Mobile phone
Networking hardware
Portable media player
Radio
Refrigerator
Robotic vacuum cleaner
Tablet
Telephone
Television
Water heater
Video game console
Washing machine
Applications
Audio electronics
Automotive electronics
Avionics
Control system
Data acquisition
e-book
e-health
Electronics industry
Electronic warfare
Embedded system
Home appliance
Home automation
Integrated circuit
Home appliance
Consumer electronics
Major appliance
Small appliance
Microwave technology
Military electronics
Multimedia
Nuclear electronics
Open hardware
Radar and Radionavigation
Radio electronics
Terahertz technology
Video hardware
Wired and Wireless Communications

Authority control: National libraries 
France (data)
Germany
Israel
United States





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Laptop&oldid=1086276049""
Categories: LaptopsClasses of computersJapanese inventionsMobile computersOffice equipmentPersonal computers1980s neologismsHidden categories: Webarchive template wayback linksCS1 Dutch-language sources (nl)Wikipedia articles in need of updating from April 2021All Wikipedia articles in need of updatingArticles with short descriptionShort description is different from WikidataWikipedia articles in need of updating from November 2021Use dmy dates from October 2017Use American English from March 2021All Wikipedia articles written in American EnglishArticles needing additional references from November 2015All articles needing additional referencesArticles needing additional references from July 2016Articles using small message boxesAll articles with unsourced statementsArticles with unsourced statements from July 2016Articles with unsourced statements from February 2020Articles with unsourced statements from February 2012Commons category link is on WikidataArticles with BNF identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiers

"
6,Mobile,"




Mobile

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
For information on using Wikipedia on mobile devices, see Help:Mobile access.



Look up Mobile, mobile, or -mobile in Wiktionary, the free dictionary.

Mobile may refer to:

Contents

1 Places
2 Arts, entertainment, and media

2.1 Music

2.1.1 Groups and labels
2.1.2 Other uses in music


2.2 Other uses in arts, entertainment, and media


3 Military and law enforcement
4 Science
5 Technology
6 See also



Places[edit]
Mobile, Alabama, a U.S. port city
Mobile County, Alabama
Mobile, Arizona, a small town near Phoenix, U.S.
Mobile, Newfoundland and Labrador
Arts, entertainment, and media[edit]
Music[edit]
Groups and labels[edit]
Mobile (band), a Canadian rock band
The Mobiles, a 1980s British band
Other uses in music[edit]
Mobile (album), a 1999 album by Brazilian Paulinho Moska
""Mobile"" (song), a 2003 song by Avril Lavigne from Let Go
""Mobile"", a song by Gentle Giant from the album Free Hand
Other uses in arts, entertainment, and media[edit]
Mobile (sculpture), a kinetic sculpture constructed to take advantage of the principle of equilibrium
Mobile (TV series), a British ITV drama
""Mobile"", a short story by J. G. Ballard, later renamed ""Venus Smiles""
Mobile, a feature of the game GunBound
Mobile Magazine, a publication on portable electronics
Military and law enforcement[edit]
Garde Mobile, historic French military unit
Mobile Brigade Corps (Brimob), the special police force of Indonesia
Mobile forces, especially:
Motorized infantry
Mounted infantry
Operation Mobile, Canadian Forces operations in the 2011 military intervention in Libya
Science[edit]
Motility
Motion (physics), the ability to move or be moved
Technology[edit]
Mobile computing, a generic term describing one's ability to use technology in mobile environments
Mobile device, such as a smartphone, tablet, or computer designed for mobile computing
Mobile game, a video game played on a mobile phone, smartphone, PDA or handheld computer
Mobile network operator, a company which provides mobile phone network access and services
Mobile operating system, the various underlying systems to power and run phones
Mobile phone, a portable telephone that can make and receive calls
Mobile radio, wireless communications systems and devices which are based on radio frequencies
Mobile rig
Mobile station, user equipment and software needed for communication with a wireless telephone network
Mobile Web, the World Wide Web as accessed from mobile devices using Mobile Web Browser
Mobile TV, TV services viewed via a mobile device
See also[edit]
Mabila, a Native American people of Alabama
Mauvilla (disambiguation)
Mavilla (disambiguation)
Mobil, a major oil company
Mobile station (disambiguation)
Mobility (disambiguation)
Topics referred to by the same term

 This disambiguation page lists  articles associated with the title Mobile.If an internal link led you here, you may wish to change the link to point directly to the intended article. 





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Mobile&oldid=1053533393""
Categories: Disambiguation pagesPlace name disambiguation pagesHidden categories: Disambiguation pages with short descriptionsShort description is different from WikidataAll article disambiguation pagesAll disambiguation pages

"
7,Computer,"





Computer

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Automatic general-purpose device for performing arithmetic or logical operations
For other uses, see Computer (disambiguation).


Computers and computing devices from different eras – clockwise from top left:Early vacuum tube computer (ENIAC)Mainframe computer (IBM System 360)Desktop computer (IBM ThinkCentre S50 with monitor)Supercomputer (IBM Summit)Video game console (Nintendo GameCube)Smartphone (LYF Water 2)
A computer is a digital electronic machine that can be programmed to carry out sequences of arithmetic or logical operations (computation) automatically. Modern computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. A computer system is a ""complete"" computer that includes the hardware, operating system (main software), and peripheral equipment needed and used for ""full"" operation. This term may also refer to a group of computers that are linked and function together, such as a computer network or computer cluster.
A broad range of industrial and consumer products use computers as control systems. Simple special-purpose devices like microwave ovens and remote controls are included, as are factory devices like industrial robots and computer-aided design, as well as general-purpose devices like personal computers and mobile devices like smartphones. Computers power the Internet, which links billions of other computers and users.
Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries.
Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.

Contents

1 Etymology
2 History

2.1 Pre-20th century
2.2 First computer
2.3 Analog computers
2.4 Digital computers

2.4.1 Electromechanical
2.4.2 Vacuum tubes and digital electronic circuits


2.5 Modern computers

2.5.1 Concept of modern computer
2.5.2 Stored programs
2.5.3 Transistors
2.5.4 Integrated circuits


2.6 Mobile computers


3 Types

3.1 By architecture
3.2 By size, form-factor and purpose


4 Hardware

4.1 History of computing hardware
4.2 Other hardware topics
4.3 Input devices
4.4 Output devices
4.5 Control unit
4.6 Central processing unit (CPU)
4.7 Arithmetic logic unit (ALU)
4.8 Memory
4.9 Input/output (I/O)
4.10 Multitasking
4.11 Multiprocessing


5 Software

5.1 Languages
5.2 Programs

5.2.1 Stored program architecture
5.2.2 Machine code
5.2.3 Programming language

5.2.3.1 Low-level languages
5.2.3.2 High-level languages


5.2.4 Program design
5.2.5 Bugs




6 Networking and the Internet
7 Unconventional computers
8 Future

8.1 Computer architecture paradigms
8.2 Artificial intelligence


9 Professions and organizations
10 See also
11 Notes
12 References
13 Sources
14 External links



Etymology
 A human computer, with microscope and calculator, 1952
According to the Oxford English Dictionary, the first known use of computer was in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: ""I haue  [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number."" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2]
The Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an ""agent noun from compute (v.)"". The Online Etymology Dictionary states that the use of the term to mean ""'calculating machine' (of any type) is from 1897.""  The Online Etymology Dictionary indicates that the ""modern use"" of the term, to mean 'programmable digital electronic computer' dates from ""1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine"".[3]

History
Main articles: History of computing hardware and History of computing
Pre-20th century
 The Ishango bone, a bone tool dating back to prehistoric Africa.
Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[a][4] The use of counting rods is one example.

 The Chinese suanpan (算盘). The number represented on this abacus is 6,302,715,408.
The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[5]

 The Antikythera mechanism, dating back to ancient Greece circa 150–100 BC, is an early analog computing device.
The Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c. 100 BC. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.[7]
Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD.
The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.
The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.

 A slide rule.
The slide rule was invented around 1620–1630 by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.
In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically ""programmed"" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[15]
In 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which, through a system of pulleys and cylinders and over, could predict the perpetual calendar for every year from AD 0 (that is, 1 BC) to AD 4000, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.
The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[16] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

First computer
 A portion of Babbage's Difference engine.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the ""father of the computer"",[17] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[18][19]
The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

Analog computers
Main article: Analog computer
 Sir William Thomson's third tide-predicting machine design, 1879–81
During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[20] The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.[16]
The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems).

Digital computers
Electromechanical
By 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.

 Replica of Konrad Zuse's Z3, the first fully automatic, digital (electromechanical) computer.
Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[21]
In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[22][23] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz.[24] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating-point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[25] The Z3 was not itself a universal computer but could be extended to be Turing complete.[26][27]
Zuse's next computer, the Z4, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the ETH Zurich.[28] The computer was manufactured by Zuse's own company, Zuse KG [de], which was founded in 1941 as the first company with the sole purpose of developing computers.[28]

Vacuum tubes and digital electronic circuits

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[20] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,[29] the first ""automatic electronic digital computer"".[30] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[31]

 Colossus, the first electronic digital programmable computing device, was used to break German ciphers during World War II. It is seen here in use at Bletchley Park in 1943.
During World War II, the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women.[32][33] To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[31] He spent eleven months from early February 1943 designing and building the first Colossus.[34] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[35] and attacked its first message on 5 February.[31]
Colossus was the world's first electronic digital programmable computer.[20] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.[36][37]

 ENIAC was the first electronic, Turing-complete device, and performed ballistics trajectory calculations for the United States Army.
The ENIAC[38] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a ""program"" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the ""ENIAC girls"".[39][40]
It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[41]

Modern computers
Concept of modern computer
The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper,[42] On Computable Numbers. Turing proposed a simple device that he called ""Universal Computing machine"" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[43] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

Stored programs
Main article: Stored-program computer
 A section of the Manchester Baby, the first electronic stored-program computer
Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.[31] With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report ""Proposed Electronic Calculator"" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.[20]
The Manchester Baby was the world's first stored-program computer. It was built at the University of Manchester in England by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[44] It was designed as a testbed for the Williams tube, the first random-access digital storage device.[45] Although the computer was considered ""small and primitive"" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[46] As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. Grace Hopper was the first person to develop a compiler for programming language.[2]
The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[47] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[48] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951[49] and ran the world's first regular routine office computer job.

Transistors
Main articles: Transistor and History of the transistor
Further information: Transistor computer and MOSFET
 Bipolar junction transistor (BJT)
The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947, which was followed by Shockley's bipolar junction transistor in 1948.[50][51] From 1955 onwards, transistors replaced vacuum tubes in computer designs, giving rise to the ""second generation"" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.[52]
At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[53] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[54] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[54][55]

 MOSFET (MOS transistor), showing gate (G), body (B), source (S) and drain (D) terminals. The gate is separated from the body by an insulating layer (pink).
The metal–oxide–silicon field-effect transistor (MOSFET), also known as the MOS transistor, was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[56] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[52] With its high scalability,[57] and much lower power consumption and higher density than bipolar junction transistors,[58] the MOSFET made it possible to build high-density integrated circuits.[59][60] In addition to data processing, it also enabled the practical use of MOS transistors as memory cell storage elements, leading to the development of MOS semiconductor memory, which replaced earlier magnetic-core memory in computers. The MOSFET led to the microcomputer revolution,[61] and became the driving force behind the computer revolution.[62][63] The MOSFET is the most widely used transistor in computers,[64][65] and is the fundamental building block of digital electronics.[66]

Integrated circuits
Main articles: Integrated circuit and Invention of the integrated circuit
Further information: Planar process and Microprocessor
The next great advance in computing power came with the advent of the integrated circuit (IC).
The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[67]
The first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[68] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[69] In his patent application of 6 February 1959, Kilby described his new device as ""a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated"".[70][71] However, Kilby's invention was a hybrid integrated circuit (hybrid IC), rather than a monolithic integrated circuit (IC) chip.[72] Kilby's IC had external wire connections, which made it difficult to mass-produce.[73]
Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[74] Noyce's invention was the first true monolithic IC chip.[75][73] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. Noyce's monolithic IC was fabricated using the planar process, developed by his colleague Jean Hoerni in early 1959. In turn, the planar process was based on Mohamed M. Atalla's work on semiconductor surface passivation by silicon dioxide in the late 1950s.[76][77][78]
Modern monolithic ICs are predominantly MOS (metal-oxide-semiconductor) integrated circuits, built from MOSFETs (MOS transistors).[79] The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962.[80] General Microelectronics later introduced the first commercial MOS IC in 1964,[81] developed by Robert Norman.[80] Following the development of the self-aligned gate (silicon-gate) MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967, the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968.[82] The MOSFET has since become the most critical device component in modern ICs.[83]
The development of the MOS integrated circuit led to the invention of the microprocessor,[84][85] and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term ""microprocessor"", it is largely undisputed that the first single-chip microprocessor was the Intel 4004,[86] designed and realized by Federico Faggin with his silicon-gate MOS IC technology,[84] along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel.[b][88] In the early 1970s, MOS IC technology enabled the integration of more than 10,000 transistors on a single chip.[60]
System on a Chip (SoCs) are complete computers on a microchip (or chip) the size of a coin.[89] They may or may not have integrated RAM and flash memory. If not integrated, the RAM is usually placed directly above (known as Package on package) or below (on the opposite side of the circuit board) the SoC, and the flash memory is usually placed right next to the SoC, this all done to improve data transfer speeds, as the data signals don't have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (Such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power.

Mobile computers
The first mobile computers were heavy and ran from mains power. The 50 lb (23 kg) IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s.[90] The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s.
These smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market.[91] These are powered by System on a Chip (SoCs), which are complete computers on a microchip the size of a coin.[89]

Types
See also: Classes of computers
Computers can be classified in a number of different ways, including:

By architecture
Analog computer
Digital computer
Hybrid computer
Harvard architecture
Von Neumann architecture
Complex instruction set computer
Reduced instruction set computer
By size, form-factor and purpose
Supercomputer
Mainframe computer
Minicomputer (term no longer used)
Server
Rackmount server
Blade server
Tower server
Personal computer
Workstation
Microcomputer (term no longer used)
Home computer
Desktop computer
Tower desktop
Slimline desktop
Multimedia computer (non-linear editing system computers, video editing PCs and the like)
Gaming computer
All-in-one PC
Nettop (Small form factor PCs, Mini PCs)
Home theater PC
Keyboard computer
Portable computer
Thin client
Internet appliance
Laptop
Desktop replacement computer
Gaming laptop
Rugged laptop
2-in-1 PC
Ultrabook
Chromebook
Subnotebook
Netbook
Mobile computers:
Tablet computer
Smartphone
Ultra-mobile PC
Pocket PC
Palmtop PC
Handheld PC
Wearable computer
Smartwatch
Smartglasses
Single-board computer
Plug computer
Stick PC
Programmable logic controller
Computer-on-module
System on module
System in a package
System-on-chip (Also known as an Application Processor or AP if it lacks circuitry such as radio circuitry)
Microcontroller
Hardware
Main articles: Computer hardware, Personal computer hardware, Central processing unit, and Microprocessor Video demonstrating the standard components of a ""slimline"" computer
The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and ""mice"" input devices are all hardware.

History of computing hardware
Main article: History of computing hardware


First generation(mechanical/electromechanical)
Calculators
Pascal's calculator, Arithmometer, Difference engine, Quevedo's analytical machines


Programmable devices
Jacquard loom, Analytical engine, IBM ASCC/Harvard Mark I, Harvard Mark II, IBM SSEC, Z1, Z2, Z3


Second generation(vacuum tubes)
Calculators
Atanasoff–Berry Computer, IBM 604, UNIVAC 60, UNIVAC 120


Programmable devices
Colossus, ENIAC, Manchester Baby, EDSAC, Manchester Mark 1, Ferranti Pegasus, Ferranti Mercury, CSIRAC, EDVAC, UNIVAC I, IBM 701, IBM 702, IBM 650, Z22


Third generation(discrete transistors and SSI, MSI, LSI integrated circuits)
Mainframes
IBM 7090, IBM 7080, IBM System/360, BUNCH


Minicomputer
HP 2116A, IBM System/32, IBM System/36, LINC, PDP-8, PDP-11


Desktop Computer
HP 9100


Fourth generation(VLSI integrated circuits)
Minicomputer
VAX, IBM AS/400


4-bit microcomputer
Intel 4004, Intel 4040


8-bit microcomputer
Intel 8008, Intel 8080, Motorola 6800, Motorola 6809, MOS Technology 6502, Zilog Z80


16-bit microcomputer
Intel 8088, Zilog Z8000, WDC 65816/65802


32-bit microcomputer
Intel 80386, Pentium, Motorola 68000, ARM


64-bit microcomputer[c]
Alpha, MIPS, PA-RISC, PowerPC, SPARC, x86-64, ARMv8-A


Embedded computer
Intel 8048, Intel 8051


Personal computer
Desktop computer, Home computer, Laptop computer, Personal digital assistant (PDA), Portable computer, Tablet PC, Wearable computer


Theoretical/experimental
Quantum computer



Chemical computer



DNA computing



Optical computer



Spintronics-based computer



Wetware/Organic computer


Other hardware topics


Peripheral device (input/output)
Input
Mouse, keyboard, joystick, image scanner, webcam, graphics tablet, microphone


Output
Monitor, printer, loudspeaker


Both
Floppy disk drive, hard disk drive, optical disc drive, teleprinter


Computer buses
Short range
RS-232, SCSI, PCI, USB


Long range (computer networking)
Ethernet, ATM, FDDI

A general-purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a ""1"", and when off it represents a ""0"" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.

Input devices
When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:

Computer keyboard
Digital camera
Digital video
Graphics tablet
Image scanner
Joystick
Microphone
Mouse
Overlay keyboard
Real-time clock
Trackball
Touchscreen
Light pen
Output devices
The means through which computer gives output are known as output devices. Some examples of output devices are:

Computer monitor
Printer
PC speaker
Projector
Sound card
Video card
Control unit
Main articles: CPU design and Control unit
 Diagram showing how a particular MIPS architecture instruction would be decoded by the control system
The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[d] Control systems in advanced computers may change the order of execution of some instructions to improve performance.
A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[e]
The control system's function is as follows— this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:

Read the code for the next instruction from the cell indicated by the program counter.
Decode the numerical code for the instruction into a set of commands or signals for each of the other systems.
Increment the program counter so it points to the next instruction.
Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code.
Provide the necessary data to an ALU or register.
If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation.
Write the result from the ALU back to a memory location or to a register or perhaps an output device.
Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as ""jumps"" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).
The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.

Central processing unit (CPU)
Main articles: Central processing unit and Microprocessor
The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor.

Arithmetic logic unit (ALU)
Main article: Arithmetic logic unit
The ALU is capable of performing two classes of operations: arithmetic and logic.[92] The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can operate only on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return Boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (""is 64 greater than 65?""). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing Boolean logic.
Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[93] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.

Memory
Main articles: Computer memory and Computer data storage
 Magnetic-core memory (using magnetic cores) was the computer memory of choice in the 1960s, until it was replaced by semiconductor memory (using MOS memory cells).
A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered ""address"" and can store a single number. The computer can be instructed to ""put the number 123 into the cell numbered 1357"" or to ""add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595."" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.
In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.
The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.
Computer main memory comes in two principal varieties:

random-access memory or RAM
read-only memory or ROM
RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[f]
In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.

Input/output (I/O)
Main article: Input/output
 Hard disk drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside world.[95] Devices that provide input or output to the computer are called peripherals.[96] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.
I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.

Multitasking
Main article: Computer multitasking
While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[97] One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running ""at the same time"". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed ""time-sharing"" since each program is allocated a ""slice"" of time in turn.[98]
Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a ""time slice"" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.

Multiprocessing
Main article: Multiprocessing
 Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed in only large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.
Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers.[g] They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called ""embarrassingly parallel"" tasks.

Software
Main article: Software
Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called ""firmware"".



Operating system /System Software

Unix and BSD
UNIX System V, IBM AIX, HP-UX, Solaris (SunOS), IRIX, List of BSD operating systems


Linux
List of Linux distributions, Comparison of Linux distributions


Microsoft Windows
Windows 95, Windows 98, Windows NT, Windows 2000, Windows ME, Windows XP, Windows Vista, Windows 7, Windows 8, Windows 8.1, Windows 10, Windows 11


DOS
86-DOS (QDOS), IBM PC DOS, MS-DOS, DR-DOS, FreeDOS


Macintosh operating systems
Classic Mac OS, macOS (previously OS X and Mac OS X)


Embedded and real-time
List of embedded operating systems


Experimental
Amoeba, Oberon–AOS, Bluebottle, A2, Plan 9 from Bell Labs


Library
Multimedia
DirectX, OpenGL, OpenAL, Vulkan (API)


Programming library
C standard library, Standard Template Library


Data
Protocol
TCP/IP, Kermit, FTP, HTTP, SMTP


File format
HTML, XML, JPEG, MPEG, PNG


User interface
Graphical user interface (WIMP)
Microsoft Windows, GNOME, KDE, QNX Photon, CDE, GEM, Aqua


Text-based user interface
Command-line interface, Text user interface


Application Software

Office suite
Word processing, Desktop publishing, Presentation program, Database management system, Scheduling & Time management, Spreadsheet, Accounting software


Internet Access
Browser, Email client, Web server, Mail transfer agent, Instant messaging


Design and manufacturing
Computer-aided design, Computer-aided manufacturing, Plant management, Robotic manufacturing, Supply chain management


Graphics
Raster graphics editor, Vector graphics editor, 3D modeler, Animation editor, 3D computer graphics, Video editing, Image processing


Audio
Digital audio editor, Audio playback, Mixing, Audio synthesis, Computer music


Software engineering
Compiler, Assembler, Interpreter, Debugger, Text editor, Integrated development environment, Software performance analysis, Revision control, Software configuration management


Educational
Edutainment, Educational game, Serious game, Flight simulator


Games
Strategy, Arcade, Puzzle, Simulation, First-person shooter, Platform, Massively multiplayer, Interactive fiction


Misc
Artificial intelligence, Antivirus software, Malware scanner, Installer/Package management systems, File manager

Languages
There are thousands of different programming languages—some intended for general purpose, others useful for only highly specialized applications.


Programming languages


Lists of programming languages
Timeline of programming languages, List of programming languages by category, Generational list of programming languages, List of programming languages, Non-English-based programming languages


Commonly used assembly languages
ARM, MIPS, x86


Commonly used high-level programming languages
Ada, BASIC, C, C++, C#, COBOL, Fortran, PL/I, REXX, Java, Lisp, Pascal, Object Pascal


Commonly used scripting languages
Bourne script, JavaScript, Python, Ruby, PHP, Perl

Programs
The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.

Stored program architecture
Main articles: Computer program and Computer programming
 Replica of the Manchester Baby, the world's first electronic stored-program computer, at the Museum of Science and Industry in Manchester, England
This section applies to most common RAM machine–based computers.
In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called ""jump"" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that ""remembers"" the location it jumped from and another instruction to return to the instruction following that jump instruction.
Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.
Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:


  begin:
  addi $8, $0, 0           # initialize sum to 0
  addi $9, $0, 1           # set first number to add = 1
  loop:
  slti $10, $9, 1000       # check if the number is less than 1000
  beq $10, $0, finish      # if odd number is greater than n then exit
  add $8, $8, $9           # update sum
  addi $9, $9, 1           # get next number
  j loop                   # repeat the summing process
  finish:
  add $2, $8, $0           # put sum in output register

Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.

Machine code
In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.
While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[h] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.

 A 1970s punched card containing one line from a Fortran program. The card reads: ""Z(1) = Y + W(1)"" and is labeled ""PROJ039"" for identification purposes.
Programming language
Main article: Programming language
Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.

Low-level languages
Main article: Low-level programming language
Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) are generally unique to the particular architecture of a computer's central processing unit (CPU). For instance, an ARM architecture CPU (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.[i] Historically a significant number of other cpu architectures were created and saw extensive use, notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80.

High-level languages
Main article: High-level programming language
Although considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually ""compiled"" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[j] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.

Program design
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2012) (Learn how and when to remove this template message)
Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.
The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.

Bugs
Main article: Software bug
 The actual first computer bug, a moth found trapped on a relay of the Harvard Mark II computer
Errors in computer programs are called ""bugs"". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to ""hang"", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash.[100] Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[k] Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term ""bugs"" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[101]

Networking and the Internet
Main articles: Computer networking and Internet
 Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.[102] In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[103] The technologies that made the Arpanet possible spread and evolved.
In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. ""Wireless"" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.


Unconventional computers
Main article: Human computer
See also: Harvard Computers
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word ""computer"" is synonymous with a personal electronic computer,[l] the modern definition of a computer is literally: ""A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.""[104] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]

Future
There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.

Computer architecture paradigms
There are many types of computer architectures:

Quantum computer vs. Chemical computer
Scalar processor vs. Vector processor
Non-Uniform Memory Access (NUMA) computers
Register machine vs. Stack machine
Harvard architecture vs. von Neumann architecture
Cellular architecture
Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[105] Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.

Artificial intelligence
A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule-based systems and pattern recognition systems. Rule-based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern-based systems use data about a problem to generate conclusions. Examples of pattern-based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.

Professions and organizations
As the use of computers has spread throughout society, there are an increasing number of careers involving computers.


Computer-related professions


Hardware-related
Electrical engineering, Electronic engineering, Computer engineering, Telecommunications engineering, Optical engineering, Nanoengineering


Software-related
Computer science, Computer engineering, Desktop publishing, Human–computer interaction, Information technology, Information systems, Computational science, Software engineering, Video game industry, Web design

The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.


Organizations


Standards groups
ANSI, IEC, IEEE, IETF, ISO, W3C


Professional societies
ACM, AIS, IET, IFIP, BCS


Free/open source software groups
Free Software Foundation, Mozilla Foundation, Apache Software Foundation

See also

Glossary of computers
Computability theory
Computer security
Glossary of computer hardware terms
History of computer science
List of computer term etymologies
List of fictional computers
List of pioneers in computer science
Pulse computation
TOP500 (list of most powerful computers)
Unconventional computing

Notes


^ According to Schmandt-Besserat 1981, these clay containers contained tokens, the total of which were the count of objects being transferred. The containers thus served as something of a bill of lading or an accounts book. In order to avoid breaking open the containers, first, clay impressions of the tokens were placed on the outside of the containers, for the count; the shapes of the impressions were abstracted into stylized marks; finally, the abstract marks were systematically used as numerals; these numerals were finally formalized as numbers.Eventually the marks on the outside of the containers were all that were needed to convey the count, and the clay containers evolved into clay tablets with marks for the count. Schmandt-Besserat 1999 estimates it took 4000 years.

^ The Intel 4004 (1971) die was 12 mm2, composed of 2300 transistors; by comparison, the Pentium Pro was 306 mm2, composed of 5.5 million transistors.[87]

^ Most major 64-bit instruction set architectures are extensions of earlier designs. All of the architectures listed in this table, except for Alpha, existed in 32-bit forms before their 64-bit incarnations were introduced.

^ The control unit's role in interpreting instructions has varied somewhat in the past. Although the control unit is solely responsible for instruction interpretation in most modern computers, this is not always the case. Some computers have instructions that are partially interpreted by the control unit with further interpretation performed by another device. For example, EDVAC, one of the earliest stored-program computers, used a central control unit that interpreted only four instructions. All of the arithmetic-related instructions were passed on to its arithmetic unit and further decoded there.

^ Instructions often occupy more than one memory address, therefore the program counter usually increases by the number of memory locations required to store one instruction.

^ Flash memory also may only be rewritten a limited number of times before wearing out, making it less useful for heavy random access usage.[94] 

^ However, it is also very common to construct supercomputers out of many pieces of cheap commodity hardware; usually individual computers connected by networks. These so-called computer clusters can often provide supercomputer performance at a much lower cost than customized designs. While custom architectures are still used for most of the most powerful supercomputers, there has been a proliferation of cluster computers in recent years.[99] 

^ Even some later computers were commonly programmed directly in machine code. Some minicomputers like the DEC PDP-8 could be programmed directly from a panel of switches. However, this method was usually used only as part of the booting process. Most modern computers boot entirely automatically by reading a boot program from some non-volatile memory.

^ However, there is sometimes some form of machine language compatibility between different computers. An x86-64 compatible microprocessor like the AMD Athlon 64 is able to run most of the same programs that an Intel Core 2 microprocessor can, as well as programs designed for earlier microprocessors like the Intel Pentiums and Intel 80486. This contrasts with very early commercial computers, which were often one-of-a-kind and totally incompatible with other computers.

^ High level languages are also often interpreted rather than compiled. Interpreted languages are translated into machine code on the fly, while running, by another program called an interpreter.

^ It is not universally true that bugs are solely due to programmer oversight. Computer hardware may fail or may itself have a fundamental problem that produces unexpected results in certain situations. For instance, the Pentium FDIV bug caused some Intel microprocessors in the early 1990s to produce inaccurate results for certain floating point division operations. This was caused by a flaw in the microprocessor design and resulted in a partial recall of the affected devices.

^ According to the Shorter Oxford English Dictionary (6th ed, 2007), the word computer dates back to the mid 17th century, when it referred to ""A person who makes calculations; specifically a person employed for this in an observatory etc.""


References


^ Evans 2018, p. 23.

^ a b Smith 2013, p. 6.

^ ""computer (n.)"". Online Etymology Dictionary.

^ Robson, Eleanor (2008), Mathematics in Ancient Iraq, p. 5, ISBN 978-0-691-09182-2: calculi were in use in Iraq for primitive accounting systems as early as 3200–3000 BCE, with commodity-specific counting representation systems. Balanced accounting was in use by 3000–2350 BCE, and a sexagesimal number system was in use 2350–2000 BCE.

^ Flegg, Graham. (1989). Numbers through the ages. Houndmills, Basingstoke, Hampshire: Macmillan Education. ISBN 0-333-49130-0. OCLC 24660570.

^ The Antikythera Mechanism Research Project Archived 28 April 2008 at the Wayback Machine, The Antikythera Mechanism Research Project. Retrieved 1 July 2007.

^ Marchant, Jo (1 November 2006). ""In search of lost time"". Nature. 444 (7119): 534–538. Bibcode:2006Natur.444..534M. doi:10.1038/444534a. PMID 17136067. S2CID 4305761. Retrieved 12 March 2022.

^ G. Wiet, V. Elisseeff, P. Wolff, J. Naudu (1975). History of Mankind, Vol 3: The Great medieval Civilisations, p. 649. George Allen & Unwin Ltd, UNESCO.

^ Fuat Sezgin ""Catalogue of the Exhibition of the Institute for the History of Arabic-Islamic Science (at the Johann Wolfgang Goethe University"", Frankfurt, Germany) Frankfurt Book Fair 2004, pp. 35 & 38.

^ Charette, François (2006). ""Archaeology: High tech from Ancient Greece"". Nature. 444 (7119): 551–552. Bibcode:2006Natur.444..551C. doi:10.1038/444551a. PMID 17136077. S2CID 33513516.

^ Bedini, Silvio A.; Maddison, Francis R. (1966). ""Mechanical Universe: The Astrarium of Giovanni de' Dondi"". Transactions of the American Philosophical Society. 56 (5): 1–69. doi:10.2307/1006002. JSTOR 1006002.

^ Price, Derek de S. (1984). ""A History of Calculating Machines"". IEEE Micro. 4 (1): 22–52. doi:10.1109/MM.1984.291305.

^ Őren, Tuncer (2001). ""Advances in Computer and Information Sciences: From Abacus to Holonic Agents"" (PDF). Turk J Elec Engin. 9 (1): 63–70.

^ Donald Routledge Hill (1985). ""Al-Biruni's mechanical calendar"", Annals of Science 42, pp. 139–163.

^ ""The Writer Automaton, Switzerland"". chonday.com. 11 July 2013.

^ a b Ray Girvan, ""The revealed grace of the mechanism: computing after Babbage"" Archived 3 November 2012 at the Wayback Machine, Scientific Computing World, May/June 2003

^ Halacy, Daniel Stephen (1970). Charles Babbage, Father of the Computer. Crowell-Collier Press. ISBN 978-0-02-741370-0.

^ ""Babbage"". Online stuff. Science Museum. 19 January 2007. Retrieved 1 August 2012.

^ ""Let's build Babbage's ultimate mechanical computer"". opinion. New Scientist. 23 December 2010. Retrieved 1 August 2012.

^ a b c d The Modern History of Computing. Stanford Encyclopedia of Philosophy. 2017.

^ Zuse, Horst. ""Part 4: Konrad Zuse's Z1 and Z3 Computers"". The Life and Work of Konrad Zuse. EPE Online. Archived from the original on 1 June 2008. Retrieved 17 June 2008.

^ Zuse, Konrad (2010) [1984], The Computer – My Life Translated by McKenna, Patricia and Ross, J. Andrew from: Der Computer, mein Lebenswerk (1984), Berlin/Heidelberg: Springer-Verlag, ISBN 978-3-642-08151-4

^ Salz Trautman, Peggy (20 April 1994). ""A Computer Pioneer Rediscovered, 50 Years On"". The New York Times.

^ Zuse, Konrad (1993). Der Computer. Mein Lebenswerk (in German) (3rd ed.). Berlin: Springer-Verlag. p. 55. ISBN 978-3-540-56292-4.

^ ""Crash! The Story of IT: Zuse"". Archived from the original on 18 September 2016. Retrieved 1 June 2016.

^ Rojas, R. (1998). ""How to make Zuse's Z3 a universal computer"". IEEE Annals of the History of Computing. 20 (3): 51–54. doi:10.1109/85.707574. S2CID 14606587.

^ Rojas, Raúl. ""How to Make Zuse's Z3 a Universal Computer"" (PDF).

^ a b O'Regan, Gerard (2010). A Brief History of Computing. Springer Nature. p. 65. ISBN 9783030665999.

^ ""notice"". Des Moines Register. 15 January 1941.

^ Arthur W. Burks (1989). The First Electronic Computer. ISBN 0472081047.

^ a b c d Copeland, Jack (2006), Colossus: The Secrets of Bletchley Park's Codebreaking Computers, Oxford: Oxford University Press, pp. 101–115, ISBN 978-0-19-284055-4

^ Miller, Joe (10 November 2014). ""The woman who cracked Enigma cyphers"". BBC News. Retrieved 14 October 2018.

^ Bearne, Suzanne (24 July 2018). ""Meet the female codebreakers of Bletchley Park"". The Guardian. Retrieved 14 October 2018.

^ ""Bletchley's code-cracking Colossus"", BBC News, 2 February 2010, retrieved 19 October 2012

^ ""Colossus – The Rebuild Story"". The National Museum of Computing. Archived from the original on 18 April 2015. Retrieved 7 January 2014.

^ Randell, Brian; Fensom, Harry; Milne, Frank A. (15 March 1995), ""Obituary: Allen Coombs"", The Independent, retrieved 18 October 2012

^ Fensom, Jim (8 November 2010), ""Harry Fensom obituary"", The Guardian, retrieved 17 October 2012

^ John Presper Eckert Jr. and John W. Mauchly, Electronic Numerical Integrator and Computer, United States Patent Office, US Patent 3,120,606, filed 26 June 1947, issued 4 February 1964, and invalidated 19 October 1973 after court ruling on Honeywell v. Sperry Rand.

^ Evans 2018, p. 39.

^ Light 1999, p. 459.

^ ""Generations of Computer"". techiwarehouse.com. Archived from the original on 2 July 2015. Retrieved 7 January 2014.

^ Turing, A. M. (1937). ""On Computable Numbers, with an Application to the Entscheidungsproblem"". Proceedings of the London Mathematical Society. 2. 42 (1): 230–265. doi:10.1112/plms/s2-42.1.230.

^ Copeland, Jack (2004), The Essential Turing, p. 22: ""von Neumann ... firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing—insofar as not anticipated by Babbage, Lovelace and others."" Letter by Stanley Frankel to Brian Randell, 1972.

^ Enticknap, Nicholas (Summer 1998), ""Computing's Golden Jubilee"", Resurrection (20), ISSN 0958-7403, archived from the original on 9 January 2012, retrieved 19 April 2008

^ ""Early computers at Manchester University"", Resurrection, 1 (4), Summer 1992, ISSN 0958-7403, archived from the original on 28 August 2017, retrieved 7 July 2010

^ Early Electronic Computers (1946–51), University of Manchester, archived from the original on 5 January 2009, retrieved 16 November 2008

^ Napper, R. B. E., Introduction to the Mark 1, The University of Manchester, archived from the original on 26 October 2008, retrieved 4 November 2008

^ Computer Conservation Society, Our Computer Heritage Pilot Study: Deliveries of Ferranti Mark I and Mark I Star computers, archived from the original on 11 December 2016, retrieved 9 January 2010

^ Lavington, Simon. ""A brief history of British computers: the first 25 years (1948–1973)"". British Computer Society. Retrieved 10 January 2010.

^ Lee, Thomas H. (2003). The Design of CMOS Radio-Frequency Integrated Circuits (PDF). Cambridge University Press. ISBN 9781139643771. Archived from the original (PDF) on 9 December 2019. Retrieved 31 July 2019.

^ Puers, Robert; Baldi, Livio; Voorde, Marcel Van de; Nooten, Sebastiaan E. van (2017). Nanoelectronics: Materials, Devices, Applications, 2 Volumes. John Wiley & Sons. p. 14. ISBN 9783527340538.

^ a b Moskowitz, Sanford L. (2016). Advanced Materials Innovation: Managing Global Technology in the 21st century. John Wiley & Sons. pp. 165–167. ISBN 9780470508923.

^ Lavington 1998, pp. 34–35.

^ a b Cooke-Yarborough, E. H. (June 1998), ""Some early transistor applications in the UK"", Engineering Science & Education Journal, 7 (3): 100–106, doi:10.1049/esej:19980301, ISSN 0963-7346, retrieved 7 June 2009 (subscription required)

^ Cooke-Yarborough, E.H. (1957). Introduction to Transistor Circuits. Edinburgh: Oliver and Boyd. p. 139.

^ ""1960: Metal Oxide Semiconductor (MOS) Transistor Demonstrated"". The Silicon Engine: A Timeline of Semiconductors in Computers. Computer History Museum. Retrieved 31 August 2019.

^ Motoyoshi, M. (2009). ""Through-Silicon Via (TSV)"". Proceedings of the IEEE. 97 (1): 43–48. doi:10.1109/JPROC.2008.2007462. ISSN 0018-9219. S2CID 29105721.

^ ""Transistors Keep Moore's Law Alive"". EETimes. 12 December 2018. Retrieved 18 July 2019.

^ ""Who Invented the Transistor?"". Computer History Museum. 4 December 2013. Retrieved 20 July 2019.

^ a b Hittinger, William C. (1973). ""Metal-Oxide-Semiconductor Technology"". Scientific American. 229 (2): 48–59. Bibcode:1973SciAm.229b..48H. doi:10.1038/scientificamerican0873-48. ISSN 0036-8733. JSTOR 24923169.

^ Malmstadt, Howard V.; Enke, Christie G.; Crouch, Stanley R. (1994). Making the Right Connections: Microcomputers and Electronic Instrumentation. American Chemical Society. p. 389. ISBN 9780841228610. The relative simplicity and low power requirements of MOSFETs have fostered today's microcomputer revolution.

^ Fossum, Jerry G.; Trivedi, Vishal P. (2013). Fundamentals of Ultra-Thin-Body MOSFETs and FinFETs. Cambridge University Press. p. vii. ISBN 9781107434493.

^ ""Remarks by Director Iancu at the 2019 International Intellectual Property Conference"". United States Patent and Trademark Office. 10 June 2019. Archived from the original on 17 December 2019. Retrieved 20 July 2019.

^ ""Dawon Kahng"". National Inventors Hall of Fame. Retrieved 27 June 2019.

^ ""Martin Atalla in Inventors Hall of Fame, 2009"". Retrieved 21 June 2013.

^ ""Triumph of the MOS Transistor"". YouTube. Computer History Museum. 6 August 2010. Archived from the original on 18 August 2021. Retrieved 21 July 2019.

^ ""The Hapless Tale of Geoffrey Dummer"" Archived 11 May 2013 at the Wayback Machine, (n.d.), (HTML), Electronic Product News, accessed 8 July 2008.

^ Kilby, Jack (2000), Nobel lecture (PDF), Stockholm: Nobel Foundation, retrieved 15 May 2008

^ The Chip that Jack Built, (c. 2008), (HTML), Texas Instruments, Retrieved 29 May 2008.

^ Jack S. Kilby, Miniaturized Electronic Circuits, United States Patent Office, US Patent 3,138,743, filed 6 February 1959, issued 23 June 1964.

^ Winston, Brian (1998). Media Technology and Society: A History : From the Telegraph to the Internet. Routledge. p. 221. ISBN 978-0-415-14230-4.

^ Saxena, Arjun N. (2009). Invention of Integrated Circuits: Untold Important Facts. World Scientific. p. 140. ISBN 9789812814456.

^ a b ""Integrated circuits"". NASA. Retrieved 13 August 2019.

^ Robert Noyce's Unitary circuit, US patent 2981877, ""Semiconductor device-and-lead structure"", issued 1961-04-25,  assigned to Fairchild Semiconductor Corporation 

^ ""1959: Practical Monolithic Integrated Circuit Concept Patented"". Computer History Museum. Retrieved 13 August 2019.

^ Lojek, Bo (2007). History of Semiconductor Engineering. Springer Science & Business Media. p. 120. ISBN 9783540342588.

^ Bassett, Ross Knox (2007). To the Digital Age: Research Labs, Start-up Companies, and the Rise of MOS Technology. Johns Hopkins University Press. p. 46. ISBN 9780801886393.

^ Huff, Howard R.; Tsuya, H.; Gösele, U. (1998). Silicon Materials Science and Technology: Proceedings of the Eighth International Symposium on Silicon Materials Science and Technology. Electrochemical Society. pp. 181–182. ISBN 9781566771931.

^ Kuo, Yue (1 January 2013). ""Thin Film Transistor Technology—Past, Present, and Future"" (PDF). The Electrochemical Society Interface. 22 (1): 55–61. Bibcode:2013ECSIn..22a..55K. doi:10.1149/2.F06131if. ISSN 1064-8208.

^ a b ""Tortoise of Transistors Wins the Race - CHM Revolution"". Computer History Museum. Retrieved 22 July 2019.

^ ""1964 – First Commercial MOS IC Introduced"". Computer History Museum.

^ ""1968: Silicon Gate Technology Developed for ICs"". Computer History Museum. Retrieved 22 July 2019.

^ Kuo, Yue (1 January 2013). ""Thin Film Transistor Technology—Past, Present, and Future"" (PDF). The Electrochemical Society Interface. 22 (1): 55–61. Bibcode:2013ECSIn..22a..55K. doi:10.1149/2.F06131if. ISSN 1064-8208.

^ a b ""1971: Microprocessor Integrates CPU Function onto a Single Chip"". Computer History Museum. Retrieved 22 July 2019.

^ Colinge, Jean-Pierre; Greer, James C. (2016). Nanowire Transistors: Physics of Devices and Materials in One Dimension. Cambridge University Press. p. 2. ISBN 9781107052406.

^ Intel's First Microprocessor—the Intel 4004, Intel Corp., November 1971, archived from the original on 13 May 2008, retrieved 17 May 2008

^ Patterson, David; Hennessy, John (1998). Computer Organization and Design. San Francisco: Morgan Kaufmann. pp. 27–39. ISBN 978-1-55860-428-5.

^ Federico Faggin, The Making of the First Microprocessor, IEEE Solid-State Circuits Magazine, Winter 2009, IEEE Xplore

^ a b ""7 dazzling smartphone improvements with Qualcomm's Snapdragon 835 chip"". 3 January 2017.

^ Chartier, David (23 December 2008). ""Global notebook shipments finally overtake desktops"". Ars Technica.

^ IDC (25 July 2013). ""Growth Accelerates in the Worldwide Mobile Phone and Smartphone Markets in the Second Quarter, According to IDC"". Archived from the original on 26 June 2014.

^ David J. Eck (2000). The Most Complex Machine: A Survey of Computers and Computing. A K Peters, Ltd. p. 54. ISBN 978-1-56881-128-4.

^ Erricos John Kontoghiorghes (2006). Handbook of Parallel Computing and Statistics. CRC Press. p. 45. ISBN 978-0-8247-4067-2.

^ Verma & Mielke 1988.

^ Donald Eadie (1968). Introduction to the Basic Computer. Prentice-Hall. p. 12.

^ Arpad Barna; Dan I. Porat (1976). Introduction to Microcomputers and the Microprocessors. Wiley. p. 85. ISBN 978-0-471-05051-3.

^ Jerry Peek; Grace Todino; John Strang (2002). Learning the UNIX Operating System: A Concise Guide for the New User. O'Reilly. p. 130. ISBN 978-0-596-00261-9.

^ Gillian M. Davis (2002). Noise Reduction in Speech Applications. CRC Press. p. 111. ISBN 978-0-8493-0949-6.

^ TOP500 2006, p. [page needed].

^ ""Why do computers crash?"". Scientific American. Retrieved 3 March 2022.

^ Taylor, Alexander L., III (16 April 1984). ""The Wizard Inside the Machine"". Time. Archived from the original on 16 March 2007. Retrieved 17 February 2007.

^ Agatha C. Hughes (2000). Systems, Experts, and Computers. MIT Press. p. 161. ISBN 978-0-262-08285-3. The experience of SAGE helped make possible the first truly large-scale commercial real-time network: the SABRE computerized airline reservations system

^ Leiner, Barry M.; Cerf, Vinton G.; Clark, David D.; Kahn, Robert E.; Kleinrock, Leonard; Lynch, Daniel C.; Postel, Jon; Roberts, Larry G.; Wolf, Stephen (1999). ""A Brief History of the Internet"". arXiv:cs/9901011.

^ ""Definition of computer"". Thefreedictionary.com. Retrieved 29 January 2012.

^ II, Joseph D. Dumas (2005). Computer Architecture: Fundamentals and Principles of Computer Design. CRC Press. p. 340. ISBN 9780849327490.


Sources

Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.
Fuegi, J.; Francis, J. (2003). ""Lovelace & Babbage and the creation of the 1843 'notes'"". IEEE Annals of the History of Computing. 25 (4): 16. doi:10.1109/MAHC.2003.1253887. S2CID 40077111.
Kempf, Karl (1961). Historical Monograph: Electronic Computers Within the Ordnance Corps. Aberdeen Proving Ground (United States Army).
Phillips, Tony (2000). ""The Antikythera Mechanism I"". American Mathematical Society. Retrieved 5 April 2006.
Shannon, Claude Elwood (1940). A symbolic analysis of relay and switching circuits (Thesis). Massachusetts Institute of Technology. hdl:1721.1/11173.
Digital Equipment Corporation (1972). PDP-11/40 Processor Handbook (PDF). Maynard, MA: Digital Equipment Corporation.
Swade, Doron D. (February 1993). ""Redeeming Charles Babbage's Mechanical Computer"". Scientific American. 268 (2): 86–91. Bibcode:1993SciAm.268b..86S. doi:10.1038/scientificamerican0293-86. JSTOR 24941379.
Meuer, Hans; Strohmaier, Erich; Simon, Horst; Dongarra, Jack (13 November 2006). ""Architectures Share Over Time"". TOP500. Archived from the original on 20 February 2007. Retrieved 27 November 2006.
Lavington, Simon (1998). A History of Manchester Computers (2nd ed.). Swindon: The British Computer Society. ISBN 978-0-902505-01-8.
Light, Jennifer S. (1999). ""When Computers Were Women"". Technology and Culture. 40 (3): 455–483. doi:10.1353/tech.1999.0128. JSTOR 25147356. S2CID 108407884.
Schmandt-Besserat, Denise (1999). ""Tokens: The Cognitive Significance"". Documenta Praehistorica. XXVI. Archived from the original on 30 January 2012.
Schmandt-Besserat, Denise (1981). ""Decipherment of the earliest tablets"". Science. 211 (4479): 283–285. Bibcode:1981Sci...211..283S. doi:10.1126/science.211.4479.283. PMID 17748027.
Stokes, Jon (2007). Inside the Machine: An Illustrated Introduction to Microprocessors and Computer Architecture. San Francisco: No Starch Press. ISBN 978-1-59327-104-6.
Zuse, Konrad (1993). The Computer – My life. Berlin: Pringler-Verlag. ISBN 978-0-387-56453-1.
Felt, Dorr E. (1916). Mechanical arithmetic, or The history of the counting machine. Chicago: Washington Institute.
Ifrah, Georges (2001). The Universal History of Computing: From the Abacus to the Quantum Computer. New York: John Wiley & Sons. ISBN 978-0-471-39671-0.
Berkeley, Edmund (1949). Giant Brains, or Machines That Think. John Wiley & Sons.
Cohen, Bernard (2000). ""Howard Aiken, Portrait of a computer pioneer"". Physics Today. Cambridge, Massachusetts: The MIT Press. 53 (3): 74–75. Bibcode:2000PhT....53c..74C. doi:10.1063/1.883007. ISBN 978-0-262-53179-5.
Ligonnière, Robert (1987). Préhistoire et Histoire des ordinateurs. Paris: Robert Laffont. ISBN 978-2-221-05261-7.
Couffignal, Louis (1933). Les machines à calculer; leurs principes, leur évolution. Paris: Gauthier-Villars.
Essinger, James (2004). Jacquard's Web, How a hand loom led to the birth of the information age. Oxford University Press. ISBN 978-0-19-280577-5.
Hyman, Anthony (1985). Charles Babbage: Pioneer of the Computer. Princeton University Press. ISBN 978-0-691-02377-9.
Bowden, B. V. (1953). Faster than thought. New York, Toronto, London: Pitman publishing corporation.
Moseley, Maboth (1964). Irascible Genius, Charles Babbage, inventor. London: Hutchinson.
Collier, Bruce (1970). The little engine that could've: The calculating machines of Charles Babbage. Garland Publishing. ISBN 978-0-8240-0043-1.
Randell, Brian (1982). ""From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and Bush"" (PDF). Archived from the original (PDF) on 21 September 2013. Retrieved 29 October 2013.
Smith, Erika E. (2013). ""Recognizing a Collective Inheritance through the History of Women in Computing"". CLCWeb: Comparative Literature and Culture. 15 (1): 1–9. doi:10.7771/1481-4374.1972.
Verma, G.; Mielke, N. (1988). Reliability performance of ETOX based flash memories. IEEE International Reliability Physics Symposium.

External links
 Media related to Computers at Wikimedia Commons
 Wikiversity has a quiz on this article
Warhol & The Computer


vteBasic computer componentsInput devicesPointing devices
Graphics tablet
Game controller
Light pen
Mouse
Optical
Optical trackpad
Pointing stick
Touchpad
Touchscreen
Trackball
Other
Keyboard
Image scanner
Graphics card
GPU
Microphone
Refreshable braille display
Sound card
Sound chip
Webcam
Softcam
Output devices
Monitor
Screen
Refreshable braille display
Printer
Plotter
Speakers
Sound card
Graphics card
Removable  data storage
Disk pack
Floppy disk
Optical disc
CD
DVD
Blu-ray
Flash memory
Memory card
USB flash drive
Computer case
Central processing unit
Microprocessor
Motherboard
Memory
RAM
BIOS
Data storage
HDD
SSD (SATA / NVMe)
SSHD
Power supply
SMPS
MOSFET
Power MOSFET
VRM
Network interface controller
Fax modem
Expansion card
PortsCurrent
Ethernet
USB
Thunderbolt
Analog audio jack
DisplayPort
HDMI
Obsolete
FireWire (IEEE 1394)
Parallel port
Serial port
PS/2 port
eSATA
DVI
VGA

vteDigital electronicsComponents
Transistor
Resistor
Inductor
Capacitor
Printed electronics
Printed circuit board
Electronic circuit
Flip-flop
Memory cell
Combinational logic
Sequential logic
Logic gate
Boolean circuit
Integrated circuit (IC)
Hybrid integrated circuit (HIC)
Mixed-signal integrated circuit
Three-dimensional integrated circuit (3D IC)
Emitter-coupled logic (ECL)
Erasable programmable logic device (EPLD)
Macrocell array
Programmable logic array (PLA)
Programmable logic device (PLD)
Programmable Array Logic (PAL)
Generic array logic (GAL)
Complex programmable logic device (CPLD)
Field-programmable gate array (FPGA)
Field-programmable object array (FPOA)
Application-specific integrated circuit (ASIC)
Tensor Processing Unit (TPU)
Theory
Digital signal
Boolean algebra
Logic synthesis
Logic in computer science
Computer architecture
Digital signal
Digital signal processing
Circuit minimization
Switching circuit theory
Gate equivalent
Design
Logic synthesis
Place and route
Placement
Routing
Register-transfer level
Hardware description language
High-level synthesis
Formal equivalence checking
Synchronous logic
Asynchronous logic
Finite-state machine
Hierarchical state machine
Applications
Computer hardware
Hardware acceleration
Digital audio
radio
Digital photography
Digital telephone
Digital video
cinematography
television
Electronic literature
Design issues
Metastability
Runt pulse

vteElectronicsBranches
Analog electronics
Digital electronics
Electronic instrumentation
Electronics engineering
Microelectronics
Optoelectronics
Power electronics
Printed electronics
Semiconductor
Schematic capture
Thermal management
Advanced topics
Atomtronics
Bioelectronics
Failure of electronic components
Flexible electronics
Low-power electronics
Molecular electronics
Nanoelectronics
Organic electronics
Photonics
Piezotronics
Quantum electronics
Spintronics
Electronic equipment
Air conditioner
Central heating
Clothes dryer
Computer/Notebook
Camera
Dishwasher
Freezer
Home robot
Home cinema
Home theater PC
Information technologies
Cooker
Microwave oven
Mobile phone
Networking hardware
Portable media player
Radio
Refrigerator
Robotic vacuum cleaner
Tablet
Telephone
Television
Water heater
Video game console
Washing machine
Applications
Audio electronics
Automotive electronics
Avionics
Control system
Data acquisition
e-book
e-health
Electronics industry
Electronic warfare
Embedded system
Home appliance
Home automation
Integrated circuit
Home appliance
Consumer electronics
Major appliance
Small appliance
Microwave technology
Military electronics
Multimedia
Nuclear electronics
Open hardware
Radar and Radionavigation
Radio electronics
Terahertz technology
Video hardware
Wired and Wireless Communications

Authority control National libraries
Spain
France (data)
Germany
Israel
United States
Japan
Other
Faceted Application of Subject Terminology
National Archives (US)





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Computer&oldid=1084472285""
Categories: ComputersConsumer electronicsElectronics industryHidden categories: Webarchive template wayback linksCS1 German-language sources (de)Pages containing links to subscription-only contentWikipedia articles needing page number citations from March 2022Articles with short descriptionShort description is different from WikidataWikipedia indefinitely semi-protected pagesWikipedia indefinitely move-protected pagesPages using multiple image with auto scaled imagesAll articles with unsourced statementsArticles with unsourced statements from December 2007Articles with unsourced statements from May 2014Articles needing additional references from July 2012All articles needing additional referencesArticles with unsourced statements from September 2015Commons category link is on WikidataUse dmy dates from July 2017Articles with BNE identifiersArticles with BNF identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with FAST identifiersArticles with NARA identifiersArticles containing video clipsArticles with example code

"
8,Windows,"





Microsoft Windows

From Wikipedia, the free encyclopedia
  (Redirected from Windows)


Jump to navigation
Jump to search
Family of computer operating systems developed by Microsoft
""Windows"" redirects here. For the part of a building, see Window. For other uses, see Windows (disambiguation).


Microsoft WindowsDeveloperMicrosoftSource modelClosed-sourceSource-available (through Shared Source Initiative)Initial releaseNovember 20, 1985; 36 years ago (1985-11-20)Latest release21H2 (10.0.22000.652) (April 25, 2022; 9 days ago (2022-04-25)[1]) [±]Latest preview10.0.22610.1 (April 29, 2022; 5 days ago (2022-04-29)[2]) [±]Marketing targetPersonal computingAvailable in110 languagesUpdate methodWindows UpdateMicrosoft StoreWindows Server Update Services (WSUS)Package managerWindows Installer (.msi, .msix, .msp), Microsoft Store (.appx, .appxbundle),[3] Windows Package ManagerPlatformsIA-32, x86-64, ARM, ARM64 Previously: 16-bit x86, DEC Alpha, MIPS, PowerPC, ItaniumKernel type
Windows NT family: Hybrid
Windows CE: Hybrid
Windows 9x and earlier: Monolithic (MS-DOS)
Defaultuser interfaceWindows shellLicenseProprietary commercial softwareOfficial websitemicrosoft.com/windows
Microsoft Windows, commonly referred to as Windows, is a group of several proprietary graphical operating system families, all of which are developed and marketed by Microsoft. Each family caters to a certain sector of the computing industry. Active Microsoft Windows families include Windows NT and Windows IoT; these may encompass subfamilies, (e.g. Windows Server or Windows Embedded Compact) (Windows CE). Defunct Microsoft Windows families include Windows 9x, Windows Mobile and Windows Phone.
Microsoft introduced an operating environment named Windows on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs).[4] Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984.
Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993).  On PCs, Windows is still the most popular operating system in all countries.[5][6] However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android,[7] because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison, however, may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to that for end user use. 
As of October 2021[update], the most recent version of Windows for PCs and tablets is Windows 11, version 21H2. The most recent version for embedded devices is Windows 10, version 21H1. The most recent version for server computers is Windows Server 2022, version 21H2.[8] A specialized version of Windows also runs on the Xbox One and Xbox Series X/S video game consoles.[9]

Contents

1 Genealogy

1.1 By marketing role


2 Version history

2.1 Early versions
2.2 Windows 3.x
2.3 Windows 9x
2.4 Windows NT

2.4.1 Version history

2.4.1.1 Early versions (Windows NT 3.1/3.5/3.51/4.0/2000)
2.4.1.2 Windows XP
2.4.1.3 Windows Vista
2.4.1.4 Windows 7
2.4.1.5 Windows 8 and 8.1
2.4.1.6 Windows 10
2.4.1.7 Windows 11


2.4.2 Windows 365
2.4.3 Multilingual support
2.4.4 Platform support


2.5 Windows CE
2.6 Xbox OS


3 Version control system

3.1 VFSForGit


4 Timeline of releases
5 Usage share and device sales
6 Security

6.1 File permissions


7 Alternative implementations
8 See also
9 References
10 External links


Genealogy
By marketing role
Microsoft, the developer of Windows, has registered several trademarks, each of which denotes a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families were being actively developed:

Windows NT: Started as a family of operating systems with Windows NT 3.1, an operating system for server computers and workstations. It now consists of three operating system subfamilies that are released almost at the same time and share the same kernel:
Windows: The operating system for mainstream personal computers and tablets. The latest version is Windows 11. The main competitor of this family is macOS by Apple for personal computers and iPadOS and Android for tablets (c.f. Usage share of operating systems § Market share by category).
Windows Server: The operating system for server computers. The latest version is Windows Server 2022. Unlike its client sibling, it has adopted a strong naming scheme. The main competitor of this family is Linux. (c.f. Usage share of operating systems § Market share by category)
Windows PE: A lightweight version of its Windows sibling, meant to operate as a live operating system, used for installing Windows on bare-metal computers (especially on many computers at once), recovery or troubleshooting purposes. The latest version is Windows PE 10.
Windows IoT (previously Windows Embedded): Initially, Microsoft developed Windows CE as a general-purpose operating system for every device that was too resource-limited to be called a full-fledged computer. Eventually, however, Windows CE was renamed Windows Embedded Compact and was folded under Windows Compact trademark which also consists of Windows Embedded Industry, Windows Embedded Professional, Windows Embedded Standard, Windows Embedded Handheld and Windows Embedded Automotive.[10]
The following Windows families are no longer being developed:

Windows 9x: An operating system that targeted the consumer market. Discontinued because of suboptimal performance.[citation needed] (PC World called its last version, Windows Me, one of the worst products of all time.[11]) Microsoft now caters to the consumer market with Windows NT.
Windows Mobile: The predecessor to Windows Phone, it was a mobile phone operating system. The first version was called Pocket PC 2000; the third version, Windows Mobile 2003 is the first version to adopt the Windows Mobile trademark. The last version is Windows Mobile 6.5.
Windows Phone: An operating system sold only to manufacturers of smartphones. The first version was Windows Phone 7, followed by Windows Phone 8, and Windows Phone 8.1. It was succeeded by Windows 10 Mobile, that is now also discontinued.
Version history
Main article: History of Microsoft Windows
See also: List of Microsoft Windows versions
The term Windows collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:

Early versions
Main articles: Windows 1.0, Windows 2.0, and Windows 2.1x
 Windows 1.0, the first version, released in 1985
The history of Windows dates back to 1981 when Microsoft started work on a program called ""Interface Manager"". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name ""Windows"", but Windows 1.0 was not released until November 1985.[12] Windows 1.0 was to compete with Apple's operating system, but achieved little popularity.  Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard Viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.[13]
Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management.[14] Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights.[15][16] Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.
Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area.[citation needed]
In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.
The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services.[17] However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.

Windows 3.x
Main articles: Windows 3.0 and Windows 3.1x
 Windows 3.0, released in 1990
Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications.[citation needed] Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.[18][19]
Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.[20]
Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1.[21] The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language.[22] Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.

Windows 9x
Main articles: Windows 9x, Windows 95, Windows 98, and Windows Me
The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that ""by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world.""[23] Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer.[24] Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.[25]
Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.[26]
On September 14, 2000, Microsoft released Windows Me (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs),[27] expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools.[28] However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. PC World considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.[11]

Windows NT
Main article: Windows NT
Version history
Early versions (Windows NT 3.1/3.5/3.51/4.0/2000)
Main articles: Windows NT 3.1, Windows NT 3.5, Windows NT 3.51, Windows NT 4.0, and Windows 2000
In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as ""NT OS/2"". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.[29][30]
Windows NT was the first Windows operating system based on a hybrid kernel. The hybrid kernel was designed as a modified microkernel, influenced by the Mach microkernel developed by Richard Rashid at Carnegie Mellon University, but without meeting all of the criteria of a pure microkernel.
The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.[30]

Windows XP
Main article: Windows XP
The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a ""task-oriented"" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, a ""compatibility mode"" to help provide backwards compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.[31][32]
At retail, Windows XP was now marketed in two main editions: the ""Home"" edition was targeted towards consumers, while the ""Professional"" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the ""Media Center"" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the ""Tablet PC"" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications).[33][34][35] Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.[36]
After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003.[30] It was followed in December 2005, by Windows Server 2003 R2.

Windows Vista
Main article: Windows Vista
After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.

Windows 7
Main article: Windows 7
On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible.[37] Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar with revealable jump lists that contain shortcuts to files frequently used with specific applications and shortcuts to tasks within the application,[38] a home networking system called HomeGroup,[39] and performance improvements.

Windows 8 and 8.1
Main articles: Windows 8 and Windows 8.1
 Previous Windows logo (2012–2021)
Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels,[40] effectively making it unfit for netbooks with 800×600-pixel screens.
Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture, and a new keyboard shortcut for screenshots.[41][42][43][44][45][46][47] An update to Windows 8, called Windows 8.1,[48] was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 have been subject to some criticism, such as removal of the Start menu.

Windows 10
Main article: Windows 10
On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1, Windows 8.1 and Windows Phone 8.1 devices from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).[49]
In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300 gigabyte repository.[50] By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.[50]
In June 2021, shortly before Microsoft's announcement of Windows 11, Microsoft updated their lifecycle policy pages for Windows 10, revealing that support for their last release of Windows 10 will be October 14, 2025.[51][52]

Windows 11
Main article: Windows 11
On June 24, 2021, Windows 11 was announced as the successor to Windows 10 during a livestream. The new operating system was designed to be more user-friendly and understandable. It was released on October 5, 2021.[53][54] Windows 11 is a free upgrade to some Windows 10 users as of now.

Windows 365
See also: Azure Virtual Desktop
In July 2021, Microsoft announced it will start selling subscriptions to virtualized Windows desktops as part of a new Windows 365 service in the following month. It is not a standalone version of Microsoft Windows, but a web service that provides access to Windows 10 and Windows 11 built on top of Azure Virtual Desktop. The new service will allow for cross-platform usage, aiming to make the operating system available for both Apple and Android users. The subscription service will be accessible through any operating system with a web browser. The new service is an attempt at capitalizing on the growing trend, fostered during the COVID-19 pandemic, for businesses to adopt a hybrid remote work environment, in which ""employees split their time between the office and home"". As the service will be accessible through web browsers, Microsoft will be able to bypass the need to publish the service through Google Play or the Apple App Store.[55][56][57][58][59]
Microsoft announced Windows 365 availability to business and enterprise customers on August 2, 2021.[60]

Multilingual support
Multilingual support has been built into Windows since Windows 3.0. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.
Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) –  they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).
The interface language of installed applications is not affected by changes in the Windows interface language. The availability of languages depends on the application developers themselves.
Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.

Platform support
Windows NT included support for several platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000 (although some of the platforms implement 64-bit computing, the OS treated them as 32-bit). Windows 2000 dropped support for all platforms, except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32 but the Windows Server line ceased supporting this platform with the release of Windows Server 2008 R2.
With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.
On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support x86-64 (or simply x64), the 64-bit version of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.
An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from Windows 10 Fall Creators Update (version 1709) and later includes support for PCs with ARM architecture.[61]
Windows 11 is the first version to drop support for 32-bit hardware.[62]

Windows CE
Main articles: Windows CE and Windows Phone
 Windows Embedded Compact 7 displaying a concept media player UI
Windows CE (officially known as Windows Embedded Compact), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.
Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.

Xbox OS
Main article: Xbox system software
Xbox OS is an unofficial name given to the version of Windows that runs on Xbox consoles.[63] From Xbox One onwards it is an implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.[64]
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC.[65] It was originally based on NT 6.2 (Windows 8) kernel, and the latest version runs on an NT 10.0 base. This system is sometimes referred to as ""Windows 10 on Xbox One"" or ""OneCore"".[66][67]
Xbox One and Xbox Series operating systems also allow limited (due to licensing restrictions and testing resources) backward compatibility with previous generation hardware,[68] and the Xbox 360's system is backwards compatible with the original Xbox.[69]

Version control system
Up to and including every version before Windows 2000, Microsoft used an in-house version control system named Source Library Manager (SLM). Shortly after Windows 2000 was released, Microsoft switched to a fork of Perforce named Source Depot.[70] This system was used up until 2017 once the system couldn't keep up with the size of Windows. Microsoft had begun to integrate Git into Team Foundation Server in 2013, but Windows continued to rely on Source Depot.[citation needed] The Windows code was divided among 65 different repositories with a kind of virtualization layer to produce unified view of all of the code.
In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds and in May 2017 they reported that has completed migration into the Git repository.[71][72][50]

VFSForGit
Because of its large, decades-long history, however, the Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage.[citation needed] Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the whole repository takes several hours.[citation needed] Microsoft has been working on a new project called the Virtual File System for Git (VFSForGit) to address these challenges.[72]
In 2021 the VFS for Git has been superseded by Scalar.[73]

Timeline of releases
Table of Windows versions
Legend:Old versionOlder version, still maintainedLatest versionLatest preview versionFuture release


Product name

Latest version

General availability date

Codename

Support until[74]

Latest version of


Mainstream

Extended

IE

DirectX

Edge


Old version, no longer maintained: Windows 1.0

1.01

November 20, 1985

Interface Manager

December 31, 2001

N/A

N/A

N/A


Old version, no longer maintained: Windows 2.0

2.03

December 9, 1987

N/A

December 31, 2001


Old version, no longer maintained: Windows 2.1

2.11

May 27, 1988

N/A

December 31, 2001


Old version, no longer maintained: Windows 3.0

3.0

May 22, 1990

N/A

December 31, 2001


Old version, no longer maintained: Windows 3.1

3.1

April 6, 1992

Janus

December 31, 2001

5


Old version, no longer maintained: Windows For Workgroups 3.1

3.1

October 1992

Sparta, Winball

December 31, 2001


Old version, no longer maintained: Windows NT 3.1

NT 3.1.528

July 27, 1993

N/A

December 31, 2001


Old version, no longer maintained: Windows For Workgroups 3.11

3.11

August 11, 1993

Sparta, Winball

December 31, 2001


Old version, no longer maintained: Windows 3.2

3.2

November 22, 1993

N/A

December 31, 2001


Old version, no longer maintained: Windows NT 3.5

NT 3.5.807

September 21, 1994

Daytona

December 31, 2001


Old version, no longer maintained: Windows NT 3.51

NT 3.51.1057

May 30, 1995

N/A

December 31, 2001


Old version, no longer maintained: Windows 95

4.0.950

August 24, 1995

Chicago, 4.0

December 31, 2000

December 31, 2001

5.5

6.1


Old version, no longer maintained: Windows NT 4.0

NT 4.0.1381

July 31, 1996

Cairo

June 30, 2002

June 30, 2004

6

N/A


Old version, no longer maintained: Windows 98

October 4, 1998

June 25, 1998

Memphis, 97, 4.1

June 30, 2002

July 11, 2006

6.1


Old version, no longer maintained: Windows 98 SE

4.10.2222

May 5, 1999

N/A

June 30, 2002

July 11, 2006


Old version, no longer maintained: Windows 2000

NT 5.0.2195

February 17, 2000

N/A

June 30, 2005

July 13, 2010

N/A


Old version, no longer maintained: Windows Me

4.90.3000

September 14, 2000

Millennium, 4.9

December 31, 2003

July 11, 2006

9.0c


Old version, no longer maintained: Windows XP

NT 5.1.2600

October 25, 2001

Whistler

April 14, 2009

April 8, 2014

8


Old version, no longer maintained: Windows XP 64-bit Edition

NT 5.2.3790

March 28, 2003

N/A

April 14, 2009

April 8, 2014


Old version, no longer maintained: Windows Server 2003

NT 5.2.3790

April 24, 2003

N/A

July 13, 2010

July 14, 2015


Old version, no longer maintained: Windows XP Professional x64 Edition

NT 5.2.3790

April 25, 2005

N/A

April 14, 2009

April 8, 2014


Old version, no longer maintained: Windows Fundamentals for Legacy PCs

NT 5.1.2600

July 8, 2006

Eiger, Mönch

April 14, 2009

April 8, 2014


Old version, no longer maintained: Windows Vista

NT 6.0.6003

January 30, 2007

Longhorn

April 10, 2012

April 11, 2017

9

11


Old version, no longer maintained: Windows Home Server

NT 5.2.4500

November 4, 2007

Quattro

January 8, 2013

8

9.0c


Old version, no longer maintained: Windows Server 2008

NT 6.0.6003

February 27, 2008

Longhorn Server

January 13, 2015

January 14, 2020

9

11


Old version, no longer maintained: Windows 7

NT 6.1.7601

October 22, 2009

Windows 7[75]

January 13, 2015

January 14, 2020

11

92


Old version, no longer maintained: Windows Server 2008 R2

NT 6.1.7601

October 22, 2009

N/A

January 13, 2015

January 14, 2020


Old version, no longer maintained: Windows Home Server 2011

NT 6.1.8400

April 6, 2011

Vail

April 12, 2016

9


Older version, yet still maintained: Windows Server 2012

NT 6.2.9200

September 4, 2012

Server 8

October 9, 2018

October 10, 2023

11

11.1


Old version, no longer maintained: Windows 8

NT 6.2.9200

October 26, 2012

N/A

January 12, 2016

10


Older version, yet still maintained: Windows 8.1

NT 6.3.9600

October 17, 2013

Blue

January 9, 2018

January 10, 2023

11

11.2


Older version, yet still maintained: Windows Server 2012 R2

NT 6.3.9600

October 18, 2013

Server Blue

October 9, 2018

October 10, 2023


Current stable version: Windows 10

NT 10.0.19044

July 29, 2015

Various

October 14, 2025[51][52]

12


Older version, yet still maintained: Windows Server 2016

NT 10.0.14393

October 12, 2016

N/A

January 11, 2022

January 12, 2027


Current stable version: Windows Server 2019

NT 10.0.17763

October 2, 2018

N/A

January 9, 2024

January 9, 2029


Current stable version: Windows Server 2022

NT 10.0.20348

August 18, 2021

N/A

October 13, 2026

October 14, 2031


Current stable version: Windows 11

NT 10.0.22000

October 5, 2021

N/A

October 10, 2023

October 8, 2024

N/A


Windows timeline: Bar chart


viewtalkedit

 The Windows family tree
Usage share and device sales
Main article: Usage share of operating systems

This box:  viewtalkedit
Version market share
As a percentage of desktop and laptop systems using Windows,[76] according to StatCounter data from April 2022.[77]



Desktop OS

StatCounter


other versions

0.35%


Windows XP

0.44%


Windows 7

12.62%


Windows 8

1.22%


Windows 8.1

3.24%


Windows 10

73.24%


Windows 11

8.89%




Use of the latest version Windows 10 has exceeded Windows 7 globally since early 2018.[78]
For desktop and laptop computers, according to Net Applications and StatCounter, which track the use of operating systems in devices that are active on the Web, Windows was the most used operating-system family in August 2021, with around 91% usage share according to Net Applications[79] and around 76% usage share according to StatCounter.[80]
Including personal computers of all kinds (e.g., desktops, laptops, mobile devices, and game consoles), Windows OSes accounted for 32.67% of usage share in August 2021, compared to Android (highest, at 46.03%), iOS's 13.76%, iPadOS's 2.81%, and macOS's 2.51%, according to Net Applications[81] and 30.73% of usage share in August 2021, compared to Android (highest, at 42.56%), iOS/iPadOS's 16.53%, and macOS's 6.51%, according to StatCounter.[82]
Those statistics do not include servers (including so-called cloud computing, where Microsoft is known not to be a leader, with Linux used more than Windows), as Net Applications and StatCounter use web browsing as a proxy for all use.

Security
This section needs to be updated. Please help update this article to reflect recent events or newly available information. (May 2020)
Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset.[83] However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.[84]
These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's Counterpane Internet Security reported that it had seen over 1,000 new viruses and worms in the previous six months.[85] In 2005, Kaspersky Lab found around 11,000 malicious programs –  viruses, Trojans, back-doors, and exploits written for Windows.[86]
Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary.[87] In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.[88]
While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.
The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.
Windows Vista changes this[89] by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or ""Run as administrator"" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.[90]
Leaked documents published by WikiLeaks, codenamed Vault 7 and dated from 2013 to 2016, detail the capabilities of the CIA to perform electronic surveillance and cyber warfare,[91] such as the ability to compromise operating systems such as Microsoft Windows.[92]
In August 2019, computer experts reported that the BlueKeep security vulnerability, CVE-2019-0708, that potentially affects older unpatched Microsoft Windows versions via the program's Remote Desktop Protocol, allowing for the possibility of remote code execution, may now include related flaws, collectively named DejaBlue, affecting newer Windows versions (i.e., Windows 7 and all recent versions) as well.[93] In addition, experts reported a Microsoft security vulnerability, CVE-2019-1162, based on legacy code involving Microsoft CTF and ctfmon (ctfmon.exe), that affects all Windows versions from the older Windows XP version to the most recent Windows 10 versions; a patch to correct the flaw is currently available.[94]

File permissions
All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Domain Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.

Alternative implementations
Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:

Wine – a free and open-source implementation of the Windows API, allowing one to run many Windows applications on x86-based platforms, including UNIX, Linux and macOS. Wine developers refer to it as a ""compatibility layer""[95] and use Windows-style APIs to emulate Windows environment.
CrossOver – a Wine package with licensed fonts. Its developers are regular contributors to Wine, and focus on Wine running officially supported applications.
Cedega – a proprietary fork of Wine by TransGaming Technologies, designed specifically for running Microsoft Windows games on Linux. A version of Cedega known as Cider allows Windows games to run on macOS. Since Wine was licensed under the LGPL, Cedega has been unable to port the improvements made to Wine to their proprietary codebase. Cedega ceased its service in February 2011.
Darwine – a port of Wine for macOS and Darwin. Operates by running Wine on QEMU.
Linux Unified Kernel – a set of patches to the Linux kernel allowing many Windows executable files in Linux (using Wine DLLs); and some Windows drivers to be used.
ReactOS – an open-source OS intended to run the same software as Windows, originally designed to simulate Windows NT 4.0, now aiming at Windows 7 compatibility. It has been in the development stage since 1996.
Linspire – formerly LindowsOS, a commercial Linux distribution initially created with the goal of running major Windows software. Changed its name to Linspire after Microsoft v. Lindows. Discontinued in favor of Xandros Desktop, that was also later discontinued.
Freedows OS – an open-source attempt at creating a Windows clone for x86 platforms, intended to be released under the GNU General Public License. Started in 1996, by Reece K. Sellin, the project was never completed, getting only to the stage of design discussions which featured a number of novel concepts until it was suspended in 2002.[96][97][98]
See also

Architecture of Windows NT
Azure Sphere, Microsoft's Linux-based operating system
BlueKeep
De facto standard
Dominant design
Windows Subsystem for Linux, a subsystem in Windows 10, not using the Linux kernel; reimplementing
Wintel


References


^ ""April 25, 2022—KB5012643 (OS Build 22000.652)"". Microsoft Support. Microsoft. April 25, 2022.

^ ""Announcing Windows 11 Insider Preview Build 22610"". Windows Insider Blog. April 29, 2022.

^ ""App packages and deployment (Windows Store apps) (Windows)"". Msdn.microsoft.com. Archived from the original on March 30, 2014. Retrieved April 5, 2014.

^ ""The Unusual History of Microsoft Windows"". Retrieved April 22, 2007.

^ ""Operating System Market Share Worldwide"". StatCounter Global Stats. Retrieved January 5, 2021.

^ ""Desktop Operating System Market Share Worldwide"". StatCounter Global Stats. Retrieved January 5, 2021.

^ Keizer, Gregg (July 14, 2014). ""Microsoft gets real, admits its device share is just 14%"". Computerworld. IDG. Archived from the original on August 21, 2016. [Microsoft's chief operating officer] Turner's 14% came from a new forecast released last week by Gartner, which estimated Windows' share of the shipped device market last year was 14%, and would decrease slightly to 13.7% in 2014. Android will dominate, Gartner said, with a 48% share this year

^ ""Windows Server release information"". docs.microsoft.com. Retrieved July 26, 2021.

^ ""Xbox One Architecture Finally Explained - Runs OS 'Virtually Indistinguishable' from Windows 8"". WCCFtech. April 20, 2014. Archived from the original on September 6, 2015.

^ ""RTOS: Embedded Real Time Operating Systems"". microsoft.com. Microsoft. Archived from the original on December 15, 2014. Retrieved November 7, 2014.

^ a b ""The 25 Worst Tech Products of All Time"". PC World. IDG. May 26, 2006. Archived from the original on February 15, 2012. Retrieved February 10, 2012.

^ A history of Windows (at microsoft.com)

^ Microsoft C 5.0 C Language Reference Guide, Microsoft Doc410840001-500-R04-0887A, 10/1987 page 250-267

^ ""A legacy of Windows, part 1: Windows 1-2-3 - TechRepublic"". TechRepublic. Archived from the original on March 27, 2017. Retrieved March 26, 2017.

^ ""The Apple vs. Microsoft GUI Lawsuit"". 2006. Archived from the original on March 4, 2008. Retrieved March 12, 2008.

^ ""Apple Computer, Inc. v. MicroSoft Corp., 35 F.3d 1435 (9th Cir. 1994)"". Archived from the original on December 14, 2007. Retrieved March 12, 2008.

^ ""Windows Evolution"". Soft32.com News. Archived from the original on February 8, 2008.

^ ""Chronology of Personal Computer Software"". Archived from the original on February 11, 2012.

^ ""Microsoft Company"". Archived from the original on May 14, 2008.

^ ""Windows 3.1 Standard Edition Support Lifecycle"". Archived from the original on January 12, 2012. Retrieved January 3, 2011.

^ ""Microsoft Windows Simplified Chinese 3.2 Upgrade Is Available"". microsoft.com. Microsoft. Archived from the original on November 8, 2006.

^ ""Microsoft Windows Simplified Chinese 3.2 Upgrade Is Available"". Microsoft. October 30, 2003. Archived from the original on May 24, 2011. Retrieved September 4, 2009.

^ ""Windows 95 turns 15: Has Microsoft's OS peaked?"". CNET/CNN Tech. August 25, 2010. Archived from the original on August 26, 2010. Retrieved August 22, 2012.

^ ""Microsoft Internet Explorer Web Browser Available on All Major Platforms, Offers Broadest International Support"". News Center. San Jose, California: Microsoft. April 30, 1996. Archived from the original on January 15, 2008. Retrieved February 14, 2011.

^ ""Windows 95 Support Lifecycle"". Microsoft. Archived from the original on November 22, 2012. Retrieved January 3, 2011.

^ ""Windows 98 Standard Edition Support Lifecycle"". Microsoft. Archived from the original on November 22, 2012. Retrieved January 3, 2011.

^ ""Improving ""Cold Boot"" Time for System Manufacturers"". Microsoft. December 4, 2001. Archived from the original on February 13, 2010. Retrieved August 26, 2010.

^ ""Windows Millennium Edition: All About Me"". PC World. Archived from the original on August 1, 2013. Retrieved May 21, 2013.

^ Custer, Helen (1993). Inside Windows NT. Redmond: Microsoft Press. ISBN 1-55615-481-X.

^ a b c Thurrott, Paul (January 24, 2003). ""Windows Server 2003: The Road To Gold - Part One: The Early Years"". Archived from the original on January 1, 2005. Retrieved May 28, 2012.

^ ""Windows XP review"". CNET. Archived from the original on May 26, 2013. Retrieved May 24, 2013.

^ ""Windows XP Program Compatibility Wizard"". ServerWatch. March 12, 2002. Retrieved November 13, 2021.

^ David Coursey (October 25, 2001). ""The 10 top things you MUST know about Win XP"". ZDNet. Archived from the original on April 3, 2009. Retrieved July 22, 2008.

^ David Coursey (August 31, 2001). ""Your top Windows XP questions answered! (Part One)"". ZDNet. CNET Networks. Archived from the original on December 19, 2007. Retrieved January 3, 2011.

^ ""A Look at Freestyle and Mira"". Paul Thurrott's SuperSite for Windows. Penton. September 3, 2002. Retrieved January 3, 2011.[permanent dead link]

^ ""Windows XP Professional Lifecycle Support"". Archived from the original on February 27, 2013. Retrieved January 3, 2011.

^ Nash, Mike (October 28, 2008). ""Windows 7 Unveiled Today at PDC 2008"". Windows Experience Blog. Microsoft. Archived from the original on November 1, 2008. Retrieved November 11, 2008.

^ Kiriaty, Yochay; Goldshtein, Sasha (2009). ""Windows 7 Taskbar APIs"". docs.microsoft.com. Retrieved August 21, 2021.

^ LeBlanc, Brandon (October 28, 2008). ""How Libraries & HomeGroup Work Together in Windows 7"". Windows Experience Blog. Microsoft. Archived from the original on November 2, 2008. Retrieved November 11, 2008.

^ ""New Windows 8 hardware specs hint at 7-inch tablets and a Microsoft Reader"". ZDNet. Retrieved March 29, 2013.

^ Paul, Ian (July 5, 2021). ""How to Take Screenshots in Windows 10, 8, and 7"".

^ Case, Loyd. ""Test Driving Windows 8 RTM"". PC World. IDG. Archived from the original on September 4, 2012. Retrieved September 9, 2012.

^ Rosoff, Matt. ""Here's Everything You Wanted To Know About Microsoft's Upcoming iPad Killers"". Business Insider. Archived from the original on January 22, 2013. Retrieved February 10, 2012.

^ ""Announcing the Windows 8 Editions"". Microsoft. April 16, 2012. Archived from the original on April 18, 2012. Retrieved April 17, 2012.

^ ""Building Windows for the ARM processor architecture"". Microsoft. Archived from the original on November 26, 2012. Retrieved November 21, 2012.

^ ""Microsoft talks Windows Store features, Metro app sandboxing for Windows 8 developers"". The Verge. Vox Media. May 17, 2012. Archived from the original on September 10, 2012. Retrieved September 8, 2012.

^ Miller, Michael. ""Build: More Details On Building Windows 8 Metro Apps"". PC Magazine. Archived from the original on February 17, 2012. Retrieved February 10, 2012.

^ Windows 8.1 now available! Archived October 19, 2013, at the Wayback Machine. Blogs.windows.com. Retrieved on October 31, 2013.

^ ""Announcing Windows 10 - Windows Blog"". September 30, 2014. Archived from the original on September 10, 2015. Retrieved September 30, 2014.

^ a b c Bright, Peter (May 24, 2017). ""Windows switch to Git almost complete: 8,500 commits and 1,760 builds each day"". Ars Technica. Condé Nast. Archived from the original on May 24, 2017.

^ a b ""Window 10 Home and Pro Lifecycle"". Microsoft. Retrieved July 2, 2021.

^ a b ""Window 10 Enterprise and Education Lifecycle"". Microsoft. Retrieved July 2, 2021.

^ Cox, George. ""Windows 11 release date is October 5"". The Spectrum. Retrieved September 18, 2021.

^ Warren, Tom (June 24, 2021). ""Microsoft announces Windows 11, with a new design, Start menu, and more"". The Verge. Retrieved June 24, 2021.

^ Foley, Mary Jo (July 14, 2021). ""Microsoft brings Windows to the cloud with Windows 365 and Cloud PC"". ZDNet. Retrieved July 14, 2021.

^ Tilley, Aaron (July 14, 2021). ""Microsoft Aims to Put Windows in Hands of Apple, Android Users Through Hybrid Work"". The Wall Street Journal. ISSN 0099-9660.

^ Higgins, Tim (June 23, 2021). ""Apple's Fight for Control Over Apps Moves to Congress and EU"". The Wall Street Journal. ISSN 0099-9660.

^ ""Microsoft unveils Windows 365, a Windows 10 PC in the cloud"". Engadget. Retrieved July 15, 2021.

^ ""Windows 365 Cloud PC | Microsoft"". www.microsoft.com. Retrieved July 15, 2021.

^ Hill, Paul (August 2, 2021). ""Microsoft announces the general availability of Windows 365"". Neowin. Retrieved August 2, 2021.

^ Bott, Ed (October 7, 2019). ""Windows 10 on Arm: What you need to know before you buy a Surface Pro X"". ZDNet.

^ ""Windows 11 Specs and System Requirements | Microsoft"". Windows. Retrieved October 6, 2021.

^ Anand Lal Shimpi. ""The Xbox One - Mini Review & Comparison to Xbox 360/PS4"". anandtech.com. Archived from the original on October 12, 2014. Retrieved October 21, 2014.

^ ""Xbox One: Hardware and software specs detailed and analyzed - Three operating systems in one"". ExtremeTech. Archived from the original on November 16, 2013. Retrieved December 1, 2013.

^ ""How to use the Offline System Update Diagnostic Tool on Xbox One"". Xbox Official Site. Microsoft. Archived from the original on April 27, 2021. Retrieved November 30, 2013.

^ ""Xbox One Is ""Literally a Windows Device"""". GameSpot. Archived from the original on December 27, 2015.

^ ""New Xbox One Update Will Make Some Functionality 50 Percent Faster"". GameSpot. Archived from the original on February 2, 2016.

^ Tom Warren (June 16, 2015). ""Xbox One dashboard update includes a huge new design and Cortana"". The Verge. Vox Media. Archived from the original on July 8, 2017.

^ Eric Qualls. ""Xbox 360 and Xbox Games Backwards Compatibility"". About.com Tech. Archived from the original on September 28, 2015.

^ Chen, Raymond (January 22, 2018). ""The history of change-packing tools at Microsoft (so far)"". The Old New Thing. Retrieved April 17, 2022.{{cite web}}:  CS1 maint: url-status (link)

^ ""The largest Git repo on the planet"". Brian Harry's Blog. May 24, 2017. Retrieved October 8, 2021.

^ a b Bright, Peter (February 6, 2017). ""Microsoft hosts the Windows source in a monstrous 300GB Git repository"". Ars Technica. Archived from the original on December 26, 2017. Retrieved December 26, 2017.

^ ""Frequently Asked Questions | VFS for Git"". GitHub. Microsoft. Archived from the original on July 7, 2021. Retrieved July 7, 2021. We transitioned our large repository strategy to focus on using git sparse-checkout instead of filesystem virtualization. We then forked the VFS for Git codebase to create Scalar.

^ ""Microsoft Support Lifecycle"". Microsoft. Archived from the original on October 11, 2008.

^ Chen, Raymond (July 22, 2019). ""What was the code name for Windows 7?"". The Old New Thing.

^ ""Frequently Asked Questions"". StatCounter. ""Are laptops included in the desktop platform?"".

^ ""Desktop Windows Version Market Share Worldwide"". StatCounter.

^ ""Desktop Windows Version Market Share Worldwide | StatCounter Global Stats"". StatCounter Global Stats. Retrieved November 24, 2019.

^ ""Desktop Operating system market share: August 2021"". Net Applications.

^ ""Desktop Operating System Market Share Worldwide: August 2021"". StatCounter.

^ ""Operating system market share: August 2021"". Net Applications.

^ ""Operating System Market Share Worldwide: August 2021"". StatCounter.

^ Multi-user memory protection was not introduced until Windows NT and XP, and a computer's default user was an administrator until Windows Vista. Source: UACBlog Archived April 28, 2006, at the Wayback Machine.

^ ""Telephones and Internet Users by Country, 1990 and 2005"". Information Please Database. Archived from the original on May 22, 2009. Retrieved June 9, 2009.

^ Bruce Schneier (June 15, 2005). ""Crypto-Gram Newsletter"". Counterpane Internet Security, Inc. Archived from the original on June 6, 2007. Retrieved April 22, 2007.

^ Andy Patrizio (April 27, 2006). ""Linux Malware On The Rise"". InternetNews. QuinStreet. Archived from the original on February 5, 2012. Retrieved January 3, 2011.

^ Ryan Naraine (June 8, 2005). ""Microsoft's Security Response Center: How Little Patches Are Made"". eWeek. Ziff Davis Enterprise. Retrieved January 3, 2011.

^ John Foley (October 20, 2004). ""Windows XP SP2 Distribution Surpasses 100 Million"". InformationWeek. UBM TechWeb. Archived from the original on May 27, 2010. Retrieved January 3, 2011.

^ Northrup, Tony (June 1, 2005). ""Windows Vista Security and Data Protection Improvements"". TechNet. Microsoft Docs. Retrieved October 20, 2021. In Windows Vista, the User Account Control (UAC) initiative introduces fundamental operating system changes to enhance the experience for the non-administrative user.

^ Kenny Kerr (September 29, 2006). ""Windows Vista for Developers – Part 4 – User Account Control"". Archived from the original on March 29, 2007. Retrieved March 15, 2007.

^ Greenberg, Andy (March 7, 2017). ""How the CIA Can Hack Your Phone, PC, and TV (Says WikiLeaks)"". WIRED.

^ ""Vault 7: Wikileaks reveals details of CIA's hacks of Android, iPhone Windows, Linux, MacOS, and even Samsung TVs"". Computing. March 7, 2017.

^ Greenberg, Andy (August 13, 2019). ""DejaBlue: New BlueKeep-Style Bugs Renew The Risk Of A Windows worm"". wired. Retrieved August 15, 2019.

^ Seals, Tara (August 14, 2019). ""20-Year-Old Bug in Legacy Microsoft Code Plagues All Windows Users"". ThreatPost.com. Retrieved August 15, 2019.

^ ""Wine"". Winehq.org. Archived from the original on April 4, 2014. Retrieved April 5, 2014.

^ ""A Student's Dream of Creating A New Operating System Encounters Problems"". The Chronicle of Higher Education. September 18, 1998. Archived from the original on May 12, 2013. Retrieved May 17, 2013.

^ ""Older blog entries for chipx86"". Advogato.org. Advogato. June 27, 2002. Archived from the original on May 20, 2013. Retrieved May 17, 2013.

^ ""Freedows splits"". Slashdot. Dice Holdings. August 31, 1998. Archived from the original on November 4, 2013. Retrieved May 17, 2013.


External links
Microsoft Windowsat Wikipedia's sister projectsMedia from CommonsTextbooks from WikibooksResources from Wikiversity
Official website
Official Windows Blog
Microsoft Developer Network
Windows Developer Center
Microsoft Windows History Timeline
Pearson Education, InformIT – History of Microsoft Windows
Microsoft Business Software Solutions
Windows 10 release Information
vteMicrosoft Windows
Components
History
Timeline
Criticism
DOS-based
Windows 1.0x
Windows 2.0x
Windows 2.1x
Windows 3.0
Windows 3.1x
Windows 9x
Windows 95
Windows 98
Windows Me
Windows NT
NT 3.1
NT 3.5
NT 3.51
NT 4.0
2000
XP
Pro x64
Media Center
Fundamentals for Legacy PCs
Vista
7
8, 8.1
Windows RT
10
11
Windows Server
Server 2003
Home Server
Server 2008
EBS 2008
HPC Server 2008
Server 2008 R2
Home Server 2011
Server 2012
Server 2012 R2
Server 2016
Server 2019
Server 2022
MultiPoint Server
Server Essentials
Specialized
Windows Preinstallation Environment
Windows IoT
Windows 365
Windows IoT
Embedded Compact
CE 5.0
Embedded CE 6.0
Embedded Compact 7
Embedded Automotive
Embedded Industry
Windows Mobile
Pocket PC 2000
Pocket PC 2002
Mobile 2003
Mobile 5.0
Mobile 6.0
Mobile 6.1
Mobile 6.5
Windows Phone
Phone 7
Phone 8
Phone 8.1
10 Mobile
Cancelled
Cairo
Nashville
Neptune
Odyssey
Related
Development
95
XP
Vista
10
11
Editions
XP
Vista
7
8
10
New features
XP
Vista
7
8
10
11
Removed features
XP
Vista
7
8
10
11
Version history
Phone
10
10 Mobile
11
Criticism
XP
Vista
10
Vista vs. XP

 List of versions
 Comparison
 Category

vteOperating systems by MicrosoftDesktop / Server
Microsoft Windows
3.0
3.1x
9x
NT
Ten
MS-DOS
MSX-DOS
Multitasking MS-DOS 4.0/4.1
MS-DOS 7
DOS/V
Z-DOS
OS/2
Xenix
Mobile
Nokia Asha platform
Nokia X platform
KIN OS
Windows Mobile
Windows Phone
Zune
Windows 10 Mobile
Embedded / IoT
Azure RTOS ThreadX
Azure Sphere
Windows Embedded Automotive
Windows Embedded Compact
Windows Embedded Industry
Windows IoT
Network
MS-Net
LAN Manager
SONiC
Others
Barrelfish
Bigtop
Cairo
CBL-Mariner
HomeOS
Midori
Singularity
Venus
Verve
Xbox system software

 List
 Category

vteMicrosoft
History
Outline
PeopleFounders
Bill Gates
Paul Allen
Board of directors
John W. Thompson (Chairman)
Satya Nadella (CEO)
Charles Noski
Helmut Panke
John W. Stanton
Reid Hoffman
Sandi Peterson
Penny Pritzker
Charles Scharf
Arne Sorenson
Padmasree Warrior
Senior leadership team
Satya Nadella (CEO)
Scott Guthrie
Amy Hood (CFO)
Brad Smith (CLO)
Harry Shum
Phil Spencer
Kathleen Hogan (CPO)
Corporate VPs
Joe Belfiore
Richard Rashid (SVP)
César Cernuda
Panos Panay (CVP)
ProductsHardware
Azure Kinect
HoloLens
LifeCam
LifeChat
Surface
Hub
Go
Laptop
Laptop Go
Pro
Studio
Duo
Neo
Xbox
Software
Microsoft 365
Dynamics
Open source software
Office
Power Platform
Servers
Visual Studio
Visual Studio Code
Windows
Xbox OS
Programming languages
BASIC
VB.NET
VBA
VBScript
Visual Basic
C#
C/AL a.k.a Navision Attain
F#
MVPL
Power Fx
PowerShell
Transact-SQL
TypeScript
Q#
Visual J#
Visual J++
Web properties
Azure
Bing
Docs
Channel 9
Developer Network
TechNet
GitHub
LinkedIn
LinkedIn Learning
MSN
Outlook.com
Store
Translator
CompanyConferences
Build
Ignite
Inspire
MIX
PDC
WinHEC
Divisions
Engineering groups
Mobile
Skype unit
Digital Crimes Unit
Garage
Press
Research
.NET Foundation
Outercurve Foundation
Xbox Game Studios
Estates
Microsoft Redmond campus
Microsoft Egypt
Microsoft India
Microsoft Japan
Microsoft Theater
Campaigns
Where do you want to go today? (1994)
Champagne (2002)
Mojave Experiment (2006)
I'm a PC (2008)
Scroogled (2012)
Criticism
Bundling of Microsoft Windows
Clippy
iLoo
Internet Explorer
Microsoft Bob
_NSAKEY
Windows
XP
Vista
10
Litigation
Alcatel-Lucent v. Microsoft
Apple v. Microsoft
European Union Microsoft competition case
Microsoft v. Lindows
Microsoft v. MikeRoweSoft
Microsoft v. Shah
United States v. Microsoft (2001 antitrust case)
Microsoft Ireland case
Acquisitions
6Wunderkinder
Access Software
Acompli
Altamira Software
AltspaceVR
aQuantive
Azyxxi
The Blue Ribbon SoundWorks
Beam
Bungie
Calista Technologies
Colloquis
Compulsion Games
Connectix
Consumers Software
Danger
Double Fine Productions
Farecast
FASA Studio
Fast Search & Transfer
Firefly
Forethought
GIANT Company Software
GitHub
GreenButton
Groove Networks
High Heat Major League Baseball
Hotmail
inXile Entertainment
Jellyfish.com
LinkedIn
LinkExchange
Lionhead Studios
Maluuba
Massive Incorporated
Metaswitch
Mobile Data Labs
Mojang Studios
Ninja Theory
Nokia Devices and Services
npm
Nuance Communications
Obsidian Entertainment
Onfolio
Pando Networks
Perceptive Pixel
Playground Games
PlaceWare
Powerset
Press Play
ProClarity
Rare
Revolution Analytics
RiskIQ
ScreenTonic
Secure Islands
Simplygon
Skype
Sunrise Atelier
SwiftKey
Winternals Software
Teleo
Telekinesys Research
Tellme Networks
Twisted Pixel Games
Undead Labs
Vermeer Technologies
Visio Corporation
Vivaty
VoloMetrix
VXtreme
WebTV Networks
Xamarin
Yammer
Yupi
ZeniMax Media

 Category

vteMicrosoft Windows componentsManagementtools
App Installer
Command Prompt
Control Panel
Applets
Device Manager
Disk Cleanup
Disk Defragmenter
Driver Verifier
DxDiag
Event Viewer
IExpress
Management Console
Netsh
Performance Monitor
Recovery Console
Resource Monitor
Settings
Sysprep
System Configuration
System File Checker
System Information
System Policy Editor
System Restore
Task Manager
Windows Error Reporting
Windows Ink
Windows Installer
PowerShell
Windows Update
Windows Insider
WinRE
WMI
Apps
3D Viewer
Alarms & Clock
Calculator
Calendar
Camera
Character Map
Cortana
Edge
Fax and Scan
Feedback Hub
Get Help
Magnifier
Mail
Maps
Messaging
Media Player
Windows 11
Movies & TV
Mobility Center
Money
News
Narrator
Notepad
OneDrive
OneNote
Paint
Paint 3D
People
Phone Link
Photos
Quick Assist
Snipping Tool
Speech Recognition
Skype
Sports
Sticky Notes
Store
Tips
Voice Recorder
Weather
WordPad
Xbox Console Companion
Shell
Action Center
Aero
AutoPlay
AutoRun
ClearType
Explorer
Search
Indexing Service
IFilter
Saved search
Namespace
Special folder
Start menu
Taskbar
Task View
Windows Spotlight
Windows XP visual styles
Services
Service Control Manager
BITS
CLFS
Multimedia Class Scheduler
Shadow Copy
Task Scheduler
Error Reporting
Wireless Zero Configuration
File systems
CDFS
DFS
exFAT
IFS
FAT
NTFS
Hard link
links
Mount Point
Reparse point
TxF
EFS
ReFS
UDF
Server
Domains
Active Directory
DNS
Group Policy
Roaming user profiles
Folder redirection
Distributed Transaction Coordinator
MSMQ
Windows Media Services
Active DRM Services
IIS
WSUS
SharePoint
Network Access Protection
PWS
DFS Replication
Print Services for UNIX
Remote Desktop Services
Remote Differential Compression
Remote Installation Services
Windows Deployment Services
System Resource Manager
Hyper-V
Server Core
Architecture
Architecture of Windows NT
Startup process
NT
NT 6
CSRSS
Desktop Window Manager
Portable Executable
EXE
DLL
Enhanced Write Filter
Graphics Device Interface
hal.dll
I/O request packet
Imaging Format
Kernel Transaction Manager
Library files
Logical Disk Manager
LSASS
MinWin
NTLDR
Ntoskrnl.exe
Object Manager
Open XML Paper Specification
Registry
Resource Protection
Security Account Manager
Server Message Block
Shadow Copy
SMSS
System Idle Process
USER
WHEA
Win32 console
Winlogon
WinUSB
Security
Security and Maintenance
AppLocker
BitLocker
Credential Guard
Data Execution Prevention
Family Safety
Kernel Patch Protection
Mandatory Integrity Control
Protected Media Path
User Account Control
User Interface Privilege Isolation
Windows Defender
Windows Firewall
Compatibility
COMMAND.COM
WoW64
Windows Subsystem for Linux
API
Active Scripting
WSH
VBScript
JScript
COM
ActiveX
ActiveX Document
COM Structured storage
DCOM
OLE
OLE Automation
Transaction Server
DirectX
.NET
Universal Windows Platform
Windows Mixed Reality
Windows Runtime
WinUSB
Games
Solitaire Collection
Surf
DiscontinuedGames
3D Pinball
Chess Titans
FreeCell
Hearts
InkBall
Hold 'Em
Purble Place
Spider Solitaire
Solitaire
Tinker
Apps
ActiveMovie
Anytime Upgrade
Address Book
Backup and Restore
Cardfile
CardSpace
CD Player
Chat
Contacts
Desktop Gadgets
Diagnostics
DriveSpace
DVD Maker
Easy Transfer
Fax
Food & Drink
Groove Music
Help and Support Center
Health & Fitness
HyperTerminal
Imaging
Internet Explorer
Journal
Media Center
Meeting Space
Messaging
Messenger
Mobile Device Center
Movie Maker
MSN Dial-up
NetMeeting
NTBackup
Outlook Express
Phone Companion
Photo Gallery
Photo Viewer
Program Manager
Steps Recorder
Syskey
Travel
WinHelp
Write
Others
ScanDisk
File Protection
Media Control Interface
Next-Generation Secure Computing Base
POSIX subsystem
HPFS
Interix
Video for Windows
Virtual DOS machine
Windows on Windows
Windows SideShow
Windows Services for UNIX
Windows System Assessment Tool
Windows To Go
WinFS
Spun off toMicrosoft Store
DVD Player
File Manager
Hover!
Mahjong
Minesweeper
Defunct
Pay

 Category
 List

vteOperating systemsGeneral
Advocacy
Comparison
Forensic engineering
History
List
Timeline
Usage share
User features comparison
Variants
Disk operating system
Distributed operating system
Embedded operating system
Hobbyist operating system
Just enough operating system
Mobile operating system
Network operating system
Object-oriented operating system
Real-time operating system
Supercomputer operating system
KernelArchitectures
Exokernel
Hybrid
Microkernel
Monolithic
vkernel
Rump kernel
Unikernel
Components
Device driver
Loadable kernel module
User space and kernel space
Process managementConcepts
Computer multitasking (Cooperative, Preemptive)
Context switch
Interrupt
IPC
Process
Process control block
Real-time
Thread
Time-sharing
Schedulingalgorithms
Fixed-priority preemptive
Multilevel feedback queue
Round-robin
Shortest job next
Memory management,resource protection
Bus error
General protection fault
Memory paging
Memory protection
Protection ring
Segmentation fault
Virtual memory
Storage access,file systems
Boot loader
Defragmentation
Device file
File attribute
Inode
Journal
Partition
Virtual file system
Virtual tape library
Supporting concepts
API
Computer network
HAL
Live CD
Live USB
Shell
CLI
GUI
3D GUI
NUI
TUI
VUI
ZUI
PXE

Authority control General
VIAF
1
WorldCat (via VIAF)
National libraries
France (data)
Germany
United States





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Microsoft_Windows&oldid=1084988049""
Categories: Microsoft Windows1985 softwareComputer-related introductions in 1985Computing platformsMicrosoft franchisesPersonal computersMicrosoft operating systemsOperating system familiesProducts introduced in 1985Hidden categories: Pages using the EasyTimeline extensionAll articles with dead external linksArticles with dead external links from December 2017Articles with permanently dead external linksWebarchive template wayback linksCS1 maint: url-statusArticles with short descriptionShort description is different from WikidataWikipedia indefinitely move-protected pagesWikipedia pages semi-protected against vandalismUse mdy dates from February 2022Articles containing potentially dated statements from October 2021All articles containing potentially dated statementsAll articles with unsourced statementsArticles with unsourced statements from September 2019Articles with unsourced statements from October 2015Articles with unsourced statements from July 2007Articles with unsourced statements from October 2021Wikipedia articles in need of updating from May 2020All Wikipedia articles in need of updatingPages using Sister project links with wikidata namespace mismatchPages using Sister project links with hidden wikidataOfficial website different in Wikidata and WikipediaArticles with VIAF identifiersArticles with BNF identifiersArticles with GND identifiersArticles with LCCN identifiersArticles with WorldCat-VIAF identifiers

"
9,Steve_Jobs,"





Steve Jobs

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
American business magnate (1955–2011)
This article is about the person. For other uses, see Steve Jobs (disambiguation).


Steve JobsBorn(1955-02-24)February 24, 1955San Francisco, California, U.S.DiedOctober 5, 2011(2011-10-05) (aged 56)Palo Alto, California, U.S.Resting placeAlta Mesa Memorial ParkEducationReed College (attended)OccupationEntrepreneurindustrial designermedia proprietorinvestorYears active1976–2011Known for
Pioneer of the personal computer revolution with Steve Wozniak
Co-creator of the Apple II, Macintosh, iPod, iPhone, iPad, and first Apple Stores
Title
Co-founder, chairman and CEO of Apple Inc.
Co-founder, primary investor and chairman of Pixar
Founder, chairman and CEO of NeXT
Board member of
The Walt Disney Company[1]
Apple Inc.
Spouse(s)Laurene Powell ​(m. 1991)​Partner(s)Chrisann Brennan (1972–1977)Children4, including LisaRelativesMona Simpson (sister)Bassma Al Jandaly (cousin)Malek Jandali (cousin)Signature
Steven Paul Jobs (February 24, 1955 – October 5, 2011) was an American entrepreneur, inventor, business magnate, media proprietor, and investor. He was the co-founder, chairman, and CEO of Apple; the chairman and majority shareholder of Pixar; a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT. He is widely recognized as a pioneer of the personal computer revolution of the 1970s and 1980s, along with his early business partner and fellow Apple co-founder Steve Wozniak.
Born in San Francisco to a Syrian father and a German-American mother, Jobs was adopted shortly after his birth. Jobs attended Reed College in 1972 before withdrawing that same year, and traveled through India in 1974 seeking enlightenment and studying Zen Buddhism. He and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer. Together, the duo gained fame and wealth a year later with the Apple II, one of the first highly successful mass-produced microcomputers. Jobs saw the commercial potential of the Xerox Alto in 1979, which was mouse-driven and had a graphical user interface (GUI). This led to the development of the unsuccessful Apple Lisa in 1983, followed by the breakthrough Macintosh in 1984, the first mass-produced computer with a GUI. The Macintosh introduced the desktop publishing industry in 1985 with the addition of the Apple LaserWriter, the first laser printer to feature vector graphics.
Jobs was forced out of Apple in 1985 after a long power struggle with the company's board and its then-CEO John Sculley. That same year, Jobs took a few Apple employees with him to found NeXT, a computer platform development company that specialized in computers for higher-education and business markets. In addition, he helped to develop the visual effects industry when he funded the computer graphics division of George Lucas's Lucasfilm in 1986. The new company was Pixar, which produced the first 3D computer animated feature film Toy Story (1995) and went on to become a major animation studio, producing over 20 films since.
Jobs became CEO of Apple in 1997, following the company's acquisition of NeXT. He was largely responsible for helping revive Apple, which had been on the verge of bankruptcy. He worked closely with English designer Jony Ive to develop a line of products that had larger cultural ramifications, beginning in 1997 with the ""Think different"" advertising campaign and leading to the Apple Store, App Store, iMac, iPad, iPod, iPhone, iTunes, and iTunes Store. In 2001, the original Mac OS was replaced with the completely new Mac OS X (now known as macOS), based on NeXT's NeXTSTEP platform, giving the OS a modern Unix-based foundation for the first time. Jobs was diagnosed with a pancreatic neuroendocrine tumor in 2003. He died of respiratory arrest related to the tumor at age 56 on October 5, 2011.

Contents

1 Background

1.1 Biological and adoptive families
1.2 Birth and early life


2 Childhood
3 Homestead High
4 Reed College
5 1972–1985

5.1 Pre-Apple
5.2 Apple (1976–1985)


6 1985–1997

6.1 NeXT computer
6.2 Pixar and Disney


7 1997–2011

7.1 Return to Apple


8 Health problems

8.1 Resignation
8.2 Death


9 Innovations and designs

9.1 Apple I
9.2 Apple II
9.3 Apple Lisa
9.4 Macintosh
9.5 NeXT Computer
9.6 iMac
9.7 iTunes
9.8 iPod
9.9 iPhone
9.10 iPad


10 Personal life

10.1 Marriage
10.2 Family
10.3 Philanthropy


11 Honors and awards
12 In popular culture
13 See also
14 References
15 External links


Background
Biological and adoptive families
Steven Paul Jobs was born in San Francisco, California, on February 24, 1955, the son of Joanne Carole Schieble and Abdulfattah Jandali (Arabic: عبد الفتاح الجندلي). He was adopted by Clara (née Hagopian) and Paul Reinhold Jobs.[2]
Jandali, Jobs's biological father, was Syrian and went by the name ""John"". He grew up in an Arab Muslim household in Homs, Syria.[3] While an undergraduate at the American University of Beirut in Lebanon, he was a student activist and spent time in prison for his political activities.[3] He pursued a PhD at the University of Wisconsin, where he met Schieble, an American Catholic of German and Swiss descent.[3][4] As a doctoral candidate, Jandali was a teaching assistant for a course Schieble was taking, although both were the same age.[5] Novelist Mona Simpson, Jobs's biological sister, noted that Schieble's parents were not happy that their daughter was dating a Muslim.[6] Walter Isaacson, author of the biography Steve Jobs, additionally states that Schieble's father ""threatened to cut her off completely"" if she continued the relationship.[4]
Jobs's adoptive father was a Coast Guard mechanic.[7] After leaving the Coast Guard, he married Hagopian, an American of Armenian descent, in 1946.[8] Their attempts to start a family were halted after Hagopian had an ectopic pregnancy, leading them to consider adoption in 1955.[7][8][9] Hagopian's parents were survivors of the Armenian Genocide.[10]

Birth and early life


""Of all the inventions of humans, the computer is going to rank near or at the top as history unfolds and we look back. It is the most awesome tool that we have ever invented. I feel incredibly lucky to be at exactly the right place in Silicon Valley, at exactly the right time, historically, where this invention has taken form.""


—Steve Jobs, 1995. From the documentary, Steve Jobs: The Lost Interview.[11]


Schieble became pregnant with Jobs in 1954, when she and Jandali spent the summer with his family in Homs. According to Jandali, Schieble deliberately did not involve him in the process: ""Without telling me, Joanne upped and left to move to San Francisco to have the baby without anyone knowing, including me.""[12]
Schieble gave birth to Jobs in San Francisco on February 24, 1955, and chose an adoptive couple for him that was ""Catholic, well-educated, and wealthy"",[13][14] but the couple later changed their mind.[13] Jobs was then placed with Paul and Clara Jobs, neither of whom had a college education, and Schieble refused to sign the adoption papers.[15] She then took the matter to court in an attempt to have her baby placed with a different family,[13] and only consented to releasing the baby to Paul and Clara after the couple pledged to pay for the boy's college education.[16] Jobs's cousin, Bassma Al Jandaly, maintains that Jobs's birth name was Abdul Lateef Jandali.[17]
In his youth, Steve's parents took him to a Lutheran church.[18] When Jobs was in high school, Clara admitted to his girlfriend, Chrisann Brennan, that she ""was too frightened to love [Steve] for the first six months of his life ... I was scared they were going to take him away from me. Even after we won the case, Steve was so difficult a child that by the time he was two I felt we had made a mistake. I wanted to return him.""[13] When Chrisann shared this comment with Steve, he stated that he was already aware,[13] and would later say he was deeply loved and indulged by Paul and Clara.[19][page needed] Many years later, Jobs's wife Laurene also noted that ""he felt he had been really blessed by having the two of them as parents.""[19][page needed] Jobs would become upset when Paul and Clara were referred to as his ""adoptive parents""; he regarded them as his parents ""1,000%"". With regard to his biological parents, Jobs referred to them as ""my sperm and egg bank. That's not harsh, it's just the way it was, a sperm bank thing, nothing more.""[7]

Childhood


""I always thought of myself as a humanities person as a kid, but I liked electronics… then I read something that one of my heroes, Edwin Land of Polaroid, said about the importance of people who could stand at the intersection of humanities and sciences, and I decided that's what I wanted to do.""


From Steve Jobs[20]


Paul Jobs worked in several jobs that included a try as a machinist,[21]
several other jobs,[22] and then ""back to work as a machinist.""
Paul and Clara adopted Jobs's sister Patricia in 1957[23] and by 1959 the family had moved to the Monta Loma neighborhood in Mountain View, California.[24] It was during this time that Paul built a workbench in his garage for his son in order to ""pass along his love of mechanics.""[25] Jobs, meanwhile, admired his father's craftsmanship ""because he knew how to build anything. If we needed a cabinet, he would build it. When he built our fence, he gave me a hammer so I could work with him ... I wasn't that into fixing cars ... but I was eager to hang out with my dad.""[25] By the time he was ten, Jobs was deeply involved in electronics and befriended many of the engineers who lived in the neighborhood.[26][page needed] He had difficulty making friends with children his own age, however, and was seen by his classmates as a ""loner.""[26][page needed]

 Childhood family home of Steve Jobs on Crist Drive in Los Altos, California, that served as the original site of Apple Computer. The home was added to a list of historic Los Altos sites in 2013.[27]
Jobs had difficulty functioning in a traditional classroom, tended to resist authority figures, frequently misbehaved, and was suspended a few times.[26][page needed] Clara had taught him to read as a toddler, and Jobs stated that he was ""pretty bored in school and [had] turned into a little terror... you should have seen us in the third grade, we basically destroyed the teacher.""[26][page needed] He frequently played pranks on others at Monta Loma Elementary School in Mountain View.[28] His father Paul (who was abused as a child) never reprimanded him, however, and instead blamed the school for not challenging his brilliant son.[28]
Jobs would later credit his fourth grade teacher, Imogene ""Teddy"" Hill, with turning him around: ""She taught an advanced fourth grade class and it took her about a month to get hip to my situation. She bribed me into learning. She would say, 'I really want you to finish this workbook. I'll give you five bucks if you finish it.' That really kindled a passion in me for learning things! I learned more that year than I think I learned in any other year in school. They wanted me to skip the next two years in grade school and go straight to junior high to learn a foreign language but my parents very wisely wouldn't let it happen.""[26][page needed] Jobs skipped the 5th grade and transferred to the 6th grade at Crittenden Middle School in Mountain View[26][page needed] where he became a ""socially awkward loner"".[29] Jobs was often ""bullied"" at Crittenden Middle, and in the middle of 7th grade, he gave his parents an ultimatum: they had to either take him out of Crittenden or he would drop out of school.[30]
Though the Jobs family was not well off, they used all their savings in 1967 to buy a new home, allowing Jobs to change schools.[26][page needed] The new house (a three-bedroom home on Crist Drive in Los Altos, California) was in the better Cupertino School District, Cupertino, California,[31] and was embedded in an environment that was even more heavily populated with engineering families than the Mountain View area was.[26][page needed] The house was declared a historic site in 2013, as it was the first site for Apple Computer;[27] as of 2013, it was owned by Jobs's sister, Patty, and occupied by his step-mother, Marilyn.[32]
When he was 13 in 1968, Jobs was given a summer job by Bill Hewlett (of Hewlett-Packard) after Jobs cold-called him to ask for parts for an electronics project.[26][page needed]

Homestead High
 Jobs's 1972 Homestead High School yearbook photo
The location of the Los Altos home meant that Jobs would be able to attend nearby Homestead High School, which had strong ties to Silicon Valley.[20] He began his first year there in late 1968 along with Bill Fernandez.[26][page needed] (Fernandez introduced Jobs to Steve Wozniak, and would later be Apple's first employee.) Neither Jobs nor Fernandez (whose father was a lawyer) came from engineering households and thus decided to enroll in John McCollum's ""Electronics 1.""[26][page needed] McCollum and the rebellious Jobs (who had grown his hair long and become involved in the growing counterculture) would eventually clash and Jobs began to lose interest in the class.[26][page needed]
He underwent a change during mid-1970: ""I got stoned for the first time; I discovered Shakespeare, Dylan Thomas, and all that classic stuff. I read Moby Dick and went back as a junior taking creative writing classes.""[26][page needed] Jobs also later noted to his official biographer that ""I started to listen to music a whole lot, and I started to read more outside of just science and technology—Shakespeare, Plato. I loved King Lear ... when I was a senior I had this phenomenal AP English class. The teacher was this guy who looked like Ernest Hemingway. He took a bunch of us snowshoeing in Yosemite.""[33] During his last two years at Homestead High, Jobs developed two different interests: electronics and literature.[33] These dual interests were particularly reflected during Jobs's senior year as his best friends were Wozniak and his first girlfriend, the artistic Homestead junior Chrisann Brennan.[citation needed]
In 1971 after Wozniak began attending University of California, Berkeley, Jobs would visit him there a few times a week. This experience led him to study in nearby Stanford University's student union. Jobs also decided that rather than join the electronics club, he would put on light shows with a friend for Homestead's avant-garde Jazz program. He was described by a Homestead classmate as ""kind of a brain and kind of a hippie ... but he never fit into either group. He was smart enough to be a nerd, but wasn't nerdy. And he was too intellectual for the hippies, who just wanted to get wasted all the time. He was kind of an outsider. In high school everything revolved around what group you were in, and if you weren't in a carefully defined group, you weren't anybody. He was an individual, in a world where individuality was suspect."" By his senior year in late 1971, he was taking freshman English class at Stanford and working on a Homestead underground film project with Chrisann Brennan.[26][page needed]
Around that time, Wozniak designed a low-cost digital ""blue box"" to generate the necessary tones to manipulate the telephone network, allowing free long-distance calls. Jobs decided then to sell them and split the profit with Wozniak. The clandestine sales of the illegal blue boxes went well and perhaps planted the seed in Jobs's mind that electronics could be both fun and profitable.[34] Jobs, in a 1994 interview, recalled that it took six months for him and Wozniak to figure out how to build the blue boxes.[35] Jobs later reflected that had it not been for Wozniak's blue boxes, ""there wouldn't have been an Apple"".[36] He states it showed them that they could take on large companies and beat them.[37][38]
By his senior year of high school, Jobs began using LSD.[39] He later recalled that on one occasion he consumed it in a wheat field outside Sunnyvale, and experienced ""the most wonderful feeling of my life up to that point"".[40] In mid-1972, after graduation and before leaving for Reed College, Jobs and Brennan rented a house from their other roommate, Al.[41]

Reed College


""I was interested in Eastern mysticism which hit the shores about then. At Reed there was a constant flow of people stopping by – from Timothy Leary and Richard Alpert, to Gary Snyder. There was a constant flow of intellectual questioning about the truth of life. That was the time when every college student in the country read Be Here Now and Diet for a Small Planet.""


—Steve Jobs[26][page needed]


In September 1972, Jobs enrolled at Reed College in Portland, Oregon.[42] He insisted on applying only to Reed although it was an expensive school that Paul and Clara could ill afford.[43] Jobs soon befriended Robert Friedland,[44] who was Reed's student body president at that time.[26][page needed] Brennan remained involved with Jobs while he was at Reed. He later asked her to come and live with him in a house he rented near the Reed campus, but she refused.
After just one semester, Jobs dropped out of Reed College without telling his parents.[45] Jobs later explained that he decided to drop out because he did not want to spend his parents' money on an education that seemed meaningless to him.[46] He continued to attend by auditing his classes,[46] which included a course on calligraphy that was taught by Robert Palladino. In a 2005 commencement speech at Stanford University, Jobs stated that during this period, he slept on the floor in friends' dorm rooms, returned Coke bottles for food money, and got weekly free meals at the local Hare Krishna temple. In that same speech, Jobs said: ""If I had never dropped in on that single calligraphy course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts.""[47]

1972–1985


I was lucky to get into computers when it was a very young and idealistic industry. There weren't many degrees offered in computer science, so people in computers were brilliant people from mathematics, physics, music, zoology, whatever. They loved it, and no one was really in it for the money [...] There are people around here who start companies just to make money, but the great companies, well, that's not what they're about.""


—Steve Jobs[48]


Pre-Apple
In February 1974, Jobs returned to his parents' home in Los Altos and began looking for a job.[49] He was soon hired by Atari, Inc. in Los Gatos, California, which gave him a job as a technician.[49][50] Back in 1973, Steve Wozniak designed his own version of the classic video game Pong and gave the board to Jobs. According to Wozniak, Atari only hired Jobs because he took the board down to the company, and they thought that he had built it himself.[51] Atari's cofounder Nolan Bushnell later described him as ""difficult but valuable"", pointing out that ""he was very often the smartest guy in the room, and he would let people know that.""[52]
During this period, Jobs and Brennan remained involved with each other while continuing to see other people. By early 1974, Jobs was living what Brennan describes as a ""simple life"" in a Los Gatos cabin, working at Atari, and saving money for his impending trip to India.[citation needed]
Jobs traveled to India in mid-1974[53] to visit Neem Karoli Baba[54] at his Kainchi ashram with his Reed friend (and eventual Apple employee) Daniel Kottke, in search of spiritual enlightenment. When they got to the Neem Karoli ashram, it was almost deserted because Neem Karoli Baba had died in September 1973.[50] Then they made a long trek up a dry riverbed to an ashram of Haidakhan Babaji.[50]
After seven months, Jobs left India[55] and returned to the US ahead of Daniel Kottke.[50] Jobs had changed his appearance; his head was shaved and he wore traditional Indian clothing.[56][57] During this time, Jobs experimented with psychedelics, later calling his LSD experiences ""one of the two or three most important things [he had] done in [his] life"".[58][59] He spent a period at the All One Farm, a commune in Oregon that was owned by Robert Friedland. Brennan joined him there for a period.[citation needed]
During this time period, Jobs and Brennan both became practitioners of Zen Buddhism through the Zen master Kōbun Chino Otogawa. Jobs was living in his parents' backyard toolshed, which he had converted into a bedroom.[citation needed] Jobs engaged in lengthy meditation retreats at the Tassajara Zen Mountain Center, the oldest Sōtō Zen monastery in the US.[60] He considered taking up monastic residence at Eihei-ji in Japan, and maintained a lifelong appreciation for Zen.[61]
In mid-1975, after returning to Atari, Jobs was assigned to create a circuit board for the arcade video game Breakout.[62] According to Bushnell, Atari offered US$100 for each TTL chip that was eliminated in the machine. Jobs had little specialized knowledge of circuit board design and made a deal with Wozniak to split the fee evenly between them if Wozniak could minimize the number of chips. Much to the amazement of Atari engineers, Wozniak reduced the TTL count to 46, a design so tight that it was impossible to reproduce on an assembly line.[63] According to Wozniak, Jobs told him that Atari gave them only $700 (instead of the $5,000 paid out), and that Wozniak's share was thus $350.[64] Wozniak did not learn about the actual bonus until ten years later, but said that if Jobs had told him about it and explained that he needed the money, Wozniak would have given it to him.[65]
Jobs and Wozniak attended meetings of the Homebrew Computer Club in 1975, which was a stepping stone to the development and marketing of the first Apple computer.[14]

Apple (1976–1985)
See also: History of Apple § 1975–1985: Jobs and Wozniak


""Basically Steve Wozniak and I invented the Apple because we wanted a personal computer. Not only couldn't we afford the computers that were on the market, those computers were impractical for us to use. We needed a Volkswagen. The Volkswagen isn't as fast or comfortable as other ways of traveling, but the VW owners can go where they want, when they want and with whom they want. The VW owners have personal control of their car.""


—Steve Jobs[26][page needed]


By March 1976, Wozniak completed the basic design of the Apple I computer and showed it to Jobs, who suggested that they sell it; Wozniak was at first skeptical of the idea but later agreed.[66] In April of that same year, Jobs, Wozniak, and administrative overseer Ronald Wayne founded Apple Computer Company (now called Apple Inc.) as a business partnership in Jobs's parents' Crist Drive home on April 1, 1976.[67] The operation originally started in Jobs's bedroom and later moved to the garage.[67][68] Wayne stayed only a short time, leaving Jobs and Wozniak as the active primary cofounders of the company.[69] The two decided on the name ""Apple"" after Jobs returned from the All One Farm commune in Oregon and told Wozniak about his time spent in the farm's apple orchard.[70] Jobs originally planned to produce bare printed circuit boards of the Apple I and sell them to computer hobbyists for $50 each.[71][72] To raise the money they needed to build the first batch of the circuit boards, Wozniak sold his HP scientific calculator and Jobs sold his Volkswagen van.[71][72] Later that year, computer retailer Paul Terrell purchased 50 fully assembled units of the Apple I from them for $500 each.[73][74] Eventually about 200 Apple I computers were produced in total.[75]

External image Jobs and Steve Wozniak with an Apple I circuit board, c. 1976.
A neighbor on Crist Drive recalled Jobs as an odd individual who would greet his clients ""with his underwear hanging out, barefoot and hippie-like"".[32] Another neighbor, Larry Waterland, who had just earned his PhD in chemical engineering at Stanford, recalled dismissing Jobs's budding business: ""'You punched cards, put them in a big deck,' he said about the mainframe machines of that time. 'Steve took me over to the garage. He had a circuit board with a chip on it, a DuMont TV set, a Panasonic cassette tape deck and a keyboard. He said, 'This is an Apple computer.' I said, 'You've got to be joking.' I dismissed the whole idea.'""[32] Jobs's friend from Reed College and India, Daniel Kottke, recalled that as an early Apple employee, he ""was the only person who worked in the garage ... Woz would show up once a week with his latest code. Steve Jobs didn't get his hands dirty in that sense."" Kottke also stated that much of the early work took place in Jobs's kitchen, where he spent hours on the phone trying to find investors for the company.[32]
They received funding from a then-semi-retired Intel product marketing manager and engineer Mike Markkula.[76] Scott McNealy, one of the cofounders of Sun Microsystems, said that Jobs broke a ""glass age ceiling"" in Silicon Valley because he'd created a very successful company at a young age.[38] Markkula brought Apple to the attention of Arthur Rock, which after looking at the crowded Apple booth at the Home Brew Computer Show, started with a $60,000 investment and went on the Apple board.[77] Jobs was not pleased when Markkula recruited Mike Scott from National Semiconductor in February 1977 to serve as the first president and CEO of Apple.[78][79]



""For what characterizes Apple is that its scientific staff always acted and performed like artists – in a field filled with dry personalities limited by the rational and binary worlds they inhabit, Apple's engineering teams had passion. They always believed that what they were doing was important and, most of all, fun. Working at Apple was never just a job; it was also a crusade, a mission, to bring better computer power to people. At its roots that attitude came from Steve Jobs. It was ""Power to the People"", the slogan of the sixties, rewritten in technology for the eighties and called Macintosh.""


—Jeffrey S. Young, 1987. From the book, Steve Jobs: The Journey is the Reward (published 1988).[26][page needed]


After Brennan returned from her own journey to India, she and Jobs fell in love again, as Brennan noted changes in him that she attributes to Kobun (whom she was also still following). It was also at this time that Jobs displayed a prototype Apple I computer for Brennan and his parents in their living room. Brennan notes a shift in this time period, where the two main influences on Jobs were Apple Inc. and Kobun. By early 1977, she and Jobs would spend time together at her home at Duveneck Ranch in Los Altos, which served as a hostel and environmental education center.
In April 1977, Jobs and Wozniak introduced the Apple II at the West Coast Computer Faire.[80] It is the first consumer product to have been sold by Apple Computer. Primarily designed by Wozniak, Jobs oversaw the development of its unusual case and Rod Holt developed the unique power supply.[81] During the design stage, Jobs argued that the Apple II should have two expansion slots, while Wozniak wanted eight. After a heated argument, Wozniak threatened that Jobs should ""go get himself another computer"". They later decided to go with eight slots.[82] The Apple II became one of the first highly successful mass-produced microcomputer products in the world.[83]
As Jobs became more successful with his new company, his relationship with Brennan grew more complex. In 1977, the success of Apple was now a part of their relationship, and Brennan, Daniel Kottke, and Jobs moved into a house near the Apple office in Cupertino.[citation needed] Brennan eventually took a position in the shipping department at Apple.[84] Brennan's relationship with Jobs deteriorated as his position with Apple grew, and she began to consider ending the relationship. In October 1977, Brennan was approached by Rod Holt, who asked her to take ""a paid apprenticeship designing blueprints for the Apples"".[citation needed] Both Holt and Jobs believed that it would be a good position for her, given her artistic abilities. Holt was particularly eager that she take the position and puzzled by her ambivalence toward it. Brennan's decision, however, was overshadowed by the fact that she realized she was pregnant and that Jobs was the father. It took her a few days to tell Jobs, whose face, according to Brennan ""turned ugly"" at the news. At the same time, according to Brennan, at the beginning of her third trimester, Jobs said to her: ""I never wanted to ask that you get an abortion. I just didn't want to do that.""[citation needed] He also refused to discuss the pregnancy with her.[85] Brennan turned down the internship and decided to leave Apple. She stated that Jobs told her ""If you give up this baby for adoption, you will be sorry"" and ""I am never going to help you.""[citation needed] According to Brennan, Jobs ""started to seed people with the notion that I slept around and he was infertile, which meant that this could not be his child."" A few weeks before she was due to give birth, Brennan was invited to deliver her baby at the All One Farm. She accepted the offer.[citation needed] When Jobs was 23 (the same age as his biological parents when they had him)[85] Brennan gave birth to her baby, Lisa Brennan, on May 17, 1978.[86] Jobs went there for the birth after he was contacted by Robert Friedland, their mutual friend and the farm owner. While distant, Jobs worked with her on a name for the baby, which they discussed while sitting in the fields on a blanket. Brennan suggested the name ""Lisa"" which Jobs also liked and notes that Jobs was very attached to the name ""Lisa"" while he ""was also publicly denying paternity."" She would discover later that during this time, Jobs was preparing to unveil a new kind of computer that he wanted to give a female name (his first choice was ""Claire"" after St. Clare). She also stated that she never gave him permission to use the baby's name for a computer and he hid the plans from her. Jobs also worked with his team to come up with the phrase, ""Local Integrated Software Architecture"" as an alternative explanation for the Apple Lisa.[87] Decades later, however, Jobs admitted to his biographer Walter Isaacson that ""obviously, it was named for my daughter"".[88]
When Jobs denied paternity, a DNA test established him as Lisa's father.[clarification needed] It required him to give Brennan $385 a month in addition to returning the welfare money she had received. Jobs gave her $500 a month at the time when Apple went public and Jobs became a millionaire. Later, Brennan agreed to give an interview with Michael Moritz for Time magazine for its Time Person of the Year special, released on January 3, 1983, in which she discussed her relationship with Jobs. Rather than name Jobs the Person of the Year, the magazine named the computer[clarification needed] the ""Machine of the Year"".[89] In the issue, Jobs questioned the reliability of the paternity test (which stated that the ""probability of paternity for Jobs, Steven... is 94.1%"").[90] Jobs responded by arguing that ""28% of the male population of the United States could be the father"".[90] Time also noted that ""the baby girl and the machine on which Apple has placed so much hope for the future share the same name: Lisa"".[90]
Jobs was worth over $1 million in 1978, when he was just 23 years old. His net worth grew to over $250 million by the time he was 25, according to estimates.[91] He was also one of the youngest ""people ever to make the Forbes list of the nation's richest people—and one of only a handful to have done it themselves, without inherited wealth"".[26][page needed]
In 1982, Jobs bought an apartment on the top two floors of The San Remo, a Manhattan building with a politically progressive reputation. Although he never lived there,[92] he spent years renovating it with the help of I. M. Pei. In 2003, he sold it to U2 singer Bono.
In 1983, Jobs lured John Sculley away from Pepsi-Cola to serve as Apple's CEO, asking, ""Do you want to spend the rest of your life selling sugared water, or do you want a chance to change the world?""[93]
In 1984, Jobs bought the Jackling House and estate, and resided there for a decade. After that, he leased it out for several years until 2000 when he stopped maintaining the house, allowing exposure to the weather to degrade it. In 2004, Jobs received permission from the town of Woodside to demolish the house in order to build a smaller contemporary styled one. After a few years in court, the house was finally demolished in 2011, a few months before he died.[94]

From left to right: Prototype of the original Macintosh from c. 1981 (at the Computer History Museum); Jobs with the Apple Macintosh, January 1984
Jobs began directing the development of the Macintosh in 1981, when he took over the project from early Apple employee Jef Raskin, who conceived the computer (Wozniak, who with Raskin had heavy influence over the program early on in its development, was on leave during this time due to an airplane crash earlier that year[95]).[96][97] On January 22, 1984, Apple aired a Super Bowl television commercial titled ""1984"", which ended with the words: ""On January 24th, Apple Computer will introduce Macintosh. And you'll see why 1984 won't be like 1984.""[98] On January 24, 1984, an emotional Jobs introduced the Macintosh to a wildly enthusiastic audience at Apple's annual shareholders meeting held in the Flint Auditorium;[99][100] Macintosh engineer Andy Hertzfeld described the scene as ""pandemonium"".[101] The Macintosh was based on The Lisa (and Xerox PARC's mouse-driven graphical user interface),[102][103] and it was widely acclaimed by the media with strong initial sales supporting it.[104][105] However, the computer's slow processing speed and limited range of available software led to a rapid sales decline in the second half of 1984.[104][105][106]

External video The Machine That Changed The World, The Paperback Computer; Interview with Steve Jobs, 1990, 50:08, May 14, 1990, WGBH Media Library & Archives[107]
Sculley's and Jobs's respective visions for the company greatly differed. The former favored open architecture computers like the Apple II, sold to education, small business, and home markets less vulnerable to IBM. Jobs wanted the company to focus on the closed architecture Macintosh as a business alternative to the IBM PC. President and CEO Sculley had little control over chairman of the board Jobs's Macintosh division; it and the Apple II division operated like separate companies, duplicating services.[108] Although its products provided 85 percent of Apple's sales in early 1985, the company's January 1985 annual meeting did not mention the Apple II division or employees. Many left, including Wozniak, who stated that the company had ""been going in the wrong direction for the last five years"" and sold most of his stock.[109] Despite being frustrated with the company's (including Jobs himself) dismissal of the Apple II employees in favor of the Macintosh, Wozniak left amicably and remained an honorary employee of Apple, maintaining a friendship with Jobs until his death.[110][111][112]

 Jobs (left) with software developer Wendell Brown in 1984
By early 1985, the Macintosh's failure to defeat the IBM PC became clear,[104][105] and it strengthened Sculley's position in the company. In May 1985, Sculley—encouraged by Arthur Rock—decided to reorganize Apple, and proposed a plan to the board that would remove Jobs from the Macintosh group and put him in charge of ""New Product Development"". This move would effectively render Jobs powerless within Apple.[26][page needed] In response, Jobs then developed a plan to get rid of Sculley and take over Apple. However, Jobs was confronted after the plan was leaked, and he said that he would leave Apple. The Board declined his resignation and asked him to reconsider. Sculley also told Jobs that he had all of the votes needed to go ahead with the reorganization. A few months later, on September 17, 1985, Jobs submitted a letter of resignation to the Apple Board. Five additional senior Apple employees also resigned and joined Jobs in his new venture, NeXT.[26][page needed]
The Macintosh's struggle continued after Jobs left Apple. Though marketed and received in fanfare, the expensive Macintosh was a hard sell.[113]: 308–309  In 1985, Bill Gates's then-developing company, Microsoft, threatened to stop developing Mac applications unless it was granted ""a license for the Mac operating system software. Microsoft was developing its graphical user interface ... for DOS, which it was calling Windows and didn't want Apple to sue over the similarities between the Windows GUI and the Mac interface.""[113]: 321  Sculley granted Microsoft the license which later led to problems for Apple.[113]: 321  In addition, cheap IBM PC clones that ran on Microsoft software and had a graphical user interface began to appear. Although the Macintosh preceded the clones, it was far more expensive, so ""through the late 1980s, the Windows user interface was getting better and better and was thus taking increasingly more share from Apple"".[113]: 322  Windows-based IBM-PC clones also led to the development of additional GUIs such as IBM's TopView or Digital Research's GEM,[113]: 322  and thus ""the graphical user interface was beginning to be taken for granted, undermining the most apparent advantage of the Mac...it seemed clear as the 1980s wound down that Apple couldn't go it alone indefinitely against the whole IBM-clone market.""[113]: 322 

1985–1997
NeXT computer
See also: NeXT
Following his resignation from Apple in 1985, Jobs founded NeXT Inc.[114] with $7 million. A year later he was running out of money, and he sought venture capital with no product on the horizon. Eventually, Jobs attracted the attention of billionaire Ross Perot, who invested heavily in the company.[115] The NeXT computer was shown to the world in what was considered Jobs's comeback event,[116] a lavish invitation-only gala launch event[117] that was described as a multimedia extravaganza.[118] The celebration was held at the Louise M. Davies Symphony Hall, San Francisco, California, on Wednesday, October 12, 1988. Steve Wozniak said in a 2013 interview that while Jobs was at NeXT he was ""really getting his head together"".[95]
NeXT workstations were first released in 1990 and priced at US$9,999. Like the Apple Lisa, the NeXT workstation was technologically advanced and designed for the education sector, but was largely dismissed as cost-prohibitive for educational institutions.[119] The NeXT workstation was known for its technical strengths, chief among them its object-oriented software development system. Jobs marketed NeXT products to the financial, scientific, and academic community, highlighting its innovative, experimental new technologies, such as the Mach kernel, the digital signal processor chip, and the built-in Ethernet port. Making use of a NeXT computer, English computer scientist Tim Berners-Lee invented the World Wide Web in 1990 at CERN in Switzerland.[120]
The revised, second generation NeXTcube was released in 1990. Jobs touted it as the first ""interpersonal"" computer that would replace the personal computer. With its innovative NeXTMail multimedia email system, NeXTcube could share voice, image, graphics, and video in email for the first time. ""Interpersonal computing is going to revolutionize human communications and groupwork"", Jobs told reporters.[121] Jobs ran NeXT with an obsession for aesthetic perfection, as evidenced by the development of and attention to NeXTcube's magnesium case.[122] This put considerable strain on NeXT's hardware division, and in 1993, after having sold only 50,000 machines, NeXT transitioned fully to software development with the release of NeXTSTEP/Intel.[123] The company reported its first yearly profit of $1.03 million in 1994.[124] In 1996, NeXT Software, Inc. released WebObjects, a framework for Web application development. After NeXT was acquired by Apple Inc. in 1997, WebObjects was used to build and run the Apple Store,[123] MobileMe services, and the iTunes Store.

Pixar and Disney
External video Presentation by Jobs on the future of computer-animated films, March 31, 1998, C-SPAN
In 1986, Jobs funded the spinout of The Graphics Group (later renamed Pixar) from Lucasfilm's computer graphics division for the price of $10 million, $5 million of which was given to the company as capital and $5 million of which was paid to Lucasfilm for technology rights.[125]
The first film produced by Pixar with its Disney partnership, Toy Story (1995), with Jobs credited as executive producer,[citation needed] brought financial success and critical acclaim to the studio when it was released. Over the course of Jobs's life, under Pixar's creative chief John Lasseter, the company produced box-office hits A Bug's Life (1998); Toy Story 2 (1999); Monsters, Inc. (2001); Finding Nemo (2003); The Incredibles (2004); Cars (2006); Ratatouille (2007); WALL-E (2008); Up (2009); Toy Story 3 (2010); and Cars 2 (2011). Brave (2012), Pixar's first film to be produced since Jobs's death, honored him with a tribute for his contributions to the studio.[126] Finding Nemo, The Incredibles, Ratatouille, WALL-E, Up, Toy Story 3 and Brave each received the Academy Award for Best Animated Feature, an award introduced in 2001.[127][128]
In 2003 and 2004, as Pixar's contract with Disney was running out, Jobs and Disney chief executive Michael Eisner tried but failed to negotiate a new partnership,[129] and in January 2004, Jobs announced that he would never deal with Disney again.[130] Pixar would seek a new partner to distribute its films after its contract expired.
In October 2005, Bob Iger replaced Eisner at Disney, and Iger quickly worked to mend relations with Jobs and Pixar. On January 24, 2006, Jobs and Iger announced that Disney had agreed to purchase Pixar in an all-stock transaction worth $7.4 billion. When the deal closed, Jobs became The Walt Disney Company's largest single shareholder with approximately seven percent of the company's stock.[131] Jobs's holdings in Disney far exceeded those of Eisner, who holds 1.7%, and of Disney family member Roy E. Disney, who until his 2009 death held about 1% of the company's stock and whose criticisms of Eisner—especially that he soured Disney's relationship with Pixar—accelerated Eisner's ousting. Upon completion of the merger, Jobs received 7% of Disney shares, and joined the board of directors as the largest individual shareholder.[131][132][133] Upon Jobs's death his shares in Disney were transferred to the Steven P. Jobs Trust led by Laurene Jobs.[134]
After Jobs's death Iger recalled in 2019 that many warned him about Jobs, ""that he would bully me and everyone else"". Iger wrote, ""Who wouldn't want Steve Jobs to have influence over how a company is run?"", and that as an active Disney board member ""he rarely created trouble for me. Not never but rarely"". He speculated that they would have seriously considered merging Disney and Apple had Jobs lived.[130] Floyd Norman, of Pixar, described Jobs as a ""mature, mellow individual"" who never interfered with the creative process of the filmmakers.[135] In early June 2014, Pixar cofounder and Walt Disney Animation Studios President Ed Catmull revealed that Jobs once advised him to ""just explain it to them until they understand"" in disagreements. Catmull released the book Creativity, Inc. in 2014, in which he recounts numerous experiences of working with Jobs. Regarding his own manner of dealing with Jobs, Catmull writes:[136][page needed]

In all the 26 years with Steve, Steve and I never had one of these loud verbal arguments and it's not my nature to do that. ... but we did disagree fairly frequently about things. ... I would say something to him and he would immediately shoot it down because he could think faster than I could. ... I would then wait a week ... I'd call him up and I give my counter argument to what he had said and he'd immediately shoot it down. So I had to wait another week, and sometimes this went on for months. But in the end one of three things happened. About a third of the time he said, 'Oh, I get it, you're right.' And that was the end of it. And it was another third of the time in which [I'd] say, 'Actually I think he is right.' The other third of the time, where we didn't reach consensus, he just let me do it my way, never said anything more about it.[137]
1997–2011
Return to Apple
See also: Apple Inc. § 1997–2007: Return to profitability
 Jobs onstage at Macworld Conference & Expo, San Francisco, January 11, 2005
In 1996, Apple announced that it would buy NeXT for $427 million. The deal was finalized in February 1997,[138] bringing Jobs back to the company he had cofounded. Jobs became de facto chief after then-CEO Gil Amelio was ousted in July 1997. He was formally named interim chief executive on September 16.[139] In March 1998, to concentrate Apple's efforts on returning to profitability, Jobs terminated a number of projects, such as Newton, Cyberdog, and OpenDoc. In the coming months, many employees developed a fear of encountering Jobs while riding in the elevator, ""afraid that they might not have a job when the doors opened. The reality was that Jobs's summary executions were rare, but a handful of victims was enough to terrorize a whole company.""[140] Jobs changed the licensing program for Macintosh clones, making it too costly for the manufacturers to continue making machines.
With the purchase of NeXT, much of the company's technology found its way into Apple products, most notably NeXTSTEP, which evolved into Mac OS X. Under Jobs's guidance, the company increased sales significantly with the introduction of the iMac and other new products; since then, appealing designs and powerful branding have worked well for Apple. At the 2000 Macworld Expo, Jobs officially dropped the ""interim"" modifier from his title at Apple and became permanent CEO.[141] Jobs quipped at the time that he would be using the title ""iCEO"".[142]
The company subsequently branched out, introducing and improving upon other digital appliances. With the introduction of the iPod portable music player, iTunes digital music software, and the iTunes Store, the company made forays into consumer electronics and music distribution. On June 29, 2007, Apple entered the cellular phone business with the introduction of the iPhone, a multi-touch display cell phone, which also included the features of an iPod and, with its own mobile browser, revolutionized the mobile browsing scene. While nurturing open-ended innovation, Jobs also reminded his employees that ""real artists ship"".[143]
Jobs had a public war of words with Dell Computer CEO Michael Dell, starting in 1987, when Jobs first criticized Dell for making ""un-innovative beige boxes"".[144] On October 6, 1997, at a Gartner Symposium, when Dell was asked what he would do if he ran the then-troubled Apple Computer company, he said: ""I'd shut it down and give the money back to the shareholders.""[145] Then, in 2006, Jobs sent an email to all employees when Apple's market capitalization rose above Dell's. It read:

Team, it turned out that Michael Dell wasn't perfect at predicting the future. Based on today's stock market close, Apple is worth more than Dell. Stocks go up and down, and things may be different tomorrow, but I thought it was worth a moment of reflection today. Steve.[146]
Jobs was both admired and criticized for his consummate skill at persuasion and salesmanship, which has been dubbed the ""reality distortion field"" and was particularly evident during his keynote speeches (colloquially known as ""Stevenotes"") at Macworld Expos and at Apple Worldwide Developers Conferences.[147]
Jobs usually went to work wearing a black long-sleeved mock turtleneck made by Issey Miyake, Levi's 501 blue jeans, and New Balance 991 sneakers.[148][149] Jobs told his biographer Walter Isaacson ""...he came to like the idea of having a uniform for himself, both because of its daily convenience (the rationale he claimed) and its ability to convey a signature style.""[148]
Jobs was a board member at Gap Inc. from 1999 to 2002.[150]

 Jobs and Bill Gates at the fifth D: All Things Digital conference (D5) in May 2007
In 2001, Jobs was granted stock options in the amount of 7.5 million shares of Apple with an exercise price of $18.30. It was alleged that the options had been backdated, and that the exercise price should have been $21.10. It was further alleged that Jobs had thereby incurred taxable income of $20,000,000 that he did not report, and that Apple overstated its earnings by that same amount. As a result, Jobs potentially faced a number of criminal charges and civil penalties. The case was the subject of active criminal and civil government investigations,[151] though an independent internal Apple investigation completed on December 29, 2006 found that Jobs was unaware of these issues and that the options granted to him were returned without being exercised in 2003.[152]
In 2005, Jobs responded to criticism of Apple's poor recycling programs for e-waste in the US by lashing out at environmental and other advocates at Apple's annual meeting in Cupertino in April. A few weeks later, Apple announced it would take back iPods for free at its retail stores. The Computer TakeBack Campaign responded by flying a banner from a plane over the Stanford University graduation at which Jobs was the commencement speaker. The banner read ""Steve, don't be a mini-player—recycle all e-waste.""
In 2006, he further expanded Apple's recycling programs to any US customer who buys a new Mac. This program includes shipping and ""environmentally friendly disposal"" of their old systems.[153] The success of Apple's unique products and services provided several years of stable financial returns, propelling Apple to become the world's most valuable publicly traded company in 2011.[154]
Jobs was perceived as a demanding perfectionist[155][156] who always aspired to position his businesses and their products at the forefront of the information technology industry by foreseeing and setting innovation and style trends. He summed up this self-concept at the end of his keynote speech at the Macworld Conference and Expo in January 2007, by quoting ice hockey player Wayne Gretzky:

There's an old Wayne Gretzky quote that I love. ""I skate to where the puck is going to be, not where it has been."" And we've always tried to do that at Apple. Since the very, very beginning. And we always will.[157]
 Jobs demonstrating the iPhone 4 to Russian President Dmitry Medvedev on June 23, 2010
On July 1, 2008, a US$7 billion class action suit was filed against several members of the Apple board of directors for revenue lost because of alleged securities fraud.[158][159]
In a 2011 interview with biographer Walter Isaacson, Jobs revealed that he had met with US President Barack Obama, complained about the nation's shortage of software engineers, and told Obama that he was ""headed for a one-term presidency"".[160] Jobs proposed that any foreign student who got an engineering degree at a US university should automatically be offered a green card. After the meeting, Jobs commented, ""The president is very smart, but he kept explaining to us reasons why things can't get done . . . . It infuriates me.""[160]

Health problems
In October 2003, Jobs was diagnosed with cancer. In mid 2004, he announced to his employees that he had a cancerous tumor in his pancreas.[161] The prognosis for pancreatic cancer is usually very poor;[162] Jobs stated that he had a rare, much less aggressive type, known as islet cell neuroendocrine tumor.[161]
Despite his diagnosis, Jobs resisted his doctors' recommendations for medical intervention for nine months,[163] instead relying on alternative medicine to thwart the disease. According to Harvard researcher Ramzi Amri, his choice of alternative treatment ""led to an unnecessarily early death"". Other doctors agree that Jobs's diet was insufficient to address his disease. However, cancer researcher and alternative medicine critic David Gorski wrote that ""it's impossible to know whether and by how much he might have decreased his chances of surviving his cancer through his flirtation with woo. My best guess was that Jobs probably only modestly decreased his chances of survival, if that.""[164][165] Barrie R. Cassileth, the chief of Memorial Sloan Kettering Cancer Center's integrative medicine department,[166] on the other hand, said, ""Jobs's faith in alternative medicine likely cost him his life ... He had the only kind of pancreatic cancer that is treatable and curable ... He essentially committed suicide.""[167] According to Jobs's biographer, Walter Isaacson, ""for nine months he refused to undergo surgery for his pancreatic cancer – a decision he later regretted as his health declined"".[168] ""Instead, he tried a vegan diet, acupuncture, herbal remedies, and other treatments he found online, and even consulted a psychic. He was also influenced by a doctor who ran a clinic that advised juice fasts, bowel cleansings and other unproven approaches, before finally having surgery in July 2004.""[169] He underwent a pancreaticoduodenectomy (or ""Whipple procedure"") that appeared to remove the tumor successfully.[170][171] Jobs did not receive chemotherapy or radiation therapy.[161][172] During Jobs's absence, Tim Cook, head of worldwide sales and operations at Apple, ran the company.[161]
As of January 2006[update], only Jobs's wife, his doctors, and Iger and his wife knew that his cancer had returned. Jobs told Iger privately that he hoped to live to see his son Reed's high school graduation in 2010.[130] In early August 2006, Jobs delivered the keynote for Apple's annual Worldwide Developers Conference. His ""thin, almost gaunt"" appearance and unusually ""listless"" delivery,[173][174] together with his choice to delegate significant portions of his keynote to other presenters, inspired a flurry of media and internet speculation about the state of his health.[175] In contrast, according to an Ars Technica journal report, Worldwide Developers Conference (WWDC) attendees who saw Jobs in person said he ""looked fine"".[176] Following the keynote, an Apple spokesperson said that ""Steve's health is robust.""[177]
Two years later, similar concerns followed Jobs's 2008 WWDC keynote address.[178] Apple officials stated that Jobs was victim to a ""common bug"" and was taking antibiotics,[179] while others surmised his cachectic appearance was due to the Whipple procedure.[172] During a July conference call discussing Apple earnings, participants responded to repeated questions about Jobs's health by insisting that it was a ""private matter"". Others said that shareholders had a right to know more, given Jobs's hands-on approach to running his company.[180][181] Based on an off-the-record phone conversation with Jobs, The New York Times reported, ""While his health problems amounted to a good deal more than 'a common bug', they weren't life-threatening and he doesn't have a recurrence of cancer.""[182]
On August 28, 2008, Bloomberg mistakenly published a 2500-word obituary of Jobs in its corporate news service, containing blank spaces for his age and cause of death. News carriers customarily stockpile up-to-date obituaries to facilitate news delivery in the event of a well-known figure's death. Although the error was promptly rectified, many news carriers and blogs reported on it,[183] intensifying rumors concerning Jobs's health.[184] Jobs responded at Apple's September 2008 Let's Rock keynote by paraphrasing Mark Twain: ""Reports of my death are greatly exaggerated.""[185][186] At a subsequent media event, Jobs concluded his presentation with a slide reading ""110/70"", referring to his blood pressure, stating he would not address further questions about his health.[187]
On December 16, 2008, Apple announced that marketing vice-president Phil Schiller would deliver the company's final keynote address at the Macworld Conference and Expo 2009, again reviving questions about Jobs's health.[188][189] In a statement given on January 5, 2009, on Apple.com, Jobs said that he had been suffering from a ""hormone imbalance"" for several months.[190][191]
On January 14, 2009, Jobs wrote in an internal Apple memo that in the previous week he had ""learned that my health-related issues are more complex than I originally thought"".[192] He announced a six-month leave of absence until the end of June 2009, to allow him to better focus on his health. Tim Cook, who previously acted as CEO in Jobs's 2004 absence, became acting CEO of Apple, with Jobs still involved with ""major strategic decisions"".[192]
In 2009, Tim Cook offered a portion of his liver to Jobs, since both share a rare blood type and the donor liver can regenerate tissue after such an operation. Jobs yelled, ""I'll never let you do that. I'll never do that.""[193]
In April 2009, Jobs underwent a liver transplant at Methodist University Hospital Transplant Institute in Memphis, Tennessee.[194][195][196] Jobs's prognosis was described as ""excellent"".[194]

Resignation
On January 17, 2011, a year and a half after Jobs returned to work following the liver transplant, Apple announced that he had been granted a medical leave of absence. Jobs announced his leave in a letter to employees, stating his decision was made ""so he could focus on his health"". As it did at the time of his 2009 medical leave, Apple announced that Tim Cook would run day-to-day operations and that Jobs would continue to be involved in major strategic decisions at the company.[197][198] While on leave, Jobs appeared at the iPad 2 launch event on March 2, the WWDC keynote introducing iCloud on June 6, and before the Cupertino City Council on June 7.[199]
On August 24, 2011, Jobs announced his resignation as Apple's CEO, writing to the board, ""I have always said if there ever came a day when I could no longer meet my duties and expectations as Apple's CEO, I would be the first to let you know. Unfortunately, that day has come.""[200] Jobs became chairman of the board and named Tim Cook as his successor as CEO.[201][202] Jobs continued to work for Apple until the day before his death six weeks later.[203][204][205]


Death
 Flags flying at half-staff outside Apple HQ in Cupertino, on the evening of Jobs's death
Jobs died at his Palo Alto, California, home around 3 p.m. (PDT) on October 5, 2011, due to complications from a relapse of his previously treated islet-cell pancreatic neuroendocrine tumor,[14][206][207] which resulted in respiratory arrest.[208] He had lost consciousness the day before and died with his wife, children, and sisters at his side.[209] His sister, Mona Simpson, described his death thus: ""Steve's final words, hours earlier, were monosyllables, repeated three times. Before embarking, he'd looked at his sister Patty, then for a long time at his children, then at his life's partner, Laurene, and then over their shoulders past them. Steve's final words were: 'Oh wow. Oh wow. Oh wow.'"" He then lost consciousness and died several hours later.[209] A small private funeral was held on October 7, 2011, the details of which, out of respect for Jobs's family, were not made public.[210]
Apple[211] and Pixar each issued announcements of his death.[212] Apple announced on the same day that they had no plans for a public service, but were encouraging ""well-wishers"" to send their remembrance messages to an email address created to receive such messages.[213] Apple and Microsoft both flew their flags at half-staff throughout their respective headquarters and campuses.[214][215]
Bob Iger ordered all Disney properties, including Walt Disney World and Disneyland, to fly their flags at half-staff from October 6 to 12, 2011.[216] For two weeks  following his death, Apple displayed on its corporate Web site a simple page that showed Jobs's name and lifespan next to his grayscale portrait.[217][218][219] On October 19, 2011, Apple employees held a private memorial service for Jobs on the Apple campus in Cupertino. Jobs's widow, Laurene, was in attendance, as well as Cook, Bill Campbell, Norah Jones, Al Gore, and Coldplay.[220] Some of Apple's retail stores closed briefly so employees could attend the memorial. A video of the service was uploaded to Apple's website.[220]
California Governor Jerry Brown declared Sunday, October 16, 2011, to be ""Steve Jobs Day"".[221] On that day, an invitation-only memorial was held at Stanford University. Those in attendance included Apple and other tech company executives, members of the media, celebrities, close friends of Jobs, and politicians, along with Jobs's family. Bono, Yo-Yo Ma, and Joan Baez performed at the service, which lasted longer than an hour. The service was highly secured, with guards at all of the university's gates, and a helicopter flying overhead from an area news station.[222][223] Each attendee was given a small brown box as a ""farewell gift"" from Jobs. The box contained a copy of the Autobiography of a Yogi by Paramahansa Yogananda.[224]
Childhood friend and fellow Apple co-founder Steve Wozniak,[225] former owner of what would become Pixar, George Lucas,[226] former rival, Microsoft co-founder Bill Gates,[227] and President Barack Obama[228] all offered statements in response to his death.
Per his request, Jobs is buried in an unmarked grave at Alta Mesa Memorial Park, the only nonsectarian cemetery in Palo Alto.[229][230]
On October 7, 2021, Apple released a commemorative YouTube video on the tenth anniversary of Jobs's passing.[231]

Innovations and designs
Jobs's design aesthetic was influenced by philosophies of Zen and Buddhism. In India, he experienced Buddhism while on his seven-month spiritual journey,[232] and his sense of intuition was influenced by the spiritual people with whom he studied.[232] He also learned from many references and sources, such as modernist architectural style of Joseph Eichler,[citation needed] and the industrial designs of Richard Sapper[233] and Dieter Rams.[citation needed]
According to Apple co-founder Steve Wozniak, ""Steve didn't ever code. He wasn't an engineer and he didn't do any original design...""[234][235] Daniel Kottke, one of Apple's earliest employees and a college friend of Jobs's, stated: ""Between Woz and Jobs, Woz was the innovator, the inventor. Steve Jobs was the marketing person.""[236]
He is listed as either primary inventor or co-inventor in 346 United States patents or patent applications related to a range of technologies from actual computer and portable devices to user interfaces (including touch-based), speakers, keyboards, power adapters, staircases, clasps, sleeves, lanyards and packages. Jobs's contributions to most of his patents were to ""the look and feel of the product"". His industrial design chief Jonathan Ive had his name along with Jobs's name for 200 of the patents.[237] Most of these are design patents (specific product designs; for example, Jobs listed as primary inventor in patents for both original and lamp-style iMacs, as well as PowerBook G4 Titanium) as opposed to utility patents (inventions).[238][239] He has 43 issued US patents on inventions.[238] The patent on the Mac OS X Dock user interface with ""magnification"" feature was issued the day before he died.[240] Although Jobs had little involvement in the engineering and technical side of the original Apple computers,[235] Jobs later used his CEO position to directly involve himself with product design.[241]
Involved in many projects throughout his career was his long-time marketing executive and confidant Joanna Hoffman, known as one of the few employees at Apple and NeXT who could successfully stand up to Jobs while also engaging with him.[242]
Even while terminally ill in the hospital, Jobs sketched new devices that would hold the iPad in a hospital bed.[209] He also despised the oxygen monitor on his finger, and suggested ways to revise the design for simplicity.[243]
Since his death, the former Apple CEO has won 141 patents, more than most inventors win during their lifetimes. Currently, Jobs holds over 450 patents.[244]

Apple I
Main article: Apple I
Although entirely designed by Steve Wozniak, Jobs had the idea of selling the desktop computer, which led to the formation of Apple Computer in 1976. Both Jobs and Wozniak constructed several of the first Apple I prototypes by hand, and sold some of their belongings in order to do so. Eventually, 200 units were produced.[75]

Apple II
 An Apple II with an external modem, designed primarily by Wozniak
Main article: Apple II series
The Apple II is an 8-bit home computer, one of the world's first highly successful mass-produced microcomputer products,[83] designed primarily by Wozniak (though Jobs oversaw the development of the Apple II's unusual case[245] and Rod Holt developed the unique power supply[81]). It was introduced in 1977 at the West Coast Computer Faire by Jobs and Wozniak and was the first consumer product sold by Apple.

Apple Lisa
Main article: Apple Lisa
The Lisa is a personal computer designed by Apple during the early 1980s. It was the first personal computer to offer a graphical user interface in a machine aimed at individual business users. Development of the Lisa began in 1978.[246] The Lisa sold poorly, with only 100,000 units sold.[247]
In 1982, after Jobs was forced out of the Lisa project,[248] he joined the Macintosh project. The Macintosh is not a direct descendant of Lisa, although there are obvious similarities between the systems. The final revision, the Lisa 2/10, was modified and sold as the Macintosh XL.[249]

Macintosh
 Jobs holding up a MacBook Air at the MacWorld Conference & Expo in 2008
Main article: Macintosh
Once he joined the original Macintosh team, Jobs took over the project after Wozniak had experienced a traumatic airplane accident and temporarily left the company.[95] Jobs introduced the Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral graphical user interface and mouse.[250] This first model was later renamed to ""Macintosh 128k"" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple's same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of ""Mac"", though the product family has been nicknamed ""Mac"" or ""the Mac"" since the development of the first model. The Macintosh was introduced by a US$1.5 million Ridley Scott television commercial, ""1984"".[251] It most notably aired during the third quarter of Super Bowl XVIII on January 22, 1984, and some people consider the ad a ""watershed event""[252] and a ""masterpiece"".[253] Regis McKenna called the ad ""more successful than the Mac itself"".[254] ""1984"" uses an unnamed heroine to represent the coming of the Macintosh (indicated by a Picasso-style picture of the computer on her white tank top) as a means of saving humanity from the conformity of IBM's attempts to dominate the computer industry. The ad alludes to George Orwell's novel, Nineteen Eighty-Four, which describes a dystopian future ruled by a televised ""Big Brother.""[255][256]
The Macintosh, however, was expensive, which hindered its ability to be competitive in a market already dominated by the Commodore 64 for consumers, as well as the IBM Personal Computer and its accompanying clone market for businesses.[257] Macintosh systems still found success in education and desktop publishing and kept Apple as the second-largest PC manufacturer for the next decade.

NeXT Computer
Main article: NeXT Computer
After Jobs was forced out of Apple in 1985, he started NeXT, a workstation computer company. The NeXT Computer was introduced in 1988 at a lavish launch event. Using the NeXT Computer, Tim Berners-Lee created the world's first web browser, the WorldWideWeb. The NeXT Computer's operating system, named NeXTSTEP, begat Darwin, which is now the foundation of most of Apple's products such as Macintosh's macOS and iPhone's iOS.[258][259]

iMac
 The original iMac, introduced in 1998, was the first consumer-facing Apple product to debut under Jobs's return.
Main article: iMac
Apple iMac G3 was introduced in 1998 and its innovative design was directly the result of Jobs's return to Apple. Apple boasted ""the back of our computer looks better than the front of anyone else's.""[260] Described as ""cartoonlike"", the first iMac, clad in Bondi Blue plastic, was unlike any personal computer that came before. In 1999, Apple introduced the Graphite gray Apple iMac and since has varied the shape, color and size considerably while maintaining the all-in-one design. Design ideas were intended to create a connection with the user such as the handle and a ""breathing"" light effect when the computer went to sleep.[261] The Apple iMac sold for $1,299 at that time. The iMac also featured forward-thinking changes, such as eschewing the floppy disk drive and moving exclusively to USB for connecting peripherals. This latter change resulted, through the iMac's success, in the interface being popularized among third-party peripheral makers—as evidenced by the fact that many early USB peripherals were made of translucent plastic (to match the iMac design).[262]

iTunes
Main article: iTunes
iTunes is a media player, media library, online radio broadcaster, and mobile device management application developed by Apple. It is used to play, download, and organize digital audio and video (as well as other types of media available on the iTunes Store) on personal computers running the macOS and Microsoft Windows operating systems. The iTunes Store is also available on the iPod Touch, iPhone, and iPad.
Through the iTunes Store, users can purchase and download music, music videos, television shows, audiobooks, podcasts, movies, and movie rentals in some countries, and ringtones, available on the iPhone and iPod Touch (fourth generation onward). Application software for the iPhone, iPad and iPod Touch can be downloaded from the App Store.

iPod
Main article: iPod
The first generation of iPod was released October 23, 2001. The major innovation of the iPod was its small size achieved by using a 1.8"" hard drive compared to the 2.5"" drives common to players at that time. The capacity of the first generation iPod ranged from 5 GB to 10 GB.[263] The iPod sold for US$399 and more than 100,000 iPods were sold before the end of 2001. The introduction of the iPod resulted in Apple becoming a major player in the music industry.[264] Also, the iPod's success prepared the way for the iTunes music store and the iPhone.[265] After the first few generations of iPod, Apple released the touchscreen iPod Touch, the reduced-size iPod Mini and iPod Nano, and the screenless iPod Shuffle in the following years.[264]

iPhone
 Jobs unveiling the iPhone at MacWorld Conference & Expo on January 9, 2007
Main article: iPhone
Apple began work on the first iPhone in 2005 and the first iPhone was released on June 29, 2007. The iPhone created such a sensation that a survey indicated six out of ten Americans were aware of its release. Time declared it ""Invention of the Year"" for 2007 and included it in the All-TIME 100 Gadgets list in 2010, in the category of Communication[266].[267] The completed iPhone had multimedia capabilities and functioned as a quad-band touch screen smartphone.[268] A year later, the iPhone 3G was released in July 2008 with three key features: support for GPS, 3G data and tri-band UMTS/HSDPA. In June 2009, the iPhone 3GS, whose improvements included voice control, a better camera, and a faster processor, was introduced by Phil Schiller.[269] The iPhone 4 was thinner than previous models, had a five megapixel camera capable of recording video in 720p HD, and added a secondary front-facing camera for video calls.[270] A major feature of the iPhone 4S, introduced in October 2011, was Siri, a virtual assistant capable of voice recognition.[267]

iPad
 Jobs introducing the iPad in San Francisco on January 27, 2010
Main article: iPad
The iPad is an iOS-based line of tablet computers designed and marketed by Apple. The first iPad was released on April 3, 2010. The user interface is built around the device's multi-touch screen, including a virtual keyboard. The iPad includes built-in Wi-Fi and cellular connectivity on select models. As of April 2015[update], more than 250 million iPads have been sold.[271]

Personal life
 Jobs's house in Palo Alto
 Jobs's house, as viewed from an adjacent sidewalk. Abundant fruit trees are visible next to the house.
Marriage
In 1989, Jobs first met his future wife, Laurene Powell, when he gave a lecture at the Stanford Graduate School of Business, where she was a student. Soon after the event, he stated that Laurene ""was right there in the front row in the lecture hall, and I couldn't take my eyes off of her ... kept losing my train of thought, and started feeling a little giddy.""[19][page needed] After the lecture, Jobs met up with her in the parking lot and invited her out to dinner. From that point forward, they were together, with a few minor exceptions, for the rest of his life.[19][page needed]
Jobs proposed on New Year's Day 1990 with ""a fistful of freshly picked wildflowers"".[19][page needed] They married on March 18, 1991, in a Buddhist ceremony at the Ahwahnee Hotel in Yosemite National Park.[19][page needed] Fifty people, including Jobs's father, Paul, and his sister Mona, attended. The ceremony was conducted by Jobs's guru, Kobun Chino Otogawa. The vegan wedding cake was in the shape of Yosemite's Half Dome, and the wedding ended with a hike (during which Laurene's brothers had a snowball fight). Jobs is reported to have said to Mona: ""You see, Mona [...], Laurene is descended from Joe Namath, and we're descended from John Muir.""[272]
Jobs's and Powell's first child, Reed, was born in September 1991.[273] Jobs's father, Paul, died a year and a half later, on March 5, 1993. Jobs's childhood home remains a tourist attraction and is currently owned by his stepmother (Paul's second wife), Marilyn Jobs.[274]
Jobs and Powell had two more children, Erin, born in August 1995, and Eve, born in May 1998.[273] The family lived in Palo Alto, California.[275] A journalist who grew up locally remembered him as owning the house with ""the scariest [Halloween] decorations in Palo Alto ... I don't remember seeing him. I was busy being terrified.""[276]
Although a billionaire, Jobs made it known that, like Bill Gates, he had stipulated that most of his monetary fortune would not be left to his children.[277][278] These technology leaders also had in common another family-related area: both men limited their children's access, age appropriate, to social media, computer games and the Internet.[279][280]

Family
Chrisann Brennan notes that after Jobs was forced out of Apple, ""he apologized many times over for his behavior"" towards her and Lisa. She also states that Jobs ""said that he never took responsibility when he should have, and that he was sorry"".[281] By this time, Jobs had developed a strong relationship with Lisa and when she was nine, Jobs had her name on her birth certificate changed from ""Lisa Brennan"" to ""Lisa Brennan-Jobs"".[13][page needed] In addition, Jobs and Brennan developed a working relationship to co-parent Lisa, a change Brennan credits to the influence of his newly found biological sister, Mona Simpson (who worked to repair the relationship between Lisa and Jobs).[13][page needed] Jobs found Mona after first finding his birth mother, Joanne Schieble Simpson, shortly after he left Apple.[282]
Jobs did not contact his birth family during his adoptive mother Clara's lifetime, however. He would later tell his official biographer Walter Isaacson: ""I never wanted [Paul and Clara] to feel like I didn't consider them my parents, because they were totally my parents [...] I loved them so much that I never wanted them to know of my search, and I even had reporters keep it quiet when any of them found out.""[282] However, in 1986, when Jobs was 31, Clara was diagnosed with lung cancer. He began to spend a great deal of time with her and learned more details about her background and his adoption, information that motivated him to find his biological mother. Jobs found on his birth certificate the name of the San Francisco doctor to whom Schieble had turned when she was pregnant. Although the doctor did not help Jobs while he was alive, he left a letter for Jobs to be opened upon his death. As he died soon afterwards, Jobs was given the letter which stated that ""his mother had been an unmarried graduate student from Wisconsin named Joanne Schieble.""[282]
Jobs only contacted Schieble after Clara died in early 1986 and after he received permission from his father, Paul. In addition, out of respect for Paul, he asked the media not to report on his search.[283] Jobs stated that he was motivated to find his birth mother out of both curiosity and a need ""to see if she was okay and to thank her, because I'm glad I didn't end up as an abortion. She was twenty-three and she went through a lot to have me.""[284] Schieble was emotional during their first meeting (though she wasn't familiar with the history of Apple or Jobs's role in it) and told him that she had been pressured into signing the adoption papers. She said that she regretted giving him up and repeatedly apologized to him for it. Jobs and Schieble would develop a friendly relationship throughout the rest of his life and would spend Christmas together.[285]
During this first visit, Schieble told Jobs that he had a sister, Mona, who was not aware that she had a brother.[284] Schieble then arranged for them to meet in New York where Mona worked. Her first impression of Jobs was that ""he was totally straightforward and lovely, just a normal and sweet guy.""[286] Simpson and Jobs then went for a long walk to get to know each other.[286] Jobs later told his biographer that ""Mona was not completely thrilled at first to have me in her life and have her mother so emotionally affectionate toward me ... As we got to know each other, we became really good friends, and she is my family. I don't know what I'd do without her. I can't imagine a better sister. My adopted sister, Patty, and I were never close.""[286]



""I grew up as an only child, with a single mother. Because we were poor and because I knew my father had emigrated from Syria, I imagined he looked like Omar Sharif. I hoped he would be rich and kind and would come into our lives (and our not-yet-furnished apartment) and help us. Later, after I'd met my father, I tried to believe he'd changed his number and left no forwarding address because he was an idealistic revolutionary, plotting a new world for the Arab people. Even as a feminist, my whole life I'd been waiting for a man to love, who could love me. For decades, I'd thought that man would be my father. When I was 25, I met that man, and he was my brother.""


—Mona Simpson[209]


Jobs then learned his family history. Six months after he was given up for adoption, Schieble's father died, she wed Jandali, and they had a daughter, Mona.[3][287] Jandali states that after finishing his PhD he returned to Syria to work and that it was during this period that Schieble left him[3] (they divorced in 1962).[20] He also states that after the divorce he lost contact with Mona for a period of time: I also bear the responsibility for being away from my daughter when she was four years old, as her mother divorced me when I went to Syria, but we got back in touch after 10 years. We lost touch again when her mother moved and I didn't know where she was, but since 10 years ago we've been in constant contact, and I see her three times a year. I organized a trip for her last year to visit Syria and Lebanon and she went with a relative from Florida.[3] A few years later, Schieble married an ice skating teacher, George Simpson.[287] Mona Jandali took her stepfather's last name and thus became Mona Simpson. In 1970, after divorcing her second husband, Schieble took Mona to Los Angeles and raised her on her own.[287]
When Simpson found that their father, Abdulfattah Jandali, was living in Sacramento, California, Jobs had no interest in meeting him as he believed Jandali didn't treat his children well.[288] Simpson went to Sacramento alone and met Jandali, who worked in a small restaurant.[289] Jandali and Simpson spoke for several hours, during which time he told her that he had left teaching for the restaurant business.[289] He also said that he and Schieble had given another child away for adoption but that ""we'll never see that baby again. That baby's gone.""[289] At the request of Jobs, Simpson did not tell Jandali that she had met his son.[289] Jandali further told Simpson that he once managed a Mediterranean restaurant near San Jose and that ""all of the successful technology people used to come there. Even Steve Jobs ... oh yeah, he used to come in, and he was a sweet guy and a big tipper.""[289]
After hearing about the visit, Jobs recalled that ""it was amazing ... I had been to that restaurant a few times, and I remember meeting the owner. He was Syrian. Balding. We shook hands.""[289] However, Jobs still did not want to meet Jandali because ""I was a wealthy man by then, and I didn't trust him not to try to blackmail me or go to the press about it ... I asked Mona not to tell him about me.""[289] Jandali later discovered his relationship to Jobs through an online blog. He then contacted Simpson and asked ""what is this thing about Steve Jobs?"" Simpson told him that it was true and later commented, ""My father is thoughtful and a beautiful storyteller, but he is very, very passive ... He never contacted Steve.""[285] Because Simpson herself researched her Syrian roots and began to meet members of the family, she assumed that Jobs would eventually want to meet their father, but he never did.[285] Jobs also never showed an interest in his Syrian heritage or the Middle East.[285] Simpson fictionalized the search for their father in her 1992 novel The Lost Father.[285] Malek Jandali is their cousin.[290]

Philanthropy
Jobs kept his philanthropic and charitable efforts private; he donated $50 million to Stanford hospital and also contributed to efforts to cure AIDS.[291] He also formed his own charitable foundation called the Steven P. Jobs foundation in 1985.[292]

Honors and awards
 Statue of Jobs at Graphisoft Park, Budapest[293]
1985: National Medal of Technology (with Steve Wozniak), awarded by US President Ronald Reagan[294]
1987: Jefferson Award for Public Service[295]
1989: Entrepreneur of the Decade by Inc. magazine[296]
1991: Howard Vollum Award from Reed College[297]
2004–2010: Listed among the Time 100 Most Influential People in the World on five separate occasions.[citation needed]
2007: Named the most powerful person in business by Fortune magazine[298]
2007: Inducted into the California Hall of Fame, located at The California Museum for History, Women and the Arts[299]
2012: Grammy Trustees Award, an award for those who have influenced the music industry in areas unrelated to performance[300]
2012: Posthumously honored with an Edison Achievement Award for his commitment to innovation throughout his career.[301]
2013: Posthumously inducted as a Disney Legend[302]
2017: Steve Jobs Theatre opens at Apple Park[303]
In popular culture
Main article: List of artistic depictions of Steve Jobs
See also


San Francisco Bay Area portal
Seva Foundation
Timeline of Steve Jobs media

References


^ ""The Walt Disney Company and Affiliated Companies - Board of Directors"". October 14, 2009. Archived from the original on October 14, 2009. Retrieved September 18, 2018.

^ ""Steve Jobs: adopted child who never met his biological father"". Daily Telegraph. October 6, 2011. ISSN 0307-1235. Archived from the original on January 10, 2022. Retrieved September 18, 2018.

^ a b c d e f ""The 'father of invention'"". Saudi Gazette. January 18, 2011. Archived from the original on July 1, 2015. Retrieved June 27, 2015.

^ a b Graff, Amy (November 18, 2015). ""Social media reminds us Steve Jobs was the son of a Syrian migrant"". SFGate. Hearst Communications. Archived from the original on May 19, 2016. Retrieved May 19, 2016.

^ Baig, Edward C. ""Steve Jobs' biological father was Syrian migrant, some note"". USA Today. Archived from the original on May 28, 2020. Retrieved February 14, 2020.

^ Meer, Ameena (Summer 1987). ""Artists in Conversation: Mona Simpson"". Bomb (20). Archived from the original on July 9, 2015. Retrieved July 7, 2015.

^ a b c Shankland, Stephen. ""'Steve Jobs' biography: A wealth of detail"". CNET. Archived from the original on August 19, 2019. Retrieved August 19, 2019.

^ a b Strochlic, Nina (April 23, 2015). ""Steve Jobs Took the Armenian Genocide Personally"". The Daily Beast. Archived from the original on February 7, 2022. Retrieved August 19, 2019 – via www.thedailybeast.com.

^ Isaacson 2015, p. 2.

^ Pappas, Gregory (April 23, 2015). ""Steve Jobs' Almost Greek Connection and the Late Apple Founder's Connection to the Armenian Genocide and the Smyrna Catastrophe"". Archived from the original on July 18, 2021. Retrieved July 18, 2021.

^ ""The Lost Interview: Steve Jobs Tells Us What Really Matters"". Forbes. November 17, 2011. Archived from the original on September 24, 2015. Retrieved July 12, 2015.

^ ""Steve Jobs: The Exotic And Tragic Family History of Apple's Late Visionary"". HuffPost UK. August 28, 2012. Archived from the original on June 7, 2021. Retrieved June 7, 2021.

^ a b c d e f g Brennan, Chrisann (2013). The Bite in the Apple: A Memoir of My Life with Steve Jobs. New York: St. Martin's Press. p. 15. ISBN 9781250038760. Archived from the original on February 7, 2022. Retrieved September 10, 2020.

^ a b c Markoff, John (October 5, 2011). ""Steven P. Jobs, 1955–2011: Apple's Visionary Redefined Digital Age"". The New York Times. Archived from the original on December 19, 2020. Retrieved February 18, 2017.

^ Isaacson 2011, pp. 3–4.

^ ""Even family life was full of drama - Latest News | Gadgets Now"". Gadget Now. Archived from the original on May 20, 2020. Retrieved February 14, 2020.

^ ""Steve Jobs: A tribute to the cousin I never met"". gulfnews.com. Archived from the original on June 2, 2021. Retrieved May 30, 2021.

^ Isaacson 2011, p. 14.

^ a b c d e f Schlender, Brent; Tetzeli, Rick (2015). Becoming Steve Jobs: The Evolution of a Reckless Upstart into a Visionary Leader. Crown (ebook).

^ a b c Isaacson 2011, p. 16.

^ ""worked as a machinist"" Brashares, Ann (2001). Steve Jobs: Thinks Different. p. 8. ISBN 978-0761-31393-9.

^ ""struggling as a machinist and then a used-car salesman .. finance company ..  earned his realtor's license. (but)
downward spiral"" Malone, Michael S. (1999). Infinite Loop: How the World's Most Insanely Great Computer Company Went Insane. ISBN 0-385-48684-7. Archived from the original on August 7, 2020. Retrieved May 22, 2020.

^ Isaacson 2011, p. 5.

^ DeBolt, Daniel (October 7, 2011). ""Steve Jobs called Mountain View home as a child"". Mountain View Voice. Archived from the original on December 4, 2019. Retrieved January 22, 2020. Hatt remembers Jobs attending Monta Loma elementary school and according to county property records, the Jobs family owned a house at 286 Diablo Avenue from 1959 to 1967.

^ a b Isaacson 2011, pp. 5–6.

^ a b c d e f g h i j k l m n o p q r s t u Young, Jeffrey S. (1987). Steve Jobs: The Journey Is the Reward. Amazon Digital Services, 2011 ebook edition (originally Scott Foresman).[pages needed]

^ a b ""Steve Jobs' childhood home becomes a landmark"". mercurynews.com. October 29, 2013. Archived from the original on June 26, 2015. Retrieved June 26, 2015.

^ a b Isaacson 2011, pp. 12–13.

^ Isaacson 2015, p. 13.

^ Isaacson 2011, pp. 13–14.

^ Isaacson 2011, pp. 14.

^ a b c d ""Steve Jobs' old garage about to become a piece of history"". mercurynews.com. September 27, 2013. Archived from the original on June 26, 2015. Retrieved June 26, 2015.

^ a b Isaacson 2015, p. 19.

^ ""Steve Jobs and the Early Apple Years"". The PC Is Born. Joomla. Archived from the original on June 24, 2012. Retrieved March 27, 2012.

^ McBurney, Sally (Director) (2013). Steve Jobs 1994 Uncut Interview with English Subtitles (Video). Menlo Park, California: Silicon Valley Historical Association.

^ Isaacson 2015, p. 30.

^ Steve Jobs Interview about the Blue Box Story. Silicon Valley Historical Association. January 19, 2009. Archived from the original on April 2, 2013. Retrieved June 14, 2015 – via YouTube.

^ a b McBurney, Sally (Director) (2013). Steve Jobs: Visionary Entrepreneur (Video). Menlo Park, California: Silicon Valley Historical Association.

^ Isaacson 2011, p. 19.

^ Isaacson 2011, pp. 31–32.

^ Brennan, Chrisann (October 19, 2011). ""Jobs at 17: Nerd, Poet, Romantic"". Rolling Stone Magazine. Archived from the original on April 25, 2012. Retrieved February 9, 2015.

^ Blumenthal, Karen (2012). Steve Jobs The Man Who Thought Different. A&C Black. ISBN 9781408832073. pp.271–272

^ Isaacson 2015, p. 33.

^ Isaacson 2015, p. 37.

^ Schlender 2016, p. 30.

^ a b Isaacson 2015, pp. 40–41.

^ John Naughton (October 8, 2011). ""Steve Jobs: Stanford commencement address, June 2005"". The Guardian. London. Archived from the original on February 11, 2012.

^ Schlender, Brent (November 9, 1998). ""The Three Faces of Steve in this exclusive, personal conversation, Apple's CEO reflects on the turnaround, and on how a wunderkind became an old pro"". Fortune. Archived from the original on April 8, 2015. Retrieved June 27, 2015.

^ a b Isaacson 2011, pp. 42–43.

^ a b c d ""An exclusive interview with Daniel Kottke"". India Today. September 13, 2011. Archived from the original on May 18, 2012. Retrieved October 27, 2011.

^ ""How Steve Wozniak's Breakout Defined Apple's Future"". Gameinformer. June 27, 2013. Archived from the original on November 1, 2013. Retrieved February 13, 2014.

^ ""Cassidy on Nolan Bushnell: 'Steve was difficult,' says man who first hired Steve Jobs"". Mercury News. March 29, 2013. Archived from the original on December 6, 2013. Retrieved April 2, 2013.

^ ""What really shaped Steve Jobs's view of India – Realms of intuition or the pains of Delhi belly?"". Economic Times. India. September 25, 2011. Archived from the original on May 11, 2012. Retrieved October 27, 2011.

^ ""Il santone della Silicon Valley che ha conquistato i tecno-boss"" (in Italian). Repubblica.it. June 9, 2008. Archived from the original on June 24, 2012. Retrieved August 30, 2011.

^ ""Wandering in India for 7 months: Steve Jobs"". Yahoo News. October 24, 2011. Archived from the original on June 24, 2012. Retrieved October 27, 2011.

^ Andrews, Amanda (January 14, 2009). ""Steve Jobs, Apple's iGod: Profile"". The Daily Telegraph. UK. Archived from the original on May 11, 2012. Retrieved October 29, 2009.

^ ""Steve Jobs profile: Apple's hard core"". Edinburgh: News scotsman. January 11, 2009. Archived from the original on September 26, 2011. Retrieved October 29, 2009.

^ Markoff, John (2005). What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry. Penguin Books. p. preface xix. ISBN 978-0-14-303676-0. Archived from the original on August 19, 2020. Retrieved October 5, 2011.

^ ""Jobs's Pentagon papers: kidnap fears, drug use and a speeding ticket"". The Sydney Morning Herald. Archived from the original on June 24, 2012. Retrieved June 12, 2012.

^ Silberman, Steve (October 28, 2011). ""What Kind of Buddhist was Steve Jobs, Really?"". NeuroTribes. Archived from the original on June 24, 2012. Retrieved December 29, 2011.

^ Burke, Daniel (November 2, 2011). ""Steve Jobs' private spirituality now an open book"". USA Today. Archived from the original on September 14, 2012. Retrieved December 29, 2011.

^ Isaacson 2011, pp. 52–54.

^ Murphy, Conor. ""The History of Breakout"". Big Fish. Big Fish Games, Inc. Archived from the original on May 28, 2015. Retrieved April 22, 2015.

^ ""Letters – General Questions Answered"". Woz.org. Archived from the original on June 12, 2011. Retrieved June 20, 2016.Wozniak, Steve (2006). iWoz. W. W. Norton. a: pp. 147–48, b: p. 180. ISBN 978-0-393-06143-7.Kent, Steven (2001). The Ultimate History of Video Games. Three Rivers. pp. 71–73. ISBN 978-0-7615-3643-7.""Breakout"". Arcade History. June 25, 2002. Archived from the original on January 5, 2016. Retrieved April 19, 2010.""Classic Gaming: A Complete History of Breakout"". GameSpy. Archived from the original on June 23, 2014. Retrieved April 19, 2010.

^ Isaacson 2015, pp. 104–107.

^ Linzmayer 2004, pp. 5–6.

^ a b Linzmayer 2004, pp. 6–8.

^ Linzmayer, Owen W. ""Apple Confidential: The Real Story of Apple Computer, Inc"". The Denver Post. Archived from the original on March 20, 2012.

^ Simon, Dan (June 24, 2010). ""The gambling man who co-founded Apple and left for $800"". CNN. Archived from the original on April 10, 2014. Retrieved June 24, 2010.

^ ""How Did Apple Computer Get Its Brand Name?"". Branding Strategy Insider. November 17, 2011. Archived from the original on July 4, 2017. Retrieved November 6, 2017.

^ a b Linzmayer, pp. 5–7.

^ a b Schlender 2016, pp. 39–40.

^ Isaacson 2015, pp. 66–68.

^ Linzmayer, pp. 7–9.

^ a b Williams, Gregg; Moore, Rob (December 1984). ""The Apple Story / Part 1: Early History"". BYTE (interview). p. A67. Retrieved November 16, 2019.

^ Markoff, John (September 1, 1997). ""An 'Unknown' Co-Founder Leaves After 20 Years of Glory and Turmoil"". The New York Times. Archived from the original on January 2, 2018. Retrieved August 24, 2011.

^ ""Done Deals: Venture Capitalists Tell Their Story: Featured HBS Arthur Rock"". HBS Working Knowledge. Archived from the original on August 16, 2019. Retrieved June 23, 2018.

^ Isaacson 2011, pp. 81–83.

^ Linzmayer 2004, p. 11.

^ Linzmayer 2004, p. 12.

^ a b Wozniak, Steve. ""woz.org: Comment From e-mail: Why didn't the early Apple II's use Fans?"". woz.org. Archived from the original on December 26, 2015. Retrieved May 10, 2015.

^ Wozniak, Steve; Smith, Gina (2006). iWoz: Computer Geek to Cult Icon: How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It. W. W. Norton & Company. ISBN 0-393-06143-4. OCLC 502898652.

^ a b Reimer, Jeremy (December 15, 2005). ""Total share: 30 years of personal computer market share figures"". Ars Technica. Condé Nast. Archived from the original on July 2, 2012. Retrieved May 25, 2010.

^ Edwards, Jim (December 26, 2013). ""These Pictures of Apple's First Employees Are Absolutely Wonderful"". Business Insider. Archived from the original on July 31, 2020. Retrieved January 19, 2015.

^ a b Isaacson 2015, pp. 88–89.

^ Metz, Rachel (October 15, 2013). ""Steve Jobs' ex-girlfriend pens memoir on life with 'vicious' Apple founder"". The Guardian. Archived from the original on June 6, 2015. Retrieved January 17, 2015.

^ Bullock, Diane (August 31, 2010). ""The Kids of Business Icons: Lisa Brennan-Jobs"". Minyanville. Archived from the original on August 31, 2012. Retrieved October 6, 2011.

^ Isaacson 2015, p. 93.

^ ""Machine of the Year: The Computer Moves in"". Time, January 3, 1983

^ a b c Cocks Jay. Reported by Michael Moritz. ""The Updated Book of Jobs Archived February 9, 2015, at the Wayback Machine"" in ""Machine of the Year: The Computer Moves in"". Time, January 3, 1983:27.

^ ""Steve Jobs: Net Worth | Investopedia"". Investopedia. October 14, 2015. Archived from the original on July 30, 2018. Retrieved July 30, 2018.

^ ""Photos: The Historic House Steve Jobs Demolished"". Wired. February 17, 2011. Archived from the original on June 3, 2012. Retrieved March 11, 2017.

^ Isaacson 2015, pp. 386–387.

^ Lee, Henry K. (February 15, 2011). ""Steve Jobs' historic Woodside mansion is torn down"". The San Francisco Chronicle. Archived from the original on December 25, 2011. Retrieved February 7, 2022.

^ a b c ""Steve Wozniak on Newton, Tesla, and why the original Macintosh was a 'lousy' product"". June 27, 2013. Archived from the original on March 12, 2016. Retrieved June 28, 2013.

^ O'Grady, Jason D. (2009). Apple Inc. ABC-CLIO. ISBN 9780313362446. pp. 8–10

^ Isaacson 2015, pp. 109–112.

^ Linzmayer 2004, pp. 110–113.

^ Isaacson 2015, pp. 167–170.

^ Schlender, Brent; Tetzeli, Rick (2016). Becoming Steve Jobs: The Evolution of a Reckless Upstart into a Visionary Leader. Crown Business; Reprint edition. ISBN 9780385347426. pp.82–83

^ Hertzfeld, Andy. ""The Times They Are A-Changin'"". folklore.org. Archived from the original on February 4, 2012.

^ Kahney, Leander (January 6, 2004). ""Wired News: We're All Mac Users Now"". Wired News. Archived from the original on January 4, 2014. Retrieved September 20, 2006.

^ ""America's Most Admired Companies: Jobs' journey timeline"". Fortune. Archived from the original on April 10, 2014. Retrieved May 24, 2010. Jobs and a team of engineers visit Xerox PARC, where they see a demo of mouse and graphical user interface

^ a b c Isaacson 2015, pp. 185–187.

^ a b c Schlender 2016, pp. 84–88.

^ Linzmayer 2004, p. 98.

^ ""Machine That Changed The World, The; Paperback Computer, The; Interview with Steve Jobs, 1990"". Open Vault. WGBH Media Library & Archives. May 14, 1990. Archived from the original on September 20, 2016. Retrieved September 15, 2016.

^ Robbeloth, DeWitt (October–November 1985). ""Whither Apple?"". II Computing. p. 8. Retrieved January 28, 2015.

^ Rice, Valerie (April 15, 1985). ""Unrecognized Apple II Employees Exit"". InfoWorld. p. 35. Archived from the original on May 14, 2021. Retrieved February 4, 2015.

^ Bunnell, David (April 30, 2010). ""Chapter 10: Steve Thumbs his Nose at the Apple II"". My Close Encounters With Steve Jobs. Archived from the original on July 19, 2019. Retrieved November 12, 2019 – via Cult of Mac.

^ ""I Never Left Apple"". Woz.org. January 3, 2018. Archived from the original on March 27, 2019. Retrieved November 12, 2019.

^ Krishnamoorthy, Anand; Li, Susan (October 6, 2011). ""Jobs's Death Was Like Lennon, JFK Getting Shot, Wozniak Says"". Bloomberg Businessweek. Archived from the original on November 12, 2019. Retrieved November 12, 2019.

^ a b c d e f Swaine, Michael and Paul Frieberger. Fire in the Valley: The Birth and Death of the Personal Computer, 3rd Edition, Dallas: Pragmatic Bookshelf, 2014

^ Spector, G (September 24, 1985). ""Apple's Jobs Starts New Firm, Targets Education Market"". PC Week. p. 109.

^ Linzmayer 2004, p. 208.

^ Schwartz, John (October 24, 1988). ""Steve Jobs Comes Back"". Newsweek. Palo Alto, California. p. Business. Archived from the original on October 14, 2014. Retrieved October 20, 2014.

^ ""NeXT Timeline"". Archived from the original on February 3, 2015. Retrieved January 21, 2015.

^ Schlender, Brenton R. (October 13, 1988). ""Next Project: Apple Era Behind Him, Steve Jobs Tries Again, Using a New System"". The Wall Street Journal (Western ed.). Palo Alto, California: Dow Jones & Company Inc. p. Front Page Leader. Archived from the original on October 20, 2014. Retrieved October 20, 2014.

^ Rose, F. (April 23, 2009). Rose, Frank (August 24, 2011). ""The End of Innocence at Apple: What Happened After Steve Jobs was Fired"". Wired. Archived from the original on October 7, 2011. Retrieved March 11, 2017.. Wired.

^ ""Welcome to info.cern.ch: The website of the world's first-ever web server"". CERN (European Organization for Nuclear Research). 2008. Archived from the original on January 18, 2010. Retrieved November 1, 2011.

^ Computimes. (May 31, 1990). Interpersonal computing –  the third revolution? Archived April 29, 2016, at the Wayback Machine. New Straits Times. (230), 20; Schlender, B. R., Alpert, M. (February 12, 1990). Schlender, Brenton R. (February 12, 1990). ""Who's ahead in the computer wars"". CNN. Archived from the original on November 29, 2020. Retrieved August 3, 2020.. Fortune.

^ Stross, R. E. (1993). Steve Jobs and the NeXT Big Thing. Atheneum. ISBN 978-0-689-12135-7. pp. 117, 120, 246.

^ a b O'Grady, J. (2008). Apple Inc. Greenwood Press. ISBN 978-0-313-36244-6.[pages needed]

^ Linzmayer 2004, p. 213.

^ Smith, Alvy Ray. ""Pixar Founding Documents"". Alvy Ray Smith Homepage. Archived from the original on April 27, 2005. Retrieved January 11, 2011.

^ ""Pixar's 'Brave' Honors Steve Jobs"". The Hollywood Reporter. May 25, 2012. Archived from the original on February 14, 2021. Retrieved February 8, 2021.

^ Hill, Jim (February 5, 2012). ""Steve Jobs bio reveals how Michael Eisner actively tried to derail Disney's 2006 acquisition of Pixar"". Jim Hill Media. Archived from the original on June 24, 2012. Retrieved February 10, 2012.

^ McClintock, Pamela (February 24, 2013). ""Oscars 2013: Brenda Chapman's 'Brave' Win a Vindication After Being Fired From the Project"". The Hollywood Reporter. Archived from the original on April 20, 2021. Retrieved May 1, 2021.

^ Wolff, Michael, ""iPod, Therefore I am"". Vanity Fair. October 10, 2006. Archived from the original on March 28, 2014., Vanity Fair, April 2006. Retrieved September 3, 2010.

^ a b c Iger, Robert (September 18, 2019). ""'We Could Say Anything to Each Other': Bob Iger Remembers Steve Jobs"". Vanity Fair. Archived from the original on March 10, 2021. Retrieved February 7, 2022.

^ a b January 25, 2006 ""Disney buys Pixar for $7.4 bn"". Archived from the original on November 9, 2013., rediff.com

^ ""The Walt Disney Company – Steve Jobs Biography"". Archived from the original on April 26, 2012. Retrieved June 22, 2008.Holson, Laura M. (January 25, 2006). ""Disney Agrees to Acquire Pixar in a $7.4 Billion Deal"". The New York Times. Archived from the original on October 9, 2011. Retrieved January 17, 2010.""Pixar Becomes Unit of Disney"". The New York Times. Associated Press. May 6, 2006. Archived from the original on April 23, 2011. Retrieved January 17, 2010.

^ ""Steve Jobs, 1955–2011"". Splashnogly. October 6, 2011. Archived from the original on April 7, 2012. Retrieved January 15, 2012.

^ ""Jobs's 7.7% Disney Stake Transfers to Trust Led by Widow Laurene"". Bloomberg. Archived from the original on April 10, 2014.

^ Norman, Floyd (January 19, 2009). ""Steve Jobs: A Tough Act to Follow"". Jim Hill Media. Archived from the original on May 8, 2010. Retrieved January 19, 2009.

^ Catmull, Edwin; Wallace, Amy (2014). Creativity, Inc.: Overcoming the Unseen Forces That Stand in the Way of True Inspiration. Transworld Publishers Limited. ISBN 978-0552167260.

^ Bort, Julie (June 5, 2014). ""Steve Jobs Taught This Man How To Win Arguments With Really Stubborn People"". Inc. Monsueto Ventures. Archived from the original on June 8, 2014. Retrieved June 8, 2014.

^ Apple Computer, Inc. Finalizes Acquisition of NeXT Software Inc. at the Wayback Machine (archive index), Apple Inc., February 7, 1997. Retrieved June 25, 2006.

^ ""Apple Formally Names Jobs as Interim Chief"". The New York Times. September 17, 1997. Archived from the original on November 17, 2017. Retrieved June 27, 2011.

^ ""The once and future Steve Jobs"". Salon.com. October 11, 2000. Archived from the original on April 16, 2009.

^ Norr, Henry (January 6, 2000). ""MacWorld Expo/Permanent Jobs/Apple CEO finally drops 'interim' from title"". San Francisco Chronicle. Archived from the original on November 2, 2011. Retrieved June 27, 2011.

^ ""Jobs announces new MacOS, becomes 'iCEO'"". CNN. January 5, 2000. Archived from the original on August 20, 2013.

^ Levy, Steven (1995). Insanely Great: The Life and Times of Macintosh, the Computer That Changed Everything. Penguin Books. p. 312. ISBN 978-0-14-023237-0. Archived from the original on August 20, 2020. Retrieved May 6, 2020.

^ ""If Apple can go home again, why not Dell?"". Archived from the original on October 10, 2011. Retrieved January 5, 2009. CNET News. May 19, 2008.

^ ""Dell: Apple should close shop"". CNET. Archived from the original on May 17, 2008.

^ Markoff, John (January 16, 2006). ""Michael Dell Should Eat His Words, Apple Chief Suggests"". The New York Times. Archived from the original on June 4, 2012. Retrieved May 24, 2010.

^ ""11 Presentation Lessons You Can Still Learn From Steve Jobs"". Forbes. May 28, 2014. Archived from the original on June 5, 2014. Retrieved June 16, 2014.

^ a b ""Steve Jobs' black turtleneck reportedly explained in biography"". The Los Angeles Times. October 11, 2011. Archived from the original on October 26, 2011. Retrieved October 14, 2011.

^ ""Wear the Exact Outfit of Steve Jobs for $458"". Gizmodo. February 28, 2006. Archived from the original on February 4, 2012. Retrieved April 19, 2010.

^ Liedtke, Michael (October 5, 2002). ""Steve Jobs resigns from Gap's board"". The Berkeley Daily Planet. Archived from the original on April 19, 2012. Retrieved December 23, 2011.

^ ""New questions raised about Steve Jobs's role in Apple stock options scandal"". December 28, 2006. Archived from the original on May 9, 2007.

^ ""Apple restates, acknowledges faked documents"". EE Times. December 29, 2006. Archived from the original on May 21, 2013. Retrieved January 1, 2007.

^ ""Apple Improves Recycling Plan"". PC Magazine. April 21, 2006. Archived from the original on October 20, 2008.

^ Nick Bilton, Bilton, Nick (August 9, 2011). ""Apple Is the Most Valuable Company"". The New York Times. Archived from the original on February 25, 2012. Retrieved February 24, 2012., New York Times, August 9, 2011

^ ""7.30"". ABCnet.au. October 6, 2011. Archived from the original on October 9, 2011. Retrieved November 12, 2011.

^ ""Lateline: ""Visionary Steve Jobs succumbs to cancer"""". ABCnet.au. October 6, 2011. Archived from the original on October 9, 2011. Retrieved November 12, 2011.

^ ""Live from Macworld 2007: Steve Jobs keynote"". 2007. Archived from the original on June 24, 2012. Retrieved April 19, 2010.

^ ""Group Wants $7B USD From Apple, Steve Jobs, Executives Over Securities Fraud"". Archived from the original on February 4, 2012. Retrieved July 2, 2008.

^ ""Apple, Steve Jobs, Executives, Board, Sued For Securities Fraud"". Archived from the original on May 19, 2009.

^ a b Andrew S. Ross (November 1, 2011). ""Steve Jobs bio sheds light on Obama relationship"". San Francisco Chronicle. Archived from the original on November 4, 2011. Retrieved November 12, 2011.

^ a b c d Evangelista, Benny (August 2, 2004). ""Apple's Jobs has cancerous tumor removed"". San Francisco Chronicle. p. A1. Archived from the original on August 18, 2006. Retrieved August 9, 2006.

^ ""Steve Jobs and the Celebrity Diagnosis Complete Guide to Tumors of the Pancreas"". Celebrity Diagnosis. October 6, 2011. Archived from the original on June 24, 2012. Retrieved November 12, 2011.

^ Elkind, Peter (March 5, 2008). ""The trouble with Steve Jobs"". Fortune. Archived from the original on May 18, 2010. Retrieved March 5, 2008.

^ Fiore, Kristina (December 28, 2012). ""Jobs Leaves Lessons for Cancer Care"". MedPage Today. Archived from the original on April 10, 2014. Retrieved July 14, 2013.

^ Gorski, David (October 31, 2011). """"And one more thing"" about Steve Jobs' battle with cancer"". Science-Based Medicine. Archived from the original on May 11, 2020. Retrieved October 9, 2020.

^ Physician Biography for Barrie R. Cassileth. Archived November 13, 2011, at the Wayback Machine

^ Liz Szabo (June 18, 2013). ""Book raises alarms about alternative medicine"". USA Today. Archived from the original on June 18, 2013. Retrieved June 19, 2013.

^ Ned Potter. ""Steve Jobs Regretted Delaying Cancer Surgery 9 Months, Biographer Says"". ABC News. Archived from the original on April 10, 2014. ABC News October 20, 2011

^ ""Bio Sheds Light on Steve Jobs' Decision to Delay Cancer Surgery, Pursue Herbal Remedies"". Fox News. October 20, 2011. Archived from the original on June 26, 2012. Associated Press October 20, 2011

^ ""Pancreatic Cancer Treatment"". Mayo Clinic. Archived from the original on February 4, 2012. Retrieved April 19, 2010.

^ Markoff, John (July 23, 2008). ""Talk of Chief's Health Weighs on Apple's Share Price"". The New York Times. Archived from the original on March 18, 2017. Retrieved February 18, 2017.

^ a b Elmer, Philip (June 13, 2008). ""Steve Jobs and Whipple"". Fortune. Archived from the original on June 24, 2012. Retrieved April 19, 2010.

^ Kahney, Leander (August 8, 2006). ""Has Steve Jobs Lost His Magic?"". Cult of Mac. Wired News. Archived from the original on February 4, 2012. Retrieved August 8, 2006. Looking very thin, almost gaunt, Jobs used the 90-minute presentation to introduce a new desktop Mac and preview the next version of Apple's operating system, code-named Leopard.

^ Meyers, Michelle. ""Jobs speech wasn't very Jobs-like"". BLOGMA. CNET News.com. Archived from the original on December 25, 2007. Retrieved August 8, 2006. [The audience was] uninspired (and concerned) by Jobs's relatively listless delivery

^ Saracevic, Al (August 9, 2006). ""Where's Jobs' Mojo?"". San Francisco Chronicle. p. C1. Archived from the original on January 28, 2012. Retrieved August 9, 2006.

^ Cheng, Jacqui (August 8, 2006). ""What happened to The Steve we know and love?"". Ars Technica. Archived from the original on February 4, 2012. Retrieved August 8, 2006.

^ Claburn, Thomas (August 11, 2006). ""Steve Jobs Lives!"". InformationWeek. Archived from the original on February 4, 2012. Retrieved October 9, 2007.

^ ""Business Technology: Steve Jobs's Appearance Grabs Notice, Not Just the IPhone"". The Wall Street Journal. Archived from the original on April 26, 2009. Retrieved April 19, 2010.

^ ""Apple says Steve Jobs feeling a little under the weather"". Archived from the original on April 10, 2014. in AppleInsider.

^ ""Steve Jobs and Apple"". Archived from the original on April 10, 2014. Marketing Doctor Blog. July 24, 2008.

^ ""Steve Jobs Did Not Have 'Pancreatic Cancer'"". Medpagetoday.com. January 24, 2011. Archived from the original on June 24, 2012. Retrieved November 12, 2011.

^ Joe Nocera (July 26, 2008). ""Apple's Culture of Secrecy"". The New York Times. Archived from the original on March 5, 2017. Retrieved February 18, 2017. While his health problems amounted to a good deal more than 'a common bug,' they weren't life-threatening and he doesn't have a recurrence of cancer.

^ ""Steve Jobs's Obituary, As Run By Bloomberg"". Gawker Media. August 27, 2008. Archived from the original on February 4, 2012. Retrieved August 28, 2008.

^ ""Bloomberg publishes Jobs obit but why?"". Zdnet Blogs. ZDnet. August 28, 2008. Archived from the original on August 31, 2008. Retrieved August 29, 2008.

^ Mikkelson, Barbara (September 26, 2007). ""And Never The Twain Shall Tweet"". Snopes.com. Archived from the original on August 22, 2011. Retrieved November 2, 2012.

^ ""Apple posts 'Lets Rock' event video"". Macworld. September 10, 2008. Archived from the original on February 4, 2012. Retrieved September 11, 2008.

^ ""Live from Apple's ""spotlight turns to notebooks"" event"". Engadget. October 14, 2008. Archived from the original on June 24, 2012. Retrieved October 14, 2008.

^ Stone, Brad (December 17, 2008). ""Apple's Chief to Skip Macworld, Fueling Speculation"". The New York Times. Archived from the original on December 6, 2011. Retrieved May 24, 2010.

^ ""Steve Jobs' Health Declining Rapidly, Reason for Macworld Cancellation"". Gizmodo. December 30, 2008. Archived from the original on June 24, 2012. Retrieved April 19, 2010.

^ ""Apple's Jobs admits poor health"". BBC News. January 5, 2009. Archived from the original on August 25, 2011. Retrieved January 5, 2009.

^ Jobs, Steve (January 5, 2009). ""Letter from Apple CEO Steve Jobs"" (Press release). Apple Inc. Archived from the original on February 4, 2012. Retrieved January 20, 2009.

^ a b ""Apple Media Advisory"" (Press release). Apple Inc. January 14, 2009. Archived from the original on February 4, 2012. Retrieved January 14, 2009.

^ ""I BEG YOU, mighty Jobs, TAKE MY LIVER, Cook told Apple's dying co-founder"". The Register. March 13, 2015. Archived from the original on August 16, 2017. Retrieved August 22, 2017.

^ a b ""Steve Jobs recovering after liver transplant"". CNN. June 23, 2009. Archived from the original on March 31, 2014. Retrieved April 19, 2010.

^ ""Liver Transplant in Memphis: Jobs' was Sickest Patient on Waiting List"". Celebrity Diagnosis. June 24, 2009. Archived from the original on June 24, 2012.

^ Grady, Denise; Meier, Barry (June 22, 2009). ""A Transplant That Is Raising Many Questions"". The New York Times. Archived from the original on April 22, 2017. Retrieved February 18, 2017.

^ Helft, Miguel (January 17, 2010). ""Apple Says Steve Jobs Will Take a New Medical Leave"". The New York Times. Archived from the original on March 18, 2017. Retrieved January 17, 2010.

^ ""Steve Jobs to take medical leave of absence but remain Apple CEO"". Archived from the original on February 4, 2012.

^ Abell, John (June 8, 2011). ""Video: Jobs Pitches New 'Mothership' to Approving Cupertino City Council"". Wired. Archived from the original on February 4, 2012. Retrieved June 9, 2011.

^ Letter from Steve Jobs To the Apple Board of Directors and the Apple Community (resignation letter August 24, 2011) Archived April 15, 2012, at WebCite

^ ""Apple Resignation Letter"" (Press release). Apple Inc. Archived from the original on April 15, 2012. Retrieved August 29, 2011.

^ ""Steve Jobs Resigns as CEO of Apple"" (Press release). Apple Inc. August 24, 2011. Archived from the original on April 15, 2012. Retrieved August 24, 2011.

^ Biddle, Sam (October 19, 2011). ""Steve Jobs Worked the Day Before He Died"". Gizmodo. Archived from the original on June 24, 2012. Retrieved October 21, 2011.

^ Gupta, Poornima (August 18, 2011). ""Steve Jobs Quits"". Reuters. Archived from the original on February 4, 2012. Retrieved August 25, 2011.

^ Siegler, M.G. ""Steve Jobs Resigns As CEO of Apple"". TechCrunch. Archived from the original on August 25, 2011. Retrieved August 25, 2011.

^ ""Rare Pancreatic Cancer Caused Steve Jobs' Death"" (Press release). Voice of America. October 7, 2011. Archived from the original on January 24, 2012. Retrieved October 7, 2011.

^ Rushe, Dominic (October 6, 2011). ""Steve Jobs, Apple co-founder, dies at 56"". The Guardian. UK. Archived from the original on June 19, 2013.

^ Gullo, Karen (October 10, 2011). ""Steve Jobs Died at Home of Respiratory Arrest Related to Pancreatic Cancer"". Bloomberg L.P. Archived from the original on February 10, 2012. Retrieved February 10, 2012.

^ a b c d Simpson, Mona (October 30, 2011). ""A Sister's Eulogy for Steve Jobs"". The New York Times. Archived from the original on September 5, 2012. Retrieved October 30, 2011.

^ Ian Sherr; Geoffrey A. Fowler (October 7, 2011). ""Steve Jobs Funeral Is Friday"". The Wall Street Journal. Archived from the original on August 13, 2013.

^ Cook, Tim (October 5, 2011). ""Statement by Apple's Board of Directors"" (Press release). Apple Inc. Archived from the original on April 25, 2012. Retrieved October 5, 2011.

^ ""Pixar Animation Studios"". Pixar. Archived from the original on June 8, 2012. Retrieved April 18, 2013.

^ ""Remembering Steve Jobs"". Apple Inc. Archived from the original on June 24, 2012. Retrieved October 10, 2011.

^ ""Apple flies flags at half staff for Steve Jobs"". KOKI-TV. October 6, 2011. Archived from the original on August 13, 2013. Retrieved October 29, 2011.

^ ""Microsoft lowers flags to half staff in tribute to Steve Jobs"". Network World. October 6, 2011. Archived from the original on November 9, 2013. Retrieved October 29, 2011.

^ ""Disney World flags at half-staff in memory of Steve Jobs"". Bay News 9. October 6, 2011. Archived from the original on December 13, 2011. Retrieved October 29, 2011.

^ Pepitone, Julianne (October 6, 2011). ""Steve Jobs: The homepage tributes"". CNN. Archived from the original on April 25, 2012. Retrieved January 10, 2012.

^ ""Apple website pays tribute to Steve Jobs"". The Times of India. India. October 5, 2011. Archived from the original on April 25, 2012. Retrieved October 7, 2011.

^ ""Remembering Steve Jobs"". Apple Inc. Archived from the original on June 24, 2012. Retrieved October 6, 2011.

^ a b ""A Celebration of Steve's Life"". Archived from the original on December 29, 2013. Apple.com Retrieved October 26, 2011

^ Fernandez, Sofia M. (October 14, 2011). ""Private Steve Jobs Memorial Set for Oct. 16 – The Hollywood Reporter"". The Hollywood Reporter. Archived from the original on December 31, 2013. Retrieved November 12, 2011.

^ ""Steve Jobs Memorial Service To Be Held Oct. 16"". The Wall Street Journal. October 15, 2011. Archived from the original on August 13, 2013. Retrieved November 12, 2011.

^ Vascellaro, Jessica E. (October 17, 2011). ""Steve Jobs's Family Gave Moving Words at Sunday Memorial – Digits – WSJ"". The Wall Street Journal. Archived from the original on April 10, 2014. Retrieved November 12, 2011.

^ Wadhwa, Hitendra (June 21, 2015). ""Steve Jobs's Secret to Greatness: Yogananda"". Inc. Archived from the original on June 22, 2015. Retrieved June 23, 2015.

^ Wozniak Tearfully Remembers His Friend Steve Jobs. YouTube. October 6, 2011. Archived from the original on December 19, 2021.

^ Patricia Sellers (October 6, 2011). ""George Lucas on Steve Jobs"". Fortune. Archived from the original on January 28, 2012. Retrieved October 6, 2011.

^ ""Steve Jobs"". Thegatesnotes.com. October 5, 2011. Archived from the original on January 27, 2012. Retrieved November 12, 2011.

^ ""Statement by the President on the Passing of Steve Jobs"". whitehouse.gov (Press release). October 5, 2011. Archived from the original on February 10, 2021 – via National Archives.

^ ""Steve Jobs Died of Respiratory Arrest Amid Pancreatic Tumor"". ABC News. October 10, 2011. Archived from the original on April 25, 2012. Retrieved November 12, 2011.

^ Gupta, Poornima (October 10, 2011). ""Steve Jobs died of respiratory arrest, tumor"". Reuters. Archived from the original on April 10, 2014. Retrieved September 21, 2012.

^ Celebrating Steve | October 5 | Apple, archived from the original on October 20, 2021, retrieved October 20, 2021

^ a b ""Steve Jobs' autobiography: a chronicle of a complex genius"". The Hindu. Chennai, India. October 24, 2011. Archived from the original on November 9, 2013.

^ Shontell, Alyson. ""This Man Could Have Made $30 Million Per Year As Apple's Designer — But He Turned Steve Jobs Down"". Business Insider. Archived from the original on May 17, 2019. Retrieved May 17, 2019.

^ ""What Made Steve Jobs So Great?"". Fast Company. August 24, 2011. Archived from the original on April 10, 2014. Retrieved August 21, 2012.

^ a b ""Does Steve Jobs know how to code?"". Archived from the original on October 31, 2013. Retrieved August 21, 2012.

^ ""Searching for Magic in India and Silicon Valley: An Interview with Daniel Kottke, Apple Employee #12"". Boing Boing. August 9, 2012. Archived from the original on January 11, 2014. Retrieved August 30, 2012.

^ ""Portfolio of over 300 patents underscores Steve Jobs' attention to detail"". Archived from the original on April 10, 2014. Retrieved September 26, 2012.

^ a b ""U.S. Government patent database"". Archived from the original on June 24, 2012. Retrieved August 29, 2011.

^ ""U.S. Government patent application database"". Archived from the original on April 20, 2012. Retrieved August 29, 2011.

^ ""United States Patent 8,032,843, Ording, et al., October 4, 2011, ""User interface for providing consolidation and access"""". Archived from the original on June 24, 2012. Retrieved November 21, 2017.

^ ""Steve Jobs Told Me Why He Loved Being A CEO"". Business Insider. Archived from the original on August 6, 2011. Retrieved February 2, 2013. He told me once that part of the reason he wanted to be CEO was so that nobody could tell him that he wasn't allowed to participate in the nitty-gritty of product design"", Reid writes. ""He was right there in the middle of it. All of it. As a team member, not as CEO. He quietly left his CEO hat by the door, and collaborated with us.

^ Kachka, Boris (August 26, 2015). ""How Kate Winslet Won a Role in Steve Jobs and Managed All That Sorkin Dialogue"". Vulture. Archived from the original on June 18, 2016. Retrieved December 28, 2017.

^ Rosenwald, Michael S. (October 24, 2011). ""Walter Isaacson's 'Steve Jobs' biography shows Apple co-founder's genius, flaws"". The Washington Post. Archived from the original on October 25, 2012. Retrieved September 16, 2012.

^ ""Steve Jobs Still Wins Plenty of Patents – MIT Technology Review"". MIT Technology Review. Archived from the original on January 15, 2015. Retrieved January 21, 2015.

^ Isaacson 2015, pp. 73–83.

^ Christoph Dernbach (October 12, 2007). ""Apple Lisa"". Mac History. Archived from the original on November 3, 2012. Retrieved November 15, 2012.

^ ""Apple Lisa computer"". Archived from the original on June 2, 2015. Retrieved May 20, 2015.

^ Simon, Jeffrey S.; Young, William L. (April 14, 2006). iCon: Steve Jobs, the greatest second act in the history of business (Newly updated ed.). Hoboken, NJ: Wiley. p. 70. ISBN 978-0471787846.

^ Linzmayer 2004, p. 79.

^ Polsson, Ken (July 29, 2009). ""Chronology of Apple Computer Personal Computers"". Archived from the original on August 21, 2009. Retrieved August 27, 2009. See May 3, 1984.

^ Linzmayer 2004, p. 113.

^ Maney, Kevin (January 28, 2004). ""Apple's '1984' Super Bowl commercial still stands as watershed event"". USA Today. Archived from the original on April 5, 2016. Retrieved April 11, 2010.

^ Leopold, Todd (February 3, 2006). ""Why 2006 isn't like '1984'"". CNN. Archived from the original on April 5, 2016. Retrieved May 10, 2008.

^ Creamer, Matthew (March 1, 2012). ""Apple's First Marketing Guru on Why '1984' Is Overrated"". Ad Age. Archived from the original on April 19, 2015. Retrieved April 19, 2015.

^ Cellini, Adelia (January 2004). ""The Story Behind Apple's '1984' TV commercial: Big Brother at 20"". MacWorld. Vol. 1, no. 21. p. 18. Archived from the original on June 28, 2009. Retrieved May 9, 2008.

^ Long, Tony (January 22, 2007). ""Jan. 22, 1984: Dawn of the Mac"". Wired. Archived from the original on April 16, 2010. Retrieved April 11, 2010.

^ Reimer, Jeremy (December 14, 2005). ""Total share: 30 years of personal computer market share figures"". Ars Technica. Archived from the original on May 14, 2021. Retrieved April 16, 2015.

^ Carter, Mia. ""Steve Jobs: 10 Products that Define this Tech Legend"". Inventions and Discoveries. Archived from the original on April 4, 2012. Retrieved March 27, 2012.

^ ""Steve Jobs Introduces NeXTComputer"". Archived from the original on April 7, 2013. Retrieved April 7, 2013. Steve Jobs unveiled the NeXT, the computer he designed after moving on from Apple Computer Inc...

^ Hoppel, Adrian. ""Magical Inventions of Steve Jobs"". Best Inventions of Steve Jobs. Magical Inventions of Steve Jobs. Archived from the original on April 10, 2014. Retrieved March 27, 2012.

^ Paola Antonelli, Paola (April 2006). ""iMac – 1998"". MetropolisMag. Archived from the original on May 11, 2013. Retrieved March 28, 2012.

^ Michael (August 7, 2007). ""Apple History: Evolution of the iMac"". Apple Gazette. Apple Gazette. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ ""iPod First Generation"". iPod History. iPod History. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ a b Block, Ryan. ""The iPod family cemetery"". iPods. EndGadget. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ Asiado, Tel (August 24, 2011). ""Steve Jobs: 10 Products that Define this Tech Legend"". Inventions and Discoveries. Archived from the original on April 4, 2012. Retrieved March 27, 2012.

^ Ha, Peter (October 25, 2010). ""All-TIME 100 Gadgets - TIME"". Time. ISSN 0040-781X. Archived from the original on August 2, 2021. Retrieved October 9, 2021.

^ a b ""iPhone History – Read About The iPhone Story Here"". The Apple Biter's Blog. November 4, 2011. Archived from the original on June 24, 2012. Retrieved October 15, 2014.

^ ""iPhone History and Development"". iPhone apps, tricks, tips, and hacks. Apple iPhone Blog. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ ""iPhone 3GS"". iPhone News. iPhoneHistory. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ ""iPhone 4 Tech Specs"". Apple. Archived from the original on June 24, 2012. Retrieved March 28, 2012.

^ ""The iPad's 5th anniversary: a timeline of Apple's category-defining tablet"". The Verge. April 3, 2015. Archived from the original on April 17, 2015. Retrieved April 17, 2015.

^ Isaacson 2015, p. 274.

^ a b Linzmayer 2004, p. 81.

^ ""Steve Jobs' Childhood Home Draws Tourists; Stepmom Laments Resignation"". Los Altos, CA Patch. August 25, 2011. Archived from the original on June 2, 2021. Retrieved May 30, 2021.

^ ""Laurene Powell Jobs – PARSA"". PARSA Community Foundation. 2006. Archived from the original on September 14, 2010. Retrieved July 8, 2008.

^ Kadifa, Margaret (October 29, 2015). ""Halloween at Steve Jobs' house"". Houston Chronicle. Archived from the original on December 8, 2015. Retrieved December 2, 2015.

^ Gelles, David (February 27, 2020). ""Laurene Powell Jobs Is Putting Her Own Dent in the Universe: An interview with the 35th-richest person in the world"". The New York Times. Archived from the original on May 25, 2020. Retrieved May 25, 2020.

^ Hartmans, Avery (February 28, 2020). ""Laurene Powell Jobs says she won't pass on billions to her children"". Business Insider. Archived from the original on June 29, 2020. Retrieved May 25, 2020. It ends with me

^ Rabbi Jonathan Schwartz, PsyD. (Spring 1979). ""Reclaiming happiness in the digital age"". Jewish Action (OU). pp. 68–72. Archived from the original on January 20, 2021. Retrieved May 25, 2020. Both Bill Gates and Steve Jobs ... raised their children with serious limits on their Internet, social media and gaming access.

^ Akhtar, Allana; Ward, Marguerite (May 15, 2020). ""Bill Gates and Steve Jobs raised their kids with limited tech — and it should have been a red flag about our own smartphone use"". Business Insider. Archived from the original on May 14, 2020. Retrieved May 25, 2020.

^ Brennan, Chrisann. The Bite in the Apple: A Memoir of My Life with Steve Jobs. St. Martin's Griffin. p. 220.

^ a b c Isaacson 2011, pp. 253–255.

^ Isaacson 2015, pp. 253–255.

^ a b Isaacson 2015, p. 254.

^ a b c d e Isaacson 2015, p. 258.

^ a b c Isaacson 2015, p. 255.

^ a b c Isaacson 2015, p. 253.

^ Isaacson 2015, p. 256.

^ a b c d e f g Isaacson 2015, p. 257.

^ Conversations: Malek Jandali, Mona Simpson, & James Gelvin Archived April 14, 2018, at the Wayback Machine (UCLA Hammer Museum event). Hammer.UCLA.edu. Retrieved October 2, 2018.

^ Nath, Trevir I. (October 6, 2021). ""How Steve Jobs changed the world"". www.investopedia.com. Retrieved March 22, 2022.{{cite web}}:  CS1 maint: url-status (link)

^ Whoriskey, Peter (October 6, 2011). ""Record thin on Steve Jobs' philanthropy"". www.washingtonpost.com. Retrieved March 22, 2022.{{cite news}}:  CS1 maint: url-status (link)

^ ""Steve Jobs statue unveiled in Hungary science park"". GlobalPost. December 21, 2011. Archived from the original on January 10, 2012. Retrieved December 28, 2011.

^ ""The National Medal of Technology Recipients 1985 Laureates"". Uspto.gov. Archived from the original on February 4, 2012. Retrieved April 19, 2010.

^ ""National Winners | public service awards"". Jefferson Awards.org. Archived from the original on February 4, 2012. Retrieved April 19, 2010.

^ Bo Burlingham and George Gendron (April 1, 1989). ""The Entrepreneur of the Decade"". Inc. magazine. Archived from the original on June 24, 2012. Retrieved October 8, 2011.

^ ""Reed College Convocation"". Apple iTunes. Portland, Oregon: Reed College. August 27, 1991. Archived from the original on December 11, 2016. Retrieved December 6, 2016.

^ ""25 most powerful people in business – #1: Steve Jobs"". Fortune. Archived from the original on April 10, 2014. Retrieved April 19, 2010.

^ ""Jobs inducted into California Hall of Fame"". Archived from the original on January 10, 2008., California Museum. Retrieved 2007.

^ Arico, Joe (December 22, 2011). ""Steve Jobs Wins Special Grammy"". Mobiledia.com. Archived from the original on September 6, 2012. Retrieved December 28, 2011.

^ ""2012 EDISON AWARDS WINNERS ANNOUNCED"" (PDF). p. 1. Archived (PDF) from the original on October 21, 2021. Retrieved October 19, 2021.

^ Ford, Rebecca (July 10, 2013). ""Steve Jobs, Billy Crystal to Receive Disney Legends Awards"". The Hollywood Reporter. Archived from the original on April 4, 2014. Retrieved July 18, 2013.

^ ""Apple Park's Steve Jobs Theater opens to host 2017 keynote"". Dezeen. September 12, 2017. Archived from the original on January 5, 2018. Retrieved January 4, 2018.


Sources:

Isaacson, Walter (2011). Steve Jobs. Simon and Schuster. ISBN 9781451648546.
Isaacson, Walter (2015). Steve Jobs. Simon and Schuster. ISBN 9781501127625.
Linzmayer, Owen W. (2004). Apple Confidential 2.0: The Definitive History of the World's Most Colorful Company. No Starch Press. p. 81. ISBN 978-1-59327-010-0. Retrieved April 15, 2014.
External links
Apple's official memorial page for Steve Jobs
Steve Jobsat Wikipedia's sister projectsMedia from CommonsNews from WikinewsQuotations from WikiquoteResources from Wikiversity
""Steve Jobs: From Garage to World's Most Valuable Company."" Computer History Museum.
Steve Jobs @ Andy Hertzfeld's The Original Macintosh (folklore.org)
Steve Jobs @ Steve Wozniak's woz.org
Steve Jobs (1955–2011) at IMDb
Forbes Profile
FBI Records: The Vault – Steven Paul Jobs at vault.fbi.gov
2005: Steve Jobs commencement speech at Stanford University
1995: Excerpts from an Oral History Interview with Steve Jobs, Founder, NeXT Computer – Smithsonian Institution, April 20, 1995.
1994: Steve Jobs in 1994: The Rolling Stone Interview – Rolling Stone
1990: Memory and Imagination
1983: The ""Lost"" Steve Jobs Speech from 1983; Foreshadowing Wireless Networking, the iPad, and the App Store (audio clip)


Business positions


Preceded byGil Amelio

 CEO of Apple 1997–2011

Succeeded byTim Cook


Preceded by

 Apple Chairman 1985

Succeeded byMike Markkula


Preceded by

 Apple Chairman 2011

Succeeded byArthur D. Levinson


vteSteve JobsCareer
Timeline
Apple Computer
history
Macintosh
NeXT
Pixar
Return to Apple
""Thoughts on Flash""
Legacy
Artistic depictions
Honors and public recognition
Books about
The Little Kingdom (1984)
The Second Coming of Steve Jobs (2000)
ICon (2005)
Steve Jobs (2011)
The Bite in the Apple (2013)
Becoming Steve Jobs (2015)
Designed by Apple in California (2016)
Small Fry (2018)
Films about
Triumph of the Nerds (1996)
Pirates of Silicon Valley (1999)
Steve Jobs: The Lost Interview (2012)
ISteve (2013)
Jobs (2013)
Steve Jobs (2015)
Steve Jobs: The Man in the Machine (2015)
Family
Laurene Powell Jobs (wife)
Mona Simpson (sister)
Chrisann Brennan (mother of his first child)
Lisa Brennan-Jobs (daughter)
Related
Stevenote
Reality distortion field
Jackling House
The (R)evolution of Steve Jobs
Seva Foundation
The Son of a Migrant from Syria (2015 mural)
Venus yacht
1984 commercial
Think different
Steve Wozniak

vteApple Inc.HistoryOutlineTimeline of productsProductsHardware
iPhone
Hardware
History
TV
Watch
AirPods
Pro
Max
AirTag
Beats
Pill
Powerbeats Pro
HomePod
Mini
Silicon
Mac
iMac
Pro
MacBook
Air
Pro
Mini
Studio
Pro
Comparison
iPod
Classic
Mini
Nano
Shuffle
Touch
iPad
Mini
Air
Pro
Accessories
SoftwareOperatingsystems
iOS / iPadOS
Devices
History
Apps
macOS
History
Server
tvOS
watchOS
bridgeOS
Darwin
Classic Mac OS

CarPlay
Classroom
HomeKit
Core Foundation
Developer Tools
FileMaker
Final Cut Pro
X
Compressor
Motion
Logic Pro
MainStage
iLife
GarageBand
iMovie
iPhoto
iTunes
iWork
Keynote
Numbers
Pages
Mail
QuickTime
Safari
Shazam
Siri
Swift
Xcode
ServicesFinancial
Card
Pay
Wallet
Media
Arcade
Books
Music
1
Beats Music
Up Next
Festival
iTunes Radio
App
News
Newsstand
Podcasts
TV
+
originals
Communication
FaceTime
Walkie-Talkie
iMessage
iChat
App
Game Center
Retail anddigital sales
App Store
iOS / iPadOS
macOS
tvOS
iTunes Store
Connect
Store
Apple Fifth Avenue
Support
AppleCare
Specialist
Certifications
Genius Bar
ProCare
One to One
Other
ID
Sign in with Apple
One
Developer
iAd
TestFlight
WWDC
iCloud
MobileMe
Find My
Fitness
Photos
Maps
Look Around
CompaniesSubsidiaries
Anobit
Apple IMC
Apple Studios
Beats
Beddit
Braeburn Capital
Claris
Acquisitions
List
Anobit
AuthenTec
Beats
Beddit
Cue
EditGrid
Emagic
FingerWorks
Intrinsity
InVisage Technologies
The Keyboard Company
Lala
Metaio
NeXT
Nothing Real
P.A. Semi
PrimeSense
Shazam Entertainment Limited
Siri
Spotsetter
Texture
Topsy
Partnerships
AIM alliance
Kaleida Labs
Taligent
Akamai
Arm Ltd.
DiDi
Digital Ocean
iFund
Imagination
Rockstar Consortium
Related
Advertising
""1984""
""Think different""
""Get a Mac""
iPod
Product Red
Ecosystem
Events
Headquarters
Campus
Park
University
Design
IDg
Typography
Book
History
Codenames
Community
AppleMasters
Criticism
Litigation
FBI–Apple encryption dispute
iOS app approvals
Worker organizations
Artistic depictions of Steve Jobs
PeopleExecutivesCurrent
Tim Cook (CEO)
Jeff Williams (COO)
Luca Maestri (CFO)
Katherine Adams (General Counsel)
Eddy Cue
Craig Federighi
Isabel Ge Mahe
John Giannandrea
Lisa Jackson
Greg Joswiak
Deirdre O'Brien
Dan Riccio
Phil Schiller
Johny Srouji
Former
Michael Scott (CEO)
Mike Markkula (CEO)
John Sculley (CEO)
Michael Spindler (CEO)
Gil Amelio (CEO)
Steve Jobs (CEO)
Jony Ive (CDO)
Angela Ahrendts
Fred D. Anderson
John Browett
Guerrino De Luca
Paul Deneve
Al Eisenstat
Tony Fadell
Scott Forstall
Ellen Hancock
Nancy R. Heinen
Ron Johnson
David Nagel
Peter Oppenheimer
Mark Papermaster
Jon Rubinstein
Bertrand Serlet
Bruce Sewell
Sina Tamaddon
Avie Tevanian
Ronald Wayne
Steve Wozniak
Board ofdirectorsCurrent
Arthur D. Levinson (Chairman)
Tim Cook (CEO)
James A. Bell
Albert Gore Jr.
Andrea Jung
Ronald D. Sugar
Susan L. Wagner
Former
Mike Markkula (Chairman)
John Sculley (Chairman)
Steve Jobs (Chairman)
Gil Amelio
Fred D. Anderson
Bill Campbell
Mickey Drexler
Al Eisenstat
Larry Ellison
Robert A. Iger
Delano Lewis
Arthur Rock
Eric Schmidt
Michael Scott
Michael Spindler
Edgar S. Woolard Jr.
Jerry York
Founders
Steve Jobs
Steve Wozniak
Ronald Wayne
Italics indicate discontinued products, services, or defunct companies.
 Category
 Commons
vteKey figures in the history of Apple Inc.Founders
Steve Jobs
Steve Wozniak
Ronald Wayne
CEOs
Michael Scott (1977–1981)
Mike Markkula (1981–1983)
John Sculley (1983–1993)
Michael Spindler (1993–1996)
Gil Amelio (1996–1997)
Steve Jobs (1997–2011)
Tim Cook (2011–present)
Currentemployees
Katherine L. Adams
Eddy Cue
Chris Espinosa
Craig Federighi
Lisa P. Jackson
Greg Joswiak
Luca Maestri
Bob Mansfield
Dan Riccio
Phil Schiller
Johny Srouji
Bud Tribble
Jeff Williams
Steve Wozniak
Formeremployees
Gil Amelio
Angela Ahrendts
Fred D. Anderson
Bill Atkinson
Susan Barnes
Chrisann Brennan
Steve Capps
Satjiv S. Chahil
George Crow
Tony Fadell
Bill Fernandez
Scott Forstall
Jean-Louis Gassée
Ellen Hancock
Nancy R. Heinen
Andy Hertzfeld
Joanna Hoffman
Rod Holt
Bruce Horn
Jony Ive
Steve Jobs
Ron Johnson
Susan Kare
Guy Kawasaki
Alan Kay
Daniel Kottke
Chris Lattner
Guerrino De Luca
Mike Markkula
David Nagel
Ike Nassi
Don Norman
Peter Oppenheimer
Rich Page
Mark Papermaster
Jef Raskin
Jon Rubinstein
Michael Scott
John Sculley
Bertrand Serlet
Bruce Sewell
Burrell Smith
Michael Spindler
Sina Tamaddon
Avie Tevanian
Ronald Wayne
Del Yocam

vteOriginal Macintosh developer team
Bill Atkinson
Steve Capps
George Crow
Chris Espinosa
Andy Hertzfeld
Joanna Hoffman
Bruce Horn
Steve Jobs
Susan Kare
Jef Raskin
Burrell Smith
Bud Tribble
Steve Wozniak
Randy Wigginton

vteNeXTCorporate directors
Steve Jobs
Ross Perot
John Patrick Crecine
Team members
Susan Barnes
George Crow
Joanna Hoffman
Steve Jobs
Susan Kare
Rich Page
Bud Tribble
Hardware products
NeXT Computer
NeXTcube
NeXTcube Turbo
NeXTstation
NeXTdimension
NeXT MegaPixel Display
NeXT Laser Printer
Software products
NeXTSTEP
OpenStep
WebObjects
Interface Builder

 Category
 Commons

vtePixar Animation StudiosA subsidiary of Walt Disney Studios, a division of The Walt Disney Company.Feature filmsComputer animated
Toy Story (1995)
A Bug's Life (1998)
Toy Story 2 (1999)
Monsters, Inc. (2001)
Finding Nemo (2003)
The Incredibles (2004)
Cars (2006)
Ratatouille (2007)
WALL-E (2008)
Up (2009)
Toy Story 3 (2010)
Cars 2 (2011)
Brave (2012)
Monsters University (2013)
Inside Out (2015)
The Good Dinosaur (2015)
Finding Dory (2016)
Cars 3 (2017)
Coco (2017)
Incredibles 2 (2018)
Toy Story 4 (2019)
Onward (2020)
Soul (2020)
Luca (2021)
Turning Red (2022)
Traditionally animated
Buzz Lightyear of Star Command: The Adventure Begins (2000)
Upcoming
Lightyear (2022)
Short filmsTheatrical
The Adventures of André & Wally B. (1984)
Luxo Jr. (1986)
Red's Dream (1987)
Tin Toy (1988)
Knick Knack (1989)
Geri's Game (1997)
For the Birds (2000)
Boundin' (2003)
One Man Band (2005)
Lifted (2006)
Presto (2008)
Partly Cloudy (2009)
Day & Night (2010)
La Luna (2011)
The Blue Umbrella (2013)
Lava (2014)
Sanjay's Super Team (2015)
Piper (2016)
Lou (2017)
Bao (2018)
SparkShorts
Purl (2019)
Smash and Grab (2019)
Kitbull (2019)
Float (2019)
Wind (2019)
Loop (2020)
Out (2020)
Burrow (2020)
Twenty Something (2021)
Nona (2021)
Feature-related
Mike's New Car (2002)
Exploring the Reef (2003)
Jack-Jack Attack (2005)
Mr. Incredible and Pals (2005)
Mater and the Ghostlight (2006)
Your Friend the Rat (2007)
BURN-E (2008)
Dug's Special Mission (2009)
George and A.J. (2009)
The Legend of Mor'du (2012)
Party Central (2013)
Riley's First Date? (2015)
Marine Life Interviews (2016)
Miss Fritter's Racing Skoool (2017)
Auntie Edna (2018)
Lamp Life (2020)
22 vs. Earth (2021)
Ciao Alberto (2021)
Series
Cars Toons (2008–14)
Toy Story Toons (2011–12)
Forky Asks a Question (2019–20)
Dug Days (2021)
Cars on the Road (2022)
Compilations
Tiny Toy Stories (1996)
Pixar Short Films Collection, Volume 1 (2007)
Cars Toons: Mater's Tall Tales  (2010)
Pixar Short Films Collection, Volume 2 (2012)
Pixar Short Films Collection, Volume 3 (2018)
Other works
Beach Chair (1986)
Flags and Waves (1986)
Light & Heavy (1990)
Surprise (1991)
Television series
Buzz Lightyear of Star Command (2000–2001)

Television specials
Toy Story of Terror! (2013)
Toy Story That Time Forgot (2014)
Franchises
Toy Story
Monsters, Inc.
Finding Nemo
The Incredibles
Cars
Associatedproductions
Planes (2013)
Planes: Fire & Rescue (2014)
Borrowed Time (2016)
Monsters at Work (television series; 2021–present)
Documentaries
The Pixar Story (2007)
Inside Pixar (2020–21; docuseries)
A Spark Story (2021)
Embrace the Panda: Making Turning Red (2022)
Disney attractionsand experiences
It's Tough to Be a Bug! (1998)
A Bug's Land (2002)
Turtle Talk with Crush (2004)
The Seas with Nemo & Friends (2007)
Crush's Coaster (2007)
Finding Nemo Submarine Voyage (2007)
Toy Story Mania! (2008)
Toy Story Land (2010)
RC Racer (2010)
Slinky Dog Zigzag Spin (2010)
Toy Soldiers Parachute Drop (2010)
Alien Swirling Saucers (2018)
Nemo & Friends SeaRider (2017)
Pixar Pier (2018)
Games of Pixar Pier (2018)
Incredicoaster (2018)
Pixar Pal-A-Round (2018)
Jessie's Critter Carousel (2019)
Inside Out: Emotional Whirlwind (2019)
Inside Out: Joyful Sweets (2022)
Products
Pixar Image Computer
RenderMan
Presto Animation System
People
John Lasseter
Edwin Catmull
Steve Jobs
Alvy Ray Smith
Jim Morris
Pete Docter
See also
List of Pixar characters
Luxo Jr.
List of Pixar awards and nominations
feature films
short films
List of Pixar film references
Computer Graphics Lab
Industrial Light & Magic
Lucasfilm Animation
Circle Seven Animation
Pixar Canada
Pixar Photoscience Team
Computer Animation Production System
A Computer Animated Hand
The Works
The Shadow King
Kingdom Hearts III
Walt Disney Animation Studios
20th Century Animation
Blue Sky Studios
The Walt Disney Studios

 Category

vteDisney Legends Awards (2010s)2011
Jodi Benson
Barton “Bo” Boyd*
Jim Henson*
Linda Larkin
Paige O'Hara
Regis Philbin
Anika Noni Rose
Lea Salonga
Raymond Watson
Guy Williams*
Bonita Granville Wrather*
Jack Wrather*
2013
Tony Baxter
Collin Campbell
Dick Clark
Billy Crystal
John Goodman
Steve Jobs*
Glen Keane
Ed Wynn*
2015
George Bodenheimer
Julie Reihm Casaletto
Andreas Deja
Johnny Depp
Eyvind Earle*
Danny Elfman
Susan Lucci
George Lucas
Carson Van Osten
2017
Clyde Geronimi*
Whoopi Goldberg
Manuel Gonzales*
Carrie Fisher*
Mark Hamill
Jack Kirby*
Wayne Jackson
Stan Lee
Garry Marshall*
Julie Taymor
Oprah Winfrey
2019
Christina Aguilera
Wing T. Chao
Robert Downey Jr.
Jon Favreau
James Earl Jones
Bette Midler
Kenny Ortega
Barnette Ricci
Robin Roberts
Diane Sawyer
Ming-Na Wen
Hans Zimmer
* Awarded posthumously
Complete list
1980s
1990s
2000s
2010s
Authority control General
ISNI
1
VIAF
1
WorldCat
National libraries
Norway
Spain
France (data)
Argentina
Catalonia
Germany
Italy
Israel
United States
Latvia
Japan
Czech Republic
Australia
2
Greece
Korea
Croatia
Netherlands
Poland
Sweden
Art research institutes
Artist Names (Getty)
Scientific databases
CiNii (Japan)
Other
Faceted Application of Subject Terminology
MusicBrainz artist
Social Networks and Archival Context
SUDOC (France)
1
Trove (Australia)
1





Retrieved from ""https://en.wikipedia.org/w/index.php?title=Steve_Jobs&oldid=1085951290""
Categories: Steve Jobs1955 births2011 deaths20th-century American businesspeople21st-century American businesspeopleAmerican adopteesAmerican billionairesAmerican computer businesspeopleAmerican film producersAmerican film studio executivesAmerican financiersAmerican industrial designersAmerican inventorsAmerican investorsAmerican people of German descentAmerican people of Swiss descentAmerican people of Syrian descentAmerican philanthropistsAmerican psychedelic drug advocatesAmerican technology chief executivesAmerican technology company foundersAmerican Zen BuddhistsApple Inc.Apple Inc. executivesAtari peopleBurials in CaliforniaBusinesspeople from San FranciscoBusinesspeople in softwareComputer designersDeaths from cancer in CaliforniaDeaths from pancreatic cancerDirectors of Apple Inc.Disney executivesInternet pioneersLiver transplant recipientsNational Medal of Technology recipientsNeXTPeople from Cupertino, CaliforniaPeople from Los Altos, CaliforniaPeople from Mountain View, CaliforniaPeople from Palo Alto, CaliforniaPersonal computingPixarPixar peopleSpokespersonsTime 100Hidden categories: Wikipedia articles needing page number citations from April 2017CS1 Italian-language sources (it)Webarchive template wayback linksWebarchive template webcite linksCS1 maint: url-statusArticles with short descriptionShort description is different from WikidataWikipedia indefinitely semi-protected pagesUse American English from March 2015All Wikipedia articles written in American EnglishUse mdy dates from January 2022Biography with signatureArticles with hCardsArticles containing Arabic-language textWikipedia articles needing page number citations from March 2018All articles with unsourced statementsArticles with unsourced statements from September 2020Wikipedia articles needing clarification from August 2019Articles with unsourced statements from July 2021Articles containing potentially dated statements from January 2006All articles containing potentially dated statementsArticles with unsourced statements from April 2018Articles containing potentially dated statements from April 2015Articles with unsourced statements from October 2021Articles with ISNI identifiersArticles with VIAF identifiersArticles with WORLDCATID identifiersArticles with BIBSYS identifiersArticles with BNE identifiersArticles with BNF identifiersArticles with BNMM identifiersArticles with CANTICN identifiersArticles with GND identifiersArticles with ICCU identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with LNB identifiersArticles with NDL identifiersArticles with NKC identifiersArticles with NLA identifiersArticles with NLG identifiersArticles with NLK identifiersArticles with NSK identifiersArticles with NTA identifiersArticles with PLWABN identifiersArticles with SELIBR identifiersArticles with ULAN identifiersArticles with CINII identifiersArticles with FAST identifiersArticles with MusicBrainz identifiersArticles with SNAC-ID identifiersArticles with SUDOC identifiersArticles with Trove identifiersAC with 29 elementsArticles with multiple identifiers

"
